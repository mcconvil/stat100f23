[
  {
    "objectID": "stat100_wk05mon.html#announcements",
    "href": "stat100_wk05mon.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nDiscuss exams:\n\nRegistrar’s Office posted our Final Exam time:\n\nIn-class: Fri, Dec 15th 9am - noon\nOral: Wed, Dec 13th & Thurs, Dec 14th\n\nMidterm next week\n\nIn-class: Wed, Oct 11th 10:30 - 11:15 11:45am\nOral: Wed afternoon - Fri, Oct 13th\nNo sections during midterm exam week!\n\n\n\nGoals for Today\n\nDiscus data ethics: responsibilities to research subjects\nFinish up data collection"
  },
  {
    "objectID": "stat100_wk05mon.html#responsibilities-to-research-subjects",
    "href": "stat100_wk05mon.html#responsibilities-to-research-subjects",
    "title": "Stat 100",
    "section": "Responsibilities to Research Subjects",
    "text": "Responsibilities to Research Subjects\n\n“The ethical statistician protects and respects the rights and interests of human and animal subjects at all stages of their involvement in a project. This includes respondents to the census or to surveys, those whose data are contained in administrative records, and subjects of physically or psychologically invasive research.”"
  },
  {
    "objectID": "stat100_wk05mon.html#detour-from-our-detour",
    "href": "stat100_wk05mon.html#detour-from-our-detour",
    "title": "Stat 100",
    "section": "Detour from Our Detour",
    "text": "Detour from Our Detour\n\n\nlibrary(tidyverse)\nlibrary(NHANES)\n\nggplot(data = NHANES, \n       mapping = aes(x = Age,\n                     y = Height)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(color = \"skyblue\")"
  },
  {
    "objectID": "stat100_wk05mon.html#detour-from-our-detour-1",
    "href": "stat100_wk05mon.html#detour-from-our-detour-1",
    "title": "Stat 100",
    "section": "Detour from Our Detour",
    "text": "Detour from Our Detour\n\n\nlibrary(tidyverse)\nlibrary(NHANES)\nlibrary(emojifont)\n\nNHANES &lt;- mutate(NHANES, \n          heart = fontawesome(\"fa-pumpkin\"))\n\nggplot(data = NHANES, \n       mapping = aes(x = Age,\n                     y = Height,\n                     label = heart)) +\n  geom_text(alpha = 0.1, color = \"red\",\n            family='fontawesome-webfont',\n            size = 16) +\n  stat_smooth(color = \"deeppink\")"
  },
  {
    "objectID": "stat100_wk05mon.html#who-are-the-data-supposed-to-represent",
    "href": "stat100_wk05mon.html#who-are-the-data-supposed-to-represent",
    "title": "Stat 100",
    "section": "Who are the data supposed to represent?",
    "text": "Who are the data supposed to represent?"
  },
  {
    "objectID": "stat100_wk05mon.html#who-are-the-data-supposed-to-represent-1",
    "href": "stat100_wk05mon.html#who-are-the-data-supposed-to-represent-1",
    "title": "Stat 100",
    "section": "Who are the data supposed to represent?",
    "text": "Who are the data supposed to represent?\n\n\n\n\n\nKey questions:\n\nWhat evidence is there that the respondents are representative of the population?\nWho is present? Who is absent?\nWho is overrepresented? Who is underrepresented?"
  },
  {
    "objectID": "stat100_wk05mon.html#nonresponse-bias",
    "href": "stat100_wk05mon.html#nonresponse-bias",
    "title": "Stat 100",
    "section": "Nonresponse bias",
    "text": "Nonresponse bias\n\n\n\n\n\nNonresponse bias: The respondents are systematically different from the non-respondents for the variables of interest."
  },
  {
    "objectID": "stat100_wk05mon.html#tackling-nonresponse-bias",
    "href": "stat100_wk05mon.html#tackling-nonresponse-bias",
    "title": "Stat 100",
    "section": "Tackling Nonresponse bias",
    "text": "Tackling Nonresponse bias\n\n\n\n\n\n\nUse multiple modes (mail, phone, in-person) and multiple attempts for reaching sampled cases.\nExplore key demographic variables to see how respondents and non-respondents vary.\nTake a survey stats course to learn how to create survey weights to adjust for potential nonresponse bias."
  },
  {
    "objectID": "stat100_wk05mon.html#is-bigger-always-better",
    "href": "stat100_wk05mon.html#is-bigger-always-better",
    "title": "Stat 100",
    "section": "Is Bigger Always Better?",
    "text": "Is Bigger Always Better?\n\n\n\n\n\nFor our Literary Digest Example, Gallup predicted Roosevelt would win based on a survey of 50,000 people (instead of 2.4 million)."
  },
  {
    "objectID": "stat100_wk05mon.html#thoughts-on-sampling",
    "href": "stat100_wk05mon.html#thoughts-on-sampling",
    "title": "Stat 100",
    "section": "Thoughts on Sampling",
    "text": "Thoughts on Sampling\n\nRandom sampling is important to ensure the sample is representative of the population.\n\nWord we will use: generalizability\n\nRepresentativeness isn’t about size.\n\nSmall random samples will tend to be more representative than large non-random samples.\n\nHowever, I bet most samples you will encounter won’t have arisen from a random mechanism.\nHow do we draw conclusions about the population from non-random samples?\n\nDeterminee if your sampled cases (and respondents) are systematically different from the non-sampled cases (and non-respondents) for the variables you care about.\nAdjust your population of interest.\nTake a survey stats course to learn how to adjust the sample to make it more representative."
  },
  {
    "objectID": "stat100_wk05mon.html#careful-with-non-random-assignment-data",
    "href": "stat100_wk05mon.html#careful-with-non-random-assignment-data",
    "title": "Stat 100",
    "section": "Careful with Non-Random Assignment Data",
    "text": "Careful with Non-Random Assignment Data\n\n\nWe have data on the number of Methodist ministers in New England and the number of barrels of rum imported into Boston each year. The data range from 1860 to 1940.\n\nShould we conclude that ministers drink a lot of rum? Or maybe that rum drinking encourages church attendance?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConfounding variable: A third variable that is associated with both the explanatory variable and the response variable.\nUnclear if the explanatory variable or the confounder (or some other variable) is causing changes in the response."
  },
  {
    "objectID": "stat100_wk05mon.html#causal-inference",
    "href": "stat100_wk05mon.html#causal-inference",
    "title": "Stat 100",
    "section": "Causal Inference",
    "text": "Causal Inference\n\nSpurious relationship: Two variables are associated but not causally related\n\nIn the age of big data, lots of good examples out there.\n\n\n\n→ “Correlation does not imply causation.”\n\n\n→ “Correlation does not imply not causation.”\n\n\nCausal inference: Methods for finding causal relationships even when the data were collected without random assignment."
  },
  {
    "objectID": "stat100_wk05mon.html#observational-studies",
    "href": "stat100_wk05mon.html#observational-studies",
    "title": "Stat 100",
    "section": "Observational Studies",
    "text": "Observational Studies\n\nA study in which the researchers don’t actively control the value of any variable, but simply observe the values as they naturally exist.\nExample: Hand washing study\n\nTo estimate what percent of people in the US wash their hands after using a public restroom, researchers pretended to comb their hair while observing 6000 people in public restrooms throughout the United States. They found that 85% of the people who were observed washed their hands after going to the bathroom."
  },
  {
    "objectID": "stat100_wk05mon.html#randomized-experiment",
    "href": "stat100_wk05mon.html#randomized-experiment",
    "title": "Stat 100",
    "section": "(Randomized) Experiment",
    "text": "(Randomized) Experiment\n\nA study in which the researcher actively controls one or more of the explanatory variables through random assignment.\nExample: COVID Trial\nCommon features:\n\nControl group that gets no treatment or a standard treatment\nPlacebo: A fake treatment to control for the placebo effect where if people believe they are receiving a treatment, they may experience the desired effect regardless of whether the treatment is any good.\nBlinding: When the subjects and/or researchers don’t know the explanatory group assignments."
  },
  {
    "objectID": "stat100_wk05mon.html#thoughts-on-data-collection-goals",
    "href": "stat100_wk05mon.html#thoughts-on-data-collection-goals",
    "title": "Stat 100",
    "section": "Thoughts on Data Collection Goals",
    "text": "Thoughts on Data Collection Goals\n\nRandom assignment allows you to explore causal relationships between your explanatory variables and the predictor variables because the randomization makes the explanatory groups roughly similar.\nHow do we draw causal conclusions from studies without random assignment?\n\nWith extreme care! Try to control for all possible confounding variables.\nDiscuss the associations/correlations you found. Use domain knowledge to address potentially causal links.\nTake more stats to learn more about causal inference.\n\nBut also consider the goals of your analysis. Often the research question isn’t causal.\n\nBottom Line: We often have to use imperfect data to make decisions."
  },
  {
    "objectID": "stat100_wk05mon.html#reminders",
    "href": "stat100_wk05mon.html#reminders",
    "title": "Stat 100",
    "section": "Reminders",
    "text": "Reminders\n\nDiscuss exams:\n\nRegistrar’s Office posted our Final Exam time:\n\nIn-class: Fri, Dec 15th 9am - noon\nOral: Wed, Dec 13th & Thurs, Dec 14th\n\nMidterm next week\n\nIn-class: Wed, Oct 11th 10:30 - 11:15 11:45am\nOral: Wed afternoon - Fri, Oct 13th\nNo sections during midterm exam week!"
  },
  {
    "objectID": "stat100_wk03mon.html#announcements",
    "href": "stat100_wk03mon.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nWith COVID working its way through campus right now, make sure to check the Sections spreadsheet and the Office hours spreadsheet for updates!\nGrab a postcard and/or a stamp from SC 316 if you lost yours.\n\nWe also have markers, colored pencils, and crayons!\n\nDon’t forget that P-Set 1 due on Tuesday by 5pm in Gradescope.\nCome by office hours with any questions."
  },
  {
    "objectID": "stat100_wk03mon.html#goals-for-today",
    "href": "stat100_wk03mon.html#goals-for-today",
    "title": "Stat 100",
    "section": "Goals for Today",
    "text": "Goals for Today\n\n\n\nCome back to the general structure of ggplot2.\nLearn a few standard graphs for numerical/quantitative data:\n\nHistogram: one numerical variable\nSide-by-side boxplot: one numerical variable and one categorical variable\nSide-by-side violin plot: one numerical variable and one categorical variable\nScatterplot: two numerical variables\nLinegraph: two numerical variables\n\n\n\n\nAnd, learn the standard graphic for categorical data:\n\nBarplot: one categorical variable\nSegmented barplot: two categorical variables\n\nAlso cover some common extensions and customizations."
  },
  {
    "objectID": "stat100_wk03mon.html#load-necessary-packages",
    "href": "stat100_wk03mon.html#load-necessary-packages",
    "title": "Stat 100",
    "section": "Load Necessary Packages",
    "text": "Load Necessary Packages\n\n\n\n\n\nggplot2 is part of this collection of data science packages.\n\n# Load necessary packages\nlibrary(tidyverse)"
  },
  {
    "objectID": "stat100_wk03mon.html#data-setting-eco-totem-broadway-bicycle-count",
    "href": "stat100_wk03mon.html#data-setting-eco-totem-broadway-bicycle-count",
    "title": "Stat 100",
    "section": "Data Setting: Eco-Totem Broadway Bicycle Count",
    "text": "Data Setting: Eco-Totem Broadway Bicycle Count"
  },
  {
    "objectID": "stat100_wk03mon.html#import-the-data",
    "href": "stat100_wk03mon.html#import-the-data",
    "title": "Stat 100",
    "section": "Import the Data",
    "text": "Import the Data\n\njuly_2019 &lt;- read_csv(\"data/july_2019.csv\")\n\n# Inspect the data\nglimpse(july_2019)\n\nRows: 192\nColumns: 8\n$ DateTime  &lt;chr&gt; \"07/04/2019 12:00:00 AM\", \"07/04/2019 12:15:00 AM\", \"07/04/2…\n$ Day       &lt;chr&gt; \"Thursday\", \"Thursday\", \"Thursday\", \"Thursday\", \"Thursday\", …\n$ Date      &lt;date&gt; 2019-07-04, 2019-07-04, 2019-07-04, 2019-07-04, 2019-07-04,…\n$ Time      &lt;time&gt; 00:00:00, 00:15:00, 00:30:00, 00:45:00, 01:00:00, 01:15:00,…\n$ Total     &lt;dbl&gt; 2, 3, 2, 0, 3, 2, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, …\n$ Westbound &lt;dbl&gt; 2, 3, 1, 0, 2, 2, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, …\n$ Eastbound &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ Occasion  &lt;chr&gt; \"Fourth of July\", \"Fourth of July\", \"Fourth of July\", \"Fourt…"
  },
  {
    "objectID": "stat100_wk03mon.html#ggplot2-example-code",
    "href": "stat100_wk03mon.html#ggplot2-example-code",
    "title": "Stat 100",
    "section": "ggplot2 example code",
    "text": "ggplot2 example code\nGuiding Principle: We will map variables from the data to the aesthetic attributes (e.g. location, size, shape, color) of geometric objects (e.g. points, lines, bars).\n\n\nggplot(data = ---, mapping = aes(---)) +\n  geom_---(---) \n\n\nThere are other layers, such as scales_---_---() and labs(), but we will wait on those."
  },
  {
    "objectID": "stat100_wk03mon.html#histograms",
    "href": "stat100_wk03mon.html#histograms",
    "title": "Stat 100",
    "section": "Histograms",
    "text": "Histograms\n\n\n# Create histogram\nggplot(data = july_2019, \n       mapping = aes(x = Total)) +\n  geom_histogram()"
  },
  {
    "objectID": "stat100_wk03mon.html#histograms-1",
    "href": "stat100_wk03mon.html#histograms-1",
    "title": "Stat 100",
    "section": "Histograms",
    "text": "Histograms\n\n\n# Create histogram\nggplot(data = july_2019, \n       mapping = aes(x = Total)) +\n  geom_histogram(color = \"white\",\n                 fill = \"violetred1\",\n                 bins = 50)\n\n\n\n\n\n\n\n\n\n\n\nmapping to a variable goes in aes()\nsetting to a specific value goes in the geom_---()"
  },
  {
    "objectID": "stat100_wk03mon.html#boxplots",
    "href": "stat100_wk03mon.html#boxplots",
    "title": "Stat 100",
    "section": "Boxplots",
    "text": "Boxplots\n\n\n\nFive number summary:\n\nMinimum\nFirst quartile (Q1)\nMedian\nThird quartile (Q3)\nMaximum\n\nInterquartile range (IQR) \\(=\\) Q3 \\(-\\) Q1\nOutliers: unusual points\n\nBoxplot defines unusual as being beyond \\(1.5*IQR\\) from \\(Q1\\) or \\(Q3\\).\n\nWhiskers: reach out to the furthest point that is NOT an outlier"
  },
  {
    "objectID": "stat100_wk03mon.html#boxplots-1",
    "href": "stat100_wk03mon.html#boxplots-1",
    "title": "Stat 100",
    "section": "Boxplots",
    "text": "Boxplots\n\n\n# Create boxplot\nggplot(data = july_2019, \n       mapping = aes(x = Occasion, \n                     y = Total)) +\n  geom_boxplot()"
  },
  {
    "objectID": "stat100_wk03mon.html#boxplots-2",
    "href": "stat100_wk03mon.html#boxplots-2",
    "title": "Stat 100",
    "section": "Boxplots",
    "text": "Boxplots\n\n\nggplot(data = july_2019, \n       mapping = aes(x = Occasion, \n                     y = Total)) +\n  geom_boxplot(fill = \"springgreen3\")"
  },
  {
    "objectID": "stat100_wk03mon.html#boxplots-3",
    "href": "stat100_wk03mon.html#boxplots-3",
    "title": "Stat 100",
    "section": "Boxplots",
    "text": "Boxplots\n\n\nggplot(data = july_2019, \n       mapping = aes(x = Occasion, \n                     y = Total,\n                     fill = Total)) +\n  geom_boxplot()"
  },
  {
    "objectID": "stat100_wk03mon.html#boxplots-4",
    "href": "stat100_wk03mon.html#boxplots-4",
    "title": "Stat 100",
    "section": "Boxplots",
    "text": "Boxplots\n\n\nggplot(data = july_2019, \n       mapping = aes(x = Occasion, \n                     y = Total,\n                     fill = Occasion)) +\n  geom_boxplot() +\n  guides(fill = \"none\")"
  },
  {
    "objectID": "stat100_wk03mon.html#violin-plots",
    "href": "stat100_wk03mon.html#violin-plots",
    "title": "Stat 100",
    "section": "Violin Plots",
    "text": "Violin Plots\n\n\nggplot(data = july_2019, \n       mapping = aes(x = Occasion, \n                     y = Total,\n                     fill = Occasion)) +\n  geom_violin() +\n  guides(fill = \"none\")"
  },
  {
    "objectID": "stat100_wk03mon.html#boxplot-versus-violin-plots",
    "href": "stat100_wk03mon.html#boxplot-versus-violin-plots",
    "title": "Stat 100",
    "section": "Boxplot Versus Violin Plots",
    "text": "Boxplot Versus Violin Plots"
  },
  {
    "objectID": "stat100_wk03mon.html#scatterplots",
    "href": "stat100_wk03mon.html#scatterplots",
    "title": "Stat 100",
    "section": "Scatterplots",
    "text": "Scatterplots\n\nExplore relationships between numerical variables.\n\nWe will be especially interested in linear relationships.\n\n\n\n\n\nggplot(data = july_2019,\n       mapping = aes(x = Time,\n                     y = Total)) +\n  geom_point(size = 2)"
  },
  {
    "objectID": "stat100_wk03mon.html#scatterplots-1",
    "href": "stat100_wk03mon.html#scatterplots-1",
    "title": "Stat 100",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\nggplot(data = july_2019,\n       mapping = aes(x = Time,\n                     y = Total)) +\n  geom_point(size = 2, alpha = 0.6)\n\n\n\n\n\n\n\n\n\n\n\nFix over-plotting\nWhy the weird pattern??"
  },
  {
    "objectID": "stat100_wk03mon.html#scatterplots-2",
    "href": "stat100_wk03mon.html#scatterplots-2",
    "title": "Stat 100",
    "section": "Scatterplots",
    "text": "Scatterplots\n\n\nggplot(data = july_2019,\n       mapping = aes(x = Time,\n                     y = Total,\n                     color = Occasion)) +\n  geom_point(size = 2, alpha = 0.6)"
  },
  {
    "objectID": "stat100_wk03mon.html#linegraphs",
    "href": "stat100_wk03mon.html#linegraphs",
    "title": "Stat 100",
    "section": "Linegraphs",
    "text": "Linegraphs\nAlso called time series plot when time is represented on the x axis.\n\n\nggplot(data = july_2019,\n       mapping = aes(x = Time,\n                     y = Total,\n                     color = Occasion)) +\n  geom_line(alpha = 0.6)"
  },
  {
    "objectID": "stat100_wk03mon.html#linegraphs-1",
    "href": "stat100_wk03mon.html#linegraphs-1",
    "title": "Stat 100",
    "section": "Linegraphs",
    "text": "Linegraphs\nAlso called time series plot when time is represented on the x axis.\n\n\nggplot(data = july_2019,\n       mapping = aes(x = Time,\n                     y = Total,\n                     color = Occasion)) +\n  geom_line(alpha = 0.6) +\n  theme(legend.pos = \"bottom\")"
  },
  {
    "objectID": "stat100_wk03mon.html#new-data-setting-dog-names-in-cambridge-ma",
    "href": "stat100_wk03mon.html#new-data-setting-dog-names-in-cambridge-ma",
    "title": "Stat 100",
    "section": "New Data Setting: Dog Names in Cambridge, MA",
    "text": "New Data Setting: Dog Names in Cambridge, MA\nBased on dog license data collected by Cambridge’s Animal Commission\n\n# Import and inspect data\ndogs &lt;- read_csv(\"https://data.cambridgema.gov/api/views/sckh-3xyx/rows.csv\")\nglimpse(dogs)\n\nRows: 3,942\nColumns: 6\n$ Dog_Name         &lt;chr&gt; \"Butch\", \"Baxter\", \"Bodhi\", \"Ocean\", \"Coco\", \"Brio\", …\n$ Dog_Breed        &lt;chr&gt; \"Mixed Breed\", \"Mixed Breed\", \"Golden Retriever\", \"Pu…\n$ Location_masked  &lt;chr&gt; \"POINT (-71.1328 42.3989)\", \"POINT (-71.1186 42.3814)…\n$ Latitude_masked  &lt;dbl&gt; 42.3989, 42.3814, 42.3998, 42.3726, 42.3610, 42.3892,…\n$ Longitude_masked &lt;dbl&gt; -71.1328, -71.1186, -71.1308, -71.1087, -71.1022, -71…\n$ Neighborhood     &lt;chr&gt; \"North Cambridge\", \"Neighborhood Nine\", \"North Cambri…"
  },
  {
    "objectID": "stat100_wk03mon.html#data-wrangling",
    "href": "stat100_wk03mon.html#data-wrangling",
    "title": "Stat 100",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nWe haven’t learned this topic yet.\nI only included this code for completeness/transparency.\n\n# Create a column for Breed\ndogs &lt;- mutate(dogs, Breed = if_else(\n                       Dog_Breed == \"Mixed Breed\",\n                       \"Mixed\", \"Single\"))\n\n\n# Find the 5 top most common names\ntop5names &lt;- count(dogs, Dog_Name) %&gt;%\n  slice_max(n = 5, order_by = n) %&gt;%\n  select(Dog_Name) %&gt;%\n  pull()\n  \n# Filter dataset to only the 5 top most common names\ndogs_top5 &lt;- filter(dogs,\n                     Dog_Name %in% top5names)"
  },
  {
    "objectID": "stat100_wk03mon.html#barplots",
    "href": "stat100_wk03mon.html#barplots",
    "title": "Stat 100",
    "section": "Barplots",
    "text": "Barplots\n\n\nDisplays the frequency for each category."
  },
  {
    "objectID": "stat100_wk03mon.html#barplots-1",
    "href": "stat100_wk03mon.html#barplots-1",
    "title": "Stat 100",
    "section": "Barplots",
    "text": "Barplots\n\n\n# Create barplot\nggplot(data = dogs_top5, \n    mapping = aes(x = Dog_Name)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nHow could we make this graph better?"
  },
  {
    "objectID": "stat100_wk03mon.html#barplots-2",
    "href": "stat100_wk03mon.html#barplots-2",
    "title": "Stat 100",
    "section": "Barplots",
    "text": "Barplots\n\n\n# Create barplot\nggplot(data = dogs_top5, \n  mapping = aes(x = fct_infreq(Dog_Name))) +\n  geom_bar()"
  },
  {
    "objectID": "stat100_wk03mon.html#segmented-barplots",
    "href": "stat100_wk03mon.html#segmented-barplots",
    "title": "Stat 100",
    "section": "Segmented Barplots",
    "text": "Segmented Barplots\n\n\n# Create segmented barplot\nggplot(data = dogs_top5, \n       mapping = aes(x = fct_infreq(Dog_Name),\n                     fill = Breed)) +\n  geom_bar() +\n  theme(legend.pos = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nEach bar is divided into the frequencies of the fill variable.\nHard to make comparisons across categories."
  },
  {
    "objectID": "stat100_wk03mon.html#segmented-barplots-1",
    "href": "stat100_wk03mon.html#segmented-barplots-1",
    "title": "Stat 100",
    "section": "Segmented Barplots",
    "text": "Segmented Barplots\n\n\n# Create segmented barplot\nggplot(data = dogs_top5, \n       mapping = aes(x = fct_infreq(Dog_Name),\n                     fill = Breed)) +\n  geom_bar(position = \"dodge\") +\n  theme(legend.pos = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nCan add the position argument into the geom_bar()."
  },
  {
    "objectID": "stat100_wk03mon.html#segmented-barplots-2",
    "href": "stat100_wk03mon.html#segmented-barplots-2",
    "title": "Stat 100",
    "section": "Segmented Barplots",
    "text": "Segmented Barplots\n\n\n# Create segmented barplot\nggplot(data = dogs_top5, \n       mapping = aes(x = fct_infreq(Dog_Name),\n                     fill = Breed)) +\n  geom_bar(position = \"fill\") +\n  theme(legend.pos = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nNow each bar is divided into proportions based on the fill variable."
  },
  {
    "objectID": "stat100_wk03mon.html#adding-more-variables",
    "href": "stat100_wk03mon.html#adding-more-variables",
    "title": "Stat 100",
    "section": "Adding More Variables",
    "text": "Adding More Variables\n\nTwo main approaches:\n\nUtilize other aesthetics of the geom\nFacet: Create multiple plots across the categories of a categorical variable."
  },
  {
    "objectID": "stat100_wk03mon.html#utilize-other-aesthetics",
    "href": "stat100_wk03mon.html#utilize-other-aesthetics",
    "title": "Stat 100",
    "section": "Utilize other aesthetics",
    "text": "Utilize other aesthetics\n\n\nggplot(data = july_2019,\n       mapping = aes(x = Time,\n                     y = Total,\n                     color = Occasion)) +\n  geom_line(size = 2) +\n  theme(legend.pos = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\nAlready saw how to add a third variable to a line graph (and a scatterplot) via color.\n\nCan also change size or type."
  },
  {
    "objectID": "stat100_wk03mon.html#facet",
    "href": "stat100_wk03mon.html#facet",
    "title": "Stat 100",
    "section": "Facet",
    "text": "Facet\n\n\nggplot(data = dogs_top5,\n       mapping = aes(x = Longitude_masked,\n                     y = Latitude_masked)) +\n  geom_point(size = 2) +\n  facet_wrap(~Dog_Name, ncol = 2)"
  },
  {
    "objectID": "stat100_wk03mon.html#facet-1",
    "href": "stat100_wk03mon.html#facet-1",
    "title": "Stat 100",
    "section": "Facet",
    "text": "Facet\n\n\nggplot(data = dogs_top5,\n       mapping = aes(x = Longitude_masked,\n                     y = Latitude_masked)) +\n  geom_point(size = 2)  +\n  facet_grid(Breed~Dog_Name)"
  },
  {
    "objectID": "stat100_wk03mon.html#consider-doing-both",
    "href": "stat100_wk03mon.html#consider-doing-both",
    "title": "Stat 100",
    "section": "Consider Doing Both!",
    "text": "Consider Doing Both!\n\n\nggplot(data = dogs_top5,\n       mapping = aes(x = Longitude_masked,\n                     y = Latitude_masked,\n                     color = Breed)) +\n  geom_point(size = 2) +\n  facet_wrap(~Dog_Name, ncol = 2)"
  },
  {
    "objectID": "stat100_wk03mon.html#adding-some-context",
    "href": "stat100_wk03mon.html#adding-some-context",
    "title": "Stat 100",
    "section": "Adding Some Context",
    "text": "Adding Some Context\n\n\nggplot(data = july_2019,\n       mapping = aes(x = Time,\n                     y = Total,\n                     color = Occasion)) +\n  geom_line(size = 2) +\n  theme(legend.pos = \"bottom\") +\n  labs(x = \"Time of Day\",\n       y = \"Number of Passes\",\n       color = \"What Type of Day?\",\n       caption = \"Data Collected by Eco-Totem\",\n       title = \"Cycling Patterns at Broadway Bike Counter\")"
  },
  {
    "objectID": "stat100_wk03mon.html#customizing-your-ggplot2-plots",
    "href": "stat100_wk03mon.html#customizing-your-ggplot2-plots",
    "title": "Stat 100",
    "section": "Customizing your ggplot2 Plots",
    "text": "Customizing your ggplot2 Plots\n\nThere are so many ways you can customize the look of your ggplot2 plots.\nLet’s look at some common changes:\n\nFussing with labels\nZooming in\nUsing multiple geoms\nColor!\nThemes"
  },
  {
    "objectID": "stat100_wk03mon.html#fussing-with-labels-rotate",
    "href": "stat100_wk03mon.html#fussing-with-labels-rotate",
    "title": "Stat 100",
    "section": "Fussing with Labels: Rotate",
    "text": "Fussing with Labels: Rotate\n\n\nggplot(data = dogs_top5,\n       mapping = aes(x = Longitude_masked,\n                     y = Latitude_masked)) +\n  geom_point(size = 2)  +\n  facet_grid(Breed~Dog_Name) + \n  theme(axis.text.x =\n          element_text(angle = 45,\n                       vjust = 1,\n                       hjust = 1))"
  },
  {
    "objectID": "stat100_wk03mon.html#zooming-in",
    "href": "stat100_wk03mon.html#zooming-in",
    "title": "Stat 100",
    "section": "Zooming In",
    "text": "Zooming In\n\n\nggplot(data = july_2019, \n       mapping = aes(x = Occasion, \n                     y = Total,\n                     fill = Occasion)) +\n  geom_boxplot() +\n  guides(fill = \"none\")"
  },
  {
    "objectID": "stat100_wk03mon.html#zooming-in-1",
    "href": "stat100_wk03mon.html#zooming-in-1",
    "title": "Stat 100",
    "section": "Zooming In",
    "text": "Zooming In\n\n\nggplot(data = july_2019, \n       mapping = aes(x = Occasion, \n                     y = Total)) +\n  geom_boxplot(fill = \"springgreen2\") +\n  guides(fill = \"none\") +\n  coord_cartesian(ylim = c(0, 40))  +\n  scale_fill_manual()"
  },
  {
    "objectID": "stat100_wk03mon.html#multiple-geoms",
    "href": "stat100_wk03mon.html#multiple-geoms",
    "title": "Stat 100",
    "section": "Multiple geoms",
    "text": "Multiple geoms\n\n\nggplot(data = july_2019, \n       mapping = aes(x = Occasion, \n                     y = Total,\n                     fill = Occasion)) +\n  geom_boxplot() +\n  guides(fill = \"none\") +\n  coord_cartesian(ylim = c(0, 40)) +\n  geom_jitter(width = 0.1,\n              height = 0, \n              alpha = 0.6)"
  },
  {
    "objectID": "stat100_wk03mon.html#multiple-geoms-1",
    "href": "stat100_wk03mon.html#multiple-geoms-1",
    "title": "Stat 100",
    "section": "Multiple geoms",
    "text": "Multiple geoms\n\n\nggplot(data = july_2019,\n       mapping = aes(x = Time,\n                     y = Total,\n                     color = Occasion)) +\n  geom_line(size = 2) +\n  theme(legend.pos = \"bottom\") +\n  geom_point(size = 3)"
  },
  {
    "objectID": "stat100_wk03mon.html#change-the-color",
    "href": "stat100_wk03mon.html#change-the-color",
    "title": "Stat 100",
    "section": "Change the Color",
    "text": "Change the Color\n\ncolors()\n\n  [1] \"white\"                \"aliceblue\"            \"antiquewhite\"        \n  [4] \"antiquewhite1\"        \"antiquewhite2\"        \"antiquewhite3\"       \n  [7] \"antiquewhite4\"        \"aquamarine\"           \"aquamarine1\"         \n [10] \"aquamarine2\"          \"aquamarine3\"          \"aquamarine4\"         \n [13] \"azure\"                \"azure1\"               \"azure2\"              \n [16] \"azure3\"               \"azure4\"               \"beige\"               \n [19] \"bisque\"               \"bisque1\"              \"bisque2\"             \n [22] \"bisque3\"              \"bisque4\"              \"black\"               \n [25] \"blanchedalmond\"       \"blue\"                 \"blue1\"               \n [28] \"blue2\"                \"blue3\"                \"blue4\"               \n [31] \"blueviolet\"           \"brown\"                \"brown1\"              \n [34] \"brown2\"               \"brown3\"               \"brown4\"              \n [37] \"burlywood\"            \"burlywood1\"           \"burlywood2\"          \n [40] \"burlywood3\"           \"burlywood4\"           \"cadetblue\"           \n [43] \"cadetblue1\"           \"cadetblue2\"           \"cadetblue3\"          \n [46] \"cadetblue4\"           \"chartreuse\"           \"chartreuse1\"         \n [49] \"chartreuse2\"          \"chartreuse3\"          \"chartreuse4\"         \n [52] \"chocolate\"            \"chocolate1\"           \"chocolate2\"          \n [55] \"chocolate3\"           \"chocolate4\"           \"coral\"               \n [58] \"coral1\"               \"coral2\"               \"coral3\"              \n [61] \"coral4\"               \"cornflowerblue\"       \"cornsilk\"            \n [64] \"cornsilk1\"            \"cornsilk2\"            \"cornsilk3\"           \n [67] \"cornsilk4\"            \"cyan\"                 \"cyan1\"               \n [70] \"cyan2\"                \"cyan3\"                \"cyan4\"               \n [73] \"darkblue\"             \"darkcyan\"             \"darkgoldenrod\"       \n [76] \"darkgoldenrod1\"       \"darkgoldenrod2\"       \"darkgoldenrod3\"      \n [79] \"darkgoldenrod4\"       \"darkgray\"             \"darkgreen\"           \n [82] \"darkgrey\"             \"darkkhaki\"            \"darkmagenta\"         \n [85] \"darkolivegreen\"       \"darkolivegreen1\"      \"darkolivegreen2\"     \n [88] \"darkolivegreen3\"      \"darkolivegreen4\"      \"darkorange\"          \n [91] \"darkorange1\"          \"darkorange2\"          \"darkorange3\"         \n [94] \"darkorange4\"          \"darkorchid\"           \"darkorchid1\"         \n [97] \"darkorchid2\"          \"darkorchid3\"          \"darkorchid4\"         \n[100] \"darkred\"              \"darksalmon\"           \"darkseagreen\"        \n[103] \"darkseagreen1\"        \"darkseagreen2\"        \"darkseagreen3\"       \n[106] \"darkseagreen4\"        \"darkslateblue\"        \"darkslategray\"       \n[109] \"darkslategray1\"       \"darkslategray2\"       \"darkslategray3\"      \n[112] \"darkslategray4\"       \"darkslategrey\"        \"darkturquoise\"       \n[115] \"darkviolet\"           \"deeppink\"             \"deeppink1\"           \n[118] \"deeppink2\"            \"deeppink3\"            \"deeppink4\"           \n[121] \"deepskyblue\"          \"deepskyblue1\"         \"deepskyblue2\"        \n[124] \"deepskyblue3\"         \"deepskyblue4\"         \"dimgray\"             \n[127] \"dimgrey\"              \"dodgerblue\"           \"dodgerblue1\"         \n[130] \"dodgerblue2\"          \"dodgerblue3\"          \"dodgerblue4\"         \n[133] \"firebrick\"            \"firebrick1\"           \"firebrick2\"          \n[136] \"firebrick3\"           \"firebrick4\"           \"floralwhite\"         \n[139] \"forestgreen\"          \"gainsboro\"            \"ghostwhite\"          \n[142] \"gold\"                 \"gold1\"                \"gold2\"               \n[145] \"gold3\"                \"gold4\"                \"goldenrod\"           \n[148] \"goldenrod1\"           \"goldenrod2\"           \"goldenrod3\"          \n[151] \"goldenrod4\"           \"gray\"                 \"gray0\"               \n[154] \"gray1\"                \"gray2\"                \"gray3\"               \n[157] \"gray4\"                \"gray5\"                \"gray6\"               \n[160] \"gray7\"                \"gray8\"                \"gray9\"               \n[163] \"gray10\"               \"gray11\"               \"gray12\"              \n[166] \"gray13\"               \"gray14\"               \"gray15\"              \n[169] \"gray16\"               \"gray17\"               \"gray18\"              \n[172] \"gray19\"               \"gray20\"               \"gray21\"              \n[175] \"gray22\"               \"gray23\"               \"gray24\"              \n[178] \"gray25\"               \"gray26\"               \"gray27\"              \n[181] \"gray28\"               \"gray29\"               \"gray30\"              \n[184] \"gray31\"               \"gray32\"               \"gray33\"              \n[187] \"gray34\"               \"gray35\"               \"gray36\"              \n[190] \"gray37\"               \"gray38\"               \"gray39\"              \n[193] \"gray40\"               \"gray41\"               \"gray42\"              \n[196] \"gray43\"               \"gray44\"               \"gray45\"              \n[199] \"gray46\"               \"gray47\"               \"gray48\"              \n[202] \"gray49\"               \"gray50\"               \"gray51\"              \n[205] \"gray52\"               \"gray53\"               \"gray54\"              \n[208] \"gray55\"               \"gray56\"               \"gray57\"              \n[211] \"gray58\"               \"gray59\"               \"gray60\"              \n[214] \"gray61\"               \"gray62\"               \"gray63\"              \n[217] \"gray64\"               \"gray65\"               \"gray66\"              \n[220] \"gray67\"               \"gray68\"               \"gray69\"              \n[223] \"gray70\"               \"gray71\"               \"gray72\"              \n[226] \"gray73\"               \"gray74\"               \"gray75\"              \n[229] \"gray76\"               \"gray77\"               \"gray78\"              \n[232] \"gray79\"               \"gray80\"               \"gray81\"              \n[235] \"gray82\"               \"gray83\"               \"gray84\"              \n[238] \"gray85\"               \"gray86\"               \"gray87\"              \n[241] \"gray88\"               \"gray89\"               \"gray90\"              \n[244] \"gray91\"               \"gray92\"               \"gray93\"              \n[247] \"gray94\"               \"gray95\"               \"gray96\"              \n[250] \"gray97\"               \"gray98\"               \"gray99\"              \n[253] \"gray100\"              \"green\"                \"green1\"              \n[256] \"green2\"               \"green3\"               \"green4\"              \n[259] \"greenyellow\"          \"grey\"                 \"grey0\"               \n[262] \"grey1\"                \"grey2\"                \"grey3\"               \n[265] \"grey4\"                \"grey5\"                \"grey6\"               \n[268] \"grey7\"                \"grey8\"                \"grey9\"               \n[271] \"grey10\"               \"grey11\"               \"grey12\"              \n[274] \"grey13\"               \"grey14\"               \"grey15\"              \n[277] \"grey16\"               \"grey17\"               \"grey18\"              \n[280] \"grey19\"               \"grey20\"               \"grey21\"              \n[283] \"grey22\"               \"grey23\"               \"grey24\"              \n[286] \"grey25\"               \"grey26\"               \"grey27\"              \n[289] \"grey28\"               \"grey29\"               \"grey30\"              \n[292] \"grey31\"               \"grey32\"               \"grey33\"              \n[295] \"grey34\"               \"grey35\"               \"grey36\"              \n[298] \"grey37\"               \"grey38\"               \"grey39\"              \n[301] \"grey40\"               \"grey41\"               \"grey42\"              \n[304] \"grey43\"               \"grey44\"               \"grey45\"              \n[307] \"grey46\"               \"grey47\"               \"grey48\"              \n[310] \"grey49\"               \"grey50\"               \"grey51\"              \n[313] \"grey52\"               \"grey53\"               \"grey54\"              \n[316] \"grey55\"               \"grey56\"               \"grey57\"              \n[319] \"grey58\"               \"grey59\"               \"grey60\"              \n[322] \"grey61\"               \"grey62\"               \"grey63\"              \n[325] \"grey64\"               \"grey65\"               \"grey66\"              \n[328] \"grey67\"               \"grey68\"               \"grey69\"              \n[331] \"grey70\"               \"grey71\"               \"grey72\"              \n[334] \"grey73\"               \"grey74\"               \"grey75\"              \n[337] \"grey76\"               \"grey77\"               \"grey78\"              \n[340] \"grey79\"               \"grey80\"               \"grey81\"              \n[343] \"grey82\"               \"grey83\"               \"grey84\"              \n[346] \"grey85\"               \"grey86\"               \"grey87\"              \n[349] \"grey88\"               \"grey89\"               \"grey90\"              \n[352] \"grey91\"               \"grey92\"               \"grey93\"              \n[355] \"grey94\"               \"grey95\"               \"grey96\"              \n[358] \"grey97\"               \"grey98\"               \"grey99\"              \n[361] \"grey100\"              \"honeydew\"             \"honeydew1\"           \n[364] \"honeydew2\"            \"honeydew3\"            \"honeydew4\"           \n[367] \"hotpink\"              \"hotpink1\"             \"hotpink2\"            \n[370] \"hotpink3\"             \"hotpink4\"             \"indianred\"           \n[373] \"indianred1\"           \"indianred2\"           \"indianred3\"          \n[376] \"indianred4\"           \"ivory\"                \"ivory1\"              \n[379] \"ivory2\"               \"ivory3\"               \"ivory4\"              \n[382] \"khaki\"                \"khaki1\"               \"khaki2\"              \n[385] \"khaki3\"               \"khaki4\"               \"lavender\"            \n[388] \"lavenderblush\"        \"lavenderblush1\"       \"lavenderblush2\"      \n[391] \"lavenderblush3\"       \"lavenderblush4\"       \"lawngreen\"           \n[394] \"lemonchiffon\"         \"lemonchiffon1\"        \"lemonchiffon2\"       \n[397] \"lemonchiffon3\"        \"lemonchiffon4\"        \"lightblue\"           \n[400] \"lightblue1\"           \"lightblue2\"           \"lightblue3\"          \n[403] \"lightblue4\"           \"lightcoral\"           \"lightcyan\"           \n[406] \"lightcyan1\"           \"lightcyan2\"           \"lightcyan3\"          \n[409] \"lightcyan4\"           \"lightgoldenrod\"       \"lightgoldenrod1\"     \n[412] \"lightgoldenrod2\"      \"lightgoldenrod3\"      \"lightgoldenrod4\"     \n[415] \"lightgoldenrodyellow\" \"lightgray\"            \"lightgreen\"          \n[418] \"lightgrey\"            \"lightpink\"            \"lightpink1\"          \n[421] \"lightpink2\"           \"lightpink3\"           \"lightpink4\"          \n[424] \"lightsalmon\"          \"lightsalmon1\"         \"lightsalmon2\"        \n[427] \"lightsalmon3\"         \"lightsalmon4\"         \"lightseagreen\"       \n[430] \"lightskyblue\"         \"lightskyblue1\"        \"lightskyblue2\"       \n[433] \"lightskyblue3\"        \"lightskyblue4\"        \"lightslateblue\"      \n[436] \"lightslategray\"       \"lightslategrey\"       \"lightsteelblue\"      \n[439] \"lightsteelblue1\"      \"lightsteelblue2\"      \"lightsteelblue3\"     \n[442] \"lightsteelblue4\"      \"lightyellow\"          \"lightyellow1\"        \n[445] \"lightyellow2\"         \"lightyellow3\"         \"lightyellow4\"        \n[448] \"limegreen\"            \"linen\"                \"magenta\"             \n[451] \"magenta1\"             \"magenta2\"             \"magenta3\"            \n[454] \"magenta4\"             \"maroon\"               \"maroon1\"             \n[457] \"maroon2\"              \"maroon3\"              \"maroon4\"             \n[460] \"mediumaquamarine\"     \"mediumblue\"           \"mediumorchid\"        \n[463] \"mediumorchid1\"        \"mediumorchid2\"        \"mediumorchid3\"       \n[466] \"mediumorchid4\"        \"mediumpurple\"         \"mediumpurple1\"       \n[469] \"mediumpurple2\"        \"mediumpurple3\"        \"mediumpurple4\"       \n[472] \"mediumseagreen\"       \"mediumslateblue\"      \"mediumspringgreen\"   \n[475] \"mediumturquoise\"      \"mediumvioletred\"      \"midnightblue\"        \n[478] \"mintcream\"            \"mistyrose\"            \"mistyrose1\"          \n[481] \"mistyrose2\"           \"mistyrose3\"           \"mistyrose4\"          \n[484] \"moccasin\"             \"navajowhite\"          \"navajowhite1\"        \n[487] \"navajowhite2\"         \"navajowhite3\"         \"navajowhite4\"        \n[490] \"navy\"                 \"navyblue\"             \"oldlace\"             \n[493] \"olivedrab\"            \"olivedrab1\"           \"olivedrab2\"          \n[496] \"olivedrab3\"           \"olivedrab4\"           \"orange\"              \n[499] \"orange1\"              \"orange2\"              \"orange3\"             \n[502] \"orange4\"              \"orangered\"            \"orangered1\"          \n[505] \"orangered2\"           \"orangered3\"           \"orangered4\"          \n[508] \"orchid\"               \"orchid1\"              \"orchid2\"             \n[511] \"orchid3\"              \"orchid4\"              \"palegoldenrod\"       \n[514] \"palegreen\"            \"palegreen1\"           \"palegreen2\"          \n[517] \"palegreen3\"           \"palegreen4\"           \"paleturquoise\"       \n[520] \"paleturquoise1\"       \"paleturquoise2\"       \"paleturquoise3\"      \n[523] \"paleturquoise4\"       \"palevioletred\"        \"palevioletred1\"      \n[526] \"palevioletred2\"       \"palevioletred3\"       \"palevioletred4\"      \n[529] \"papayawhip\"           \"peachpuff\"            \"peachpuff1\"          \n[532] \"peachpuff2\"           \"peachpuff3\"           \"peachpuff4\"          \n[535] \"peru\"                 \"pink\"                 \"pink1\"               \n[538] \"pink2\"                \"pink3\"                \"pink4\"               \n[541] \"plum\"                 \"plum1\"                \"plum2\"               \n[544] \"plum3\"                \"plum4\"                \"powderblue\"          \n[547] \"purple\"               \"purple1\"              \"purple2\"             \n[550] \"purple3\"              \"purple4\"              \"red\"                 \n[553] \"red1\"                 \"red2\"                 \"red3\"                \n[556] \"red4\"                 \"rosybrown\"            \"rosybrown1\"          \n[559] \"rosybrown2\"           \"rosybrown3\"           \"rosybrown4\"          \n[562] \"royalblue\"            \"royalblue1\"           \"royalblue2\"          \n[565] \"royalblue3\"           \"royalblue4\"           \"saddlebrown\"         \n[568] \"salmon\"               \"salmon1\"              \"salmon2\"             \n[571] \"salmon3\"              \"salmon4\"              \"sandybrown\"          \n[574] \"seagreen\"             \"seagreen1\"            \"seagreen2\"           \n[577] \"seagreen3\"            \"seagreen4\"            \"seashell\"            \n[580] \"seashell1\"            \"seashell2\"            \"seashell3\"           \n[583] \"seashell4\"            \"sienna\"               \"sienna1\"             \n[586] \"sienna2\"              \"sienna3\"              \"sienna4\"             \n[589] \"skyblue\"              \"skyblue1\"             \"skyblue2\"            \n[592] \"skyblue3\"             \"skyblue4\"             \"slateblue\"           \n[595] \"slateblue1\"           \"slateblue2\"           \"slateblue3\"          \n[598] \"slateblue4\"           \"slategray\"            \"slategray1\"          \n[601] \"slategray2\"           \"slategray3\"           \"slategray4\"          \n[604] \"slategrey\"            \"snow\"                 \"snow1\"               \n[607] \"snow2\"                \"snow3\"                \"snow4\"               \n[610] \"springgreen\"          \"springgreen1\"         \"springgreen2\"        \n[613] \"springgreen3\"         \"springgreen4\"         \"steelblue\"           \n[616] \"steelblue1\"           \"steelblue2\"           \"steelblue3\"          \n[619] \"steelblue4\"           \"tan\"                  \"tan1\"                \n[622] \"tan2\"                 \"tan3\"                 \"tan4\"                \n[625] \"thistle\"              \"thistle1\"             \"thistle2\"            \n[628] \"thistle3\"             \"thistle4\"             \"tomato\"              \n[631] \"tomato1\"              \"tomato2\"              \"tomato3\"             \n[634] \"tomato4\"              \"turquoise\"            \"turquoise1\"          \n[637] \"turquoise2\"           \"turquoise3\"           \"turquoise4\"          \n[640] \"violet\"               \"violetred\"            \"violetred1\"          \n[643] \"violetred2\"           \"violetred3\"           \"violetred4\"          \n[646] \"wheat\"                \"wheat1\"               \"wheat2\"              \n[649] \"wheat3\"               \"wheat4\"               \"whitesmoke\"          \n[652] \"yellow\"               \"yellow1\"              \"yellow2\"             \n[655] \"yellow3\"              \"yellow4\"              \"yellowgreen\""
  },
  {
    "objectID": "stat100_wk03mon.html#change-the-color-1",
    "href": "stat100_wk03mon.html#change-the-color-1",
    "title": "Stat 100",
    "section": "Change the Color",
    "text": "Change the Color\n\n\nggplot(data = july_2019,\n       mapping = aes(x = Time,\n                     y = Total,\n                     color = Occasion)) +\n  geom_line(size = 2) +\n  theme(legend.pos = \"bottom\") +\n  scale_color_manual(values = c(\"violetred2\",\n                                \"steelblue4\"))"
  },
  {
    "objectID": "stat100_wk03mon.html#change-the-color-2",
    "href": "stat100_wk03mon.html#change-the-color-2",
    "title": "Stat 100",
    "section": "Change the Color",
    "text": "Change the Color\n\n\nggplot(data = july_2019,\n       mapping = aes(x = Time,\n                     y = Total,\n                     color = Occasion)) +\n  geom_line(size = 2) +\n  theme(legend.pos = \"bottom\") +\n  scale_color_manual(values = c(\"#0D6759\",\n                                \"#E4844A\"))"
  },
  {
    "objectID": "stat100_wk03mon.html#use-a-different-theme",
    "href": "stat100_wk03mon.html#use-a-different-theme",
    "title": "Stat 100",
    "section": "Use a Different Theme",
    "text": "Use a Different Theme\n\n\nggplot(data = july_2019,\n       mapping = aes(x = Time,\n                     y = Total,\n                     color = Occasion)) +\n  geom_line(size = 2) +\n  scale_color_manual(values = c(\"#0D6759\",\n                                \"#E4844A\")) +\n  theme_bw() +\n  theme(legend.pos = \"bottom\")"
  },
  {
    "objectID": "stat100_wk03mon.html#reminders",
    "href": "stat100_wk03mon.html#reminders",
    "title": "Stat 100",
    "section": "Reminders",
    "text": "Reminders\n\nWith COVID working its way through campus right now, make sure to check the Sections spreadsheet and the Office hours spreadsheet for updates!\nGrab a postcard and/or a stamp from SC 316 if you lost yours.\n\nWe also have markers, colored pencils, and crayons!\n\nDon’t forget that P-Set 1 due on Tuesday by 5pm in Gradescope.\nCome by office hours with any questions."
  },
  {
    "objectID": "stat100_wk11wed.html#announcements",
    "href": "stat100_wk11wed.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nOnly Thursday wrap-ups this week!\nNo sections or wrap-ups during Thanksgiving Week.\nOH schedule for Thanksgiving Week:\n\nSun, Nov 19th - Tues, Nov 21st: Happening with some modifications\nNo OHs Wed, Nov 22nd - Sun, Nov 26th!\n\n\nGoals for Today\n\n\n\nLearn about conditional probabilities\nCover continuous random variables\n\n\n\nLearn important named random variables\nDiscuss the Central Limit Theorem"
  },
  {
    "objectID": "stat100_wk11wed.html#example",
    "href": "stat100_wk11wed.html#example",
    "title": "Stat 100",
    "section": "Example",
    "text": "Example\nTesting for COVID-19 was an important part of the Keep Harvard Healthy Program. There are a variety of COVID-19 tests out there but for this problem let’s assume the following:\n\n\nThe test gives a false negative result 13% of the time where a false negative case is a person with COVID-19 but the test says they don’t have it.\nThe test gives a false positive result 5% of the time where a false positive case is a person who doesn’t have COVID-19 but the test says they do.\n\n\nLet’s assume the true prevalence is 1%. During the 2021-2022 school year, each week they tested about 30,000 Harvard affiliates. Use the assumed percentages to fill in the following table of potential outcomes:\n\n\n\n\n\n\n\n\n\n\nPositive Test Result\nNegative Test Result\nTotal\n\n\n\n\nActually have COVID-19\n\n\n\n\n\nActually don’t have COVID-19\n\n\n\n\n\nTotal\n\n\n30,000\n\n\n\n\n\nP(test - | have COVID) =\nP(have COVID | test -) =\n\nP(test + | don’t have COVID) =\nP(don’t have COVID | test +) ="
  },
  {
    "objectID": "stat100_wk11wed.html#example-1",
    "href": "stat100_wk11wed.html#example-1",
    "title": "Stat 100",
    "section": "Example",
    "text": "Example\nThe false negative rate of COVID-19 tests have varied wildly. One paper estimated it could be as high as 54%.\nRecreate the table with this new false negative rate.\n\n\n\n\n\n\n\n\n\n\nPositive Test Result\nNegative Test Result\nTotal\n\n\n\n\nActually have COVID-19\n\n\n\n\n\nActually don’t have COVID-19\n\n\n\n\n\nTotal\n\n\n30,000\n\n\n\n\n\nP(test - | have COVID) =\nP(have COVID | test -) =\n\nP(test + | don’t have COVID) =\nP(don’t have COVID | test +) ="
  },
  {
    "objectID": "stat100_wk11wed.html#reminders",
    "href": "stat100_wk11wed.html#reminders",
    "title": "Stat 100",
    "section": "Reminders:",
    "text": "Reminders:\n\n\nNo sections or wrap-ups during Thanksgiving Week.\nOH schedule for Thanksgiving Week:\n\nSun, Nov 19th - Tues, Nov 21st: Happening with some modifications\nNo OHs Wed, Nov 22nd - Sun, Nov 26th!"
  },
  {
    "objectID": "stat100_wk07mon.html#announcements",
    "href": "stat100_wk07mon.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nThis Wednesday we will be in Sanders Hall instead of Hall C.\nBack to a normal schedule.\n\nHave section & wrap-ups this week!\n\nNotes on the midterm.\n\nGoals for Today\n\n\n\nRecap: Simple linear regression model\nBroadening our idea of linear regression\n\n\n\nRegression with a single, categorical explanatory variable\nRegression with multiple explanatory variables"
  },
  {
    "objectID": "stat100_wk07mon.html#example",
    "href": "stat100_wk07mon.html#example",
    "title": "Stat 100",
    "section": "Example",
    "text": "Example\nMeadowfoam is a plant that grows in the Pacific Northwest and is harvested for its seed oil. In a randomized experiment, researchers at Oregon State University looked at how two light-related factors influenced the number of flowers per meadowfoam plant, the primary measure of productivity for this plant. The two light measures were light intensity (in mmol/ \\(m^2\\) /sec) and the timing of onset of the light (early or late in terms of photo periodic floral induction).\n\nResponse variable:\n\nExplanatory variables:\n\nModel Form:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 100: Introduction to Statistics and Data Science",
    "section": "",
    "text": "The lecture slides (both in HTML and PDF) will be posted here.\n\n\n\n\nClass Period\nHTML Slides\nPDF Slides\n\n\n\n\nWeek 1, Wed\n\n\n\n\nWeek 2, Mon\n\n\n\n\nWeek 2, Wed\n\n\n\n\nWeek 3, Mon\n\n\n\n\nWeek 3, Wed\n\n\n\n\nWeek 4, Mon\n\n\n\n\nWeek 4, Wed\n\n\n\n\nWeek 5, Mon\n\n\n\n\nWeek 5, Wed\n\n\n\n\nWeek 7, Mon\n\n\n\n\nWeek 7, Wed\n\n\n\n\nWeek 8, Mon\n\n\n\n\nWeek 8, Wed\n\n\n\n\nWeek 9, Mon\n\n\n\n\nWeek 9, Wed\n\n\n\n\nWeek 10, Mon\n\n\n\n\nWeek 10, Wed\n\n\n\n\nWeek 11, Mon\n\n\n\n\nWeek 11, Wed\n\n\n\n\nWeek 12, Mon"
  },
  {
    "objectID": "stat100_wk04mon.html#announcements",
    "href": "stat100_wk04mon.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nStarting 1-on-1, virtual Office Hours\n\n15 minute appointments, max 30 minutes per week\nFor conceptual, not p-set, questions\n\n\nGoals for Today\n\n\n\nMore data wrangling\n\n\n\nData joins"
  },
  {
    "objectID": "stat100_wk04mon.html#load-necessary-packages",
    "href": "stat100_wk04mon.html#load-necessary-packages",
    "title": "Stat 100",
    "section": "Load Necessary Packages",
    "text": "Load Necessary Packages\n\n\n\n\n\ndplyr is part of this collection of data science packages.\n\n# Load necessary packages\nlibrary(tidyverse)"
  },
  {
    "objectID": "stat100_wk04mon.html#data-setting-bureau-of-labor-statistics-bls-consumer-expenditure-survey",
    "href": "stat100_wk04mon.html#data-setting-bureau-of-labor-statistics-bls-consumer-expenditure-survey",
    "title": "Stat 100",
    "section": "Data Setting: Bureau of Labor Statistics (BLS) Consumer Expenditure Survey",
    "text": "Data Setting: Bureau of Labor Statistics (BLS) Consumer Expenditure Survey\nBLS Mission: “Measures labor market activity, working conditions, price changes, and productivity in the U.S. economy to support public and private decision making.”\nData: Last quarter of the 2016 BLS Consumer Expenditure Survey.\n\nlibrary(tidyverse)\n\nce_raw &lt;- read_csv(\"data/fmli.csv\", \n                 na = c(\"NA\", \".\"))\nglimpse(ce_raw)\n\nRows: 6,301\nColumns: 51\n$ NEWID    &lt;chr&gt; \"03324174\", \"03324204\", \"03324214\", \"03324244\", \"03324274\", \"…\n$ PRINEARN &lt;chr&gt; \"01\", \"01\", \"01\", \"01\", \"02\", \"01\", \"01\", \"01\", \"02\", \"01\", \"…\n$ FINLWT21 &lt;dbl&gt; 25984.767, 6581.018, 20208.499, 18078.372, 20111.619, 19907.3…\n$ FINCBTAX &lt;dbl&gt; 116920, 200, 117000, 0, 2000, 942, 0, 91000, 95000, 40037, 10…\n$ BLS_URBN &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ POPSIZE  &lt;dbl&gt; 2, 3, 4, 2, 2, 2, 1, 2, 5, 2, 3, 2, 2, 3, 4, 3, 3, 1, 4, 1, 1…\n$ EDUC_REF &lt;chr&gt; \"16\", \"15\", \"16\", \"15\", \"14\", \"11\", \"10\", \"13\", \"12\", \"12\", \"…\n$ EDUCA2   &lt;dbl&gt; 15, 15, 13, NA, NA, NA, NA, 15, 15, 14, 12, 12, NA, NA, NA, 1…\n$ AGE_REF  &lt;dbl&gt; 63, 50, 47, 37, 51, 63, 77, 37, 51, 64, 26, 59, 81, 51, 67, 4…\n$ AGE2     &lt;dbl&gt; 50, 47, 46, NA, NA, NA, NA, 36, 53, 67, 44, 62, NA, NA, NA, 4…\n$ SEX_REF  &lt;dbl&gt; 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1…\n$ SEX2     &lt;dbl&gt; 2, 2, 1, NA, NA, NA, NA, 2, 2, 1, 1, 1, NA, NA, NA, 1, NA, 1,…\n$ REF_RACE &lt;dbl&gt; 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1…\n$ RACE2    &lt;dbl&gt; 1, 4, 1, NA, NA, NA, NA, 1, 1, 1, 1, 1, NA, NA, NA, 2, NA, 1,…\n$ HISP_REF &lt;dbl&gt; 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1…\n$ HISP2    &lt;dbl&gt; 2, 2, 1, NA, NA, NA, NA, 2, 2, 2, 2, 2, NA, NA, NA, 2, NA, 2,…\n$ FAM_TYPE &lt;dbl&gt; 3, 4, 1, 8, 9, 9, 8, 3, 1, 1, 3, 1, 8, 9, 8, 5, 9, 4, 8, 3, 2…\n$ MARITAL1 &lt;dbl&gt; 1, 1, 1, 5, 3, 3, 2, 1, 1, 1, 1, 1, 2, 3, 5, 1, 3, 1, 3, 1, 1…\n$ REGION   &lt;dbl&gt; 4, 4, 3, 4, 4, 3, 4, 1, 3, 2, 1, 4, 1, 3, 3, 3, 2, 1, 2, 4, 3…\n$ SMSASTAT &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ HIGH_EDU &lt;chr&gt; \"16\", \"15\", \"16\", \"15\", \"14\", \"11\", \"10\", \"15\", \"15\", \"14\", \"…\n$ EHOUSNGC &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ TOTEXPCQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ FOODCQ   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ TRANSCQ  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ HEALTHCQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ENTERTCQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ EDUCACQ  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ TOBACCCQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ STUDFINX &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IRAX     &lt;dbl&gt; 1000000, 10000, 0, NA, NA, 0, 0, 15000, NA, 477000, NA, NA, N…\n$ CUTENURE &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 4, 1, 1, 2, 1, 2, 2, 2, 2, 4, 1, 1, 1, 4, 4…\n$ FAM_SIZE &lt;dbl&gt; 4, 6, 2, 1, 2, 2, 1, 5, 2, 2, 4, 2, 1, 2, 1, 4, 2, 4, 1, 3, 3…\n$ VEHQ     &lt;dbl&gt; 3, 5, 0, 4, 2, 0, 0, 2, 4, 2, 3, 2, 1, 3, 1, 2, 4, 4, 0, 2, 3…\n$ ROOMSQ   &lt;dbl&gt; 8, 5, 6, 4, 4, 4, 7, 5, 4, 9, 6, 10, 4, 7, 5, 6, 6, 8, 18, 4,…\n$ INC_HRS1 &lt;dbl&gt; 40, 40, 40, 44, 40, NA, NA, 40, 40, NA, 40, NA, NA, NA, NA, 4…\n$ INC_HRS2 &lt;dbl&gt; 30, 40, 52, NA, NA, NA, NA, 40, 40, NA, 65, NA, NA, NA, NA, 6…\n$ EARNCOMP &lt;dbl&gt; 3, 2, 2, 1, 4, 7, 8, 2, 2, 8, 2, 8, 8, 7, 8, 2, 7, 3, 1, 2, 1…\n$ NO_EARNR &lt;dbl&gt; 4, 2, 2, 1, 2, 1, 0, 2, 2, 0, 2, 0, 0, 1, 0, 2, 1, 3, 1, 2, 1…\n$ OCCUCOD1 &lt;chr&gt; \"03\", \"03\", \"05\", \"03\", \"04\", \"\", \"\", \"12\", \"04\", \"\", \"01\", \"…\n$ OCCUCOD2 &lt;chr&gt; \"04\", \"02\", \"01\", \"\", \"\", \"\", \"\", \"02\", \"03\", \"\", \"11\", \"\", \"…\n$ STATE    &lt;chr&gt; \"41\", \"15\", \"48\", \"06\", \"06\", \"48\", \"06\", \"42\", \"\", \"27\", \"25…\n$ DIVISION &lt;dbl&gt; 9, 9, 7, 9, 9, 7, 9, 2, NA, 4, 1, 8, 2, 5, 6, 7, 3, 2, 3, 9, …\n$ TOTXEST  &lt;dbl&gt; 15452, 11459, 15738, 25978, 588, 0, 0, 7261, 9406, -1414, 141…\n$ CREDFINX &lt;dbl&gt; 0, NA, 0, NA, 5, NA, NA, NA, NA, 0, NA, 0, NA, NA, NA, 2, 35,…\n$ CREDITB  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREDITX  &lt;dbl&gt; 4000, 5000, 2000, NA, 7000, 1800, NA, 6000, NA, 719, NA, 1200…\n$ BUILDING &lt;chr&gt; \"01\", \"01\", \"01\", \"02\", \"08\", \"01\", \"01\", \"01\", \"01\", \"01\", \"…\n$ ST_HOUS  &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ INT_PHON &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ INT_HOME &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…"
  },
  {
    "objectID": "stat100_wk04mon.html#wrangling-ce-data",
    "href": "stat100_wk04mon.html#wrangling-ce-data",
    "title": "Stat 100",
    "section": "Wrangling CE Data",
    "text": "Wrangling CE Data\n\n\nWant to better understand a family’s income and expenditures\n\nce &lt;- ce_raw %&gt;%\n  select(NEWID, PRINEARN, FINCBTAX,\n         BLS_URBN, HIGH_EDU, TOTEXPCQ, IRAX)\ndim(ce)\n\n[1] 6301    7\n\n\nVariables:\n\n\nNEWID: ID for the household\nPRINEARN: ID for which member of the household is the principal earner\nFINCBTAX: Final income before taxes for the year\n\n\n\n\n\nBLS_URBN: 1 = urban, 2 = rural\nHIGH_EDU: Highest education in the household. 00 = Never attended, 10 = Grades 1-8, 11 = Grades 9-12, no degree, 12 = High school graduate, 13 = Some college, no degree, 14 = Associates degree, 15 = Bachelor’s degree, 16 = Masters, Professional/doctorate degree\nTOTEXPCQ = Total household expenditures for the current quarter\nIRAX = Total in retirement funds"
  },
  {
    "objectID": "stat100_wk04mon.html#wrangling-ce-data-1",
    "href": "stat100_wk04mon.html#wrangling-ce-data-1",
    "title": "Stat 100",
    "section": "Wrangling CE Data",
    "text": "Wrangling CE Data\n\nce &lt;- ce %&gt;%\n  mutate(YEARLY_EXP = TOTEXPCQ*4)\nce\n\n# A tibble: 6,301 × 8\n   NEWID    PRINEARN FINCBTAX BLS_URBN HIGH_EDU TOTEXPCQ    IRAX YEARLY_EXP\n   &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 03324174 01         116920        1 16              0 1000000          0\n 2 03324204 01            200        1 15              0   10000          0\n 3 03324214 01         117000        1 16              0       0          0\n 4 03324244 01              0        1 15              0      NA          0\n 5 03324274 02           2000        1 14              0      NA          0\n 6 03324284 01            942        1 11              0       0          0\n 7 03324294 01              0        1 10              0       0          0\n 8 03324304 01          91000        1 15              0   15000          0\n 9 03324324 02          95000        2 15              0      NA          0\n10 03324334 01          40037        1 14              0  477000          0\n# ℹ 6,291 more rows"
  },
  {
    "objectID": "stat100_wk04mon.html#logical-operators",
    "href": "stat100_wk04mon.html#logical-operators",
    "title": "Stat 100",
    "section": "Logical Operators",
    "text": "Logical Operators\n\nce_sub &lt;- ce %&gt;%\n  filter(YEARLY_EXP &gt; 0, BLS_URBN == 1, HIGH_EDU != \"00\")\nce_sub\n\n# A tibble: 3,950 × 8\n   NEWID    PRINEARN FINCBTAX BLS_URBN HIGH_EDU TOTEXPCQ   IRAX YEARLY_EXP\n   &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n 1 03335204 01          37000        1 14          2492.      0      9968.\n 2 03335214 01         103000        1 16          6128.     NA     24513.\n 3 03335224 01          14686        1 13          1072.     NA      4287.\n 4 03335244 02          33396        1 12          1630       0      6520 \n 5 03335264 01              0        1 13          3213.     NA     12853.\n 6 03335274 01              0        1 15          4674.      0     18694.\n 7 03335294 01         745136        1 16          8693. 280000     34773.\n 8 03335304 01          36000        1 16          3733.     NA     14933.\n 9 03335314 02          45000        1 15          3627.   3000     14509 \n10 03335334 01          20862        1 13           802.      0      3209.\n# ℹ 3,940 more rows"
  },
  {
    "objectID": "stat100_wk04mon.html#logical-operators-1",
    "href": "stat100_wk04mon.html#logical-operators-1",
    "title": "Stat 100",
    "section": "Logical Operators",
    "text": "Logical Operators\n\nce_sub &lt;- ce %&gt;%\n  filter(YEARLY_EXP &gt; 0, (BLS_URBN == 1 | HIGH_EDU != \"00\"))\nce_sub\n\n# A tibble: 4,178 × 8\n   NEWID    PRINEARN FINCBTAX BLS_URBN HIGH_EDU TOTEXPCQ   IRAX YEARLY_EXP\n   &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n 1 03335204 01          37000        1 14          2492.      0      9968.\n 2 03335214 01         103000        1 16          6128.     NA     24513.\n 3 03335224 01          14686        1 13          1072.     NA      4287.\n 4 03335244 02          33396        1 12          1630       0      6520 \n 5 03335264 01              0        1 13          3213.     NA     12853.\n 6 03335274 01              0        1 15          4674.      0     18694.\n 7 03335294 01         745136        1 16          8693. 280000     34773.\n 8 03335304 01          36000        1 16          3733.     NA     14933.\n 9 03335314 02          45000        1 15          3627.   3000     14509 \n10 03335334 01          20862        1 13           802.      0      3209.\n# ℹ 4,168 more rows"
  },
  {
    "objectID": "stat100_wk04mon.html#case_when-recoding-variables",
    "href": "stat100_wk04mon.html#case_when-recoding-variables",
    "title": "Stat 100",
    "section": "case_when: Recoding Variables",
    "text": "case_when: Recoding Variables\n\n\n\ncount(ce, BLS_URBN)\n\n# A tibble: 2 × 2\n  BLS_URBN     n\n     &lt;dbl&gt; &lt;int&gt;\n1        1  5952\n2        2   349\n\n\n\n\nce &lt;- ce %&gt;%\n  mutate(BLS_URBN = case_when(\n    BLS_URBN == 1 ~ \"Urban\",\n    BLS_URBN == 2 ~ \"Rural\"\n  ))\ncount(ce, BLS_URBN)\n\n# A tibble: 2 × 2\n  BLS_URBN     n\n  &lt;chr&gt;    &lt;int&gt;\n1 Rural      349\n2 Urban     5952"
  },
  {
    "objectID": "stat100_wk04mon.html#case_when-creating-variables",
    "href": "stat100_wk04mon.html#case_when-creating-variables",
    "title": "Stat 100",
    "section": "case_when: Creating Variables",
    "text": "case_when: Creating Variables\n\n\n\ncount(ce, HIGH_EDU)\n\n# A tibble: 8 × 2\n  HIGH_EDU     n\n  &lt;chr&gt;    &lt;int&gt;\n1 00           8\n2 10         110\n3 11         302\n4 12        1272\n5 13        1297\n6 14         714\n7 15        1528\n8 16        1070\n\nce &lt;- ce %&gt;%\n  mutate(HIGH_EDU = as.numeric(HIGH_EDU))\ncount(ce, HIGH_EDU)\n\n# A tibble: 8 × 2\n  HIGH_EDU     n\n     &lt;dbl&gt; &lt;int&gt;\n1        0     8\n2       10   110\n3       11   302\n4       12  1272\n5       13  1297\n6       14   714\n7       15  1528\n8       16  1070\n\n\n\n\nce &lt;- ce %&gt;%\n  mutate(HIGH_EDU2 = case_when(\n    is.na(HIGH_EDU) ~ NA,\n    HIGH_EDU &lt;= 11 ~ \"Less than high school degree\",\n    between(HIGH_EDU, 12, 13) ~ \"High school degree\",\n    HIGH_EDU &gt;= 14 ~ \"College degree\"\n  ))\ncount(ce, HIGH_EDU2)\n\n# A tibble: 3 × 2\n  HIGH_EDU2                        n\n  &lt;chr&gt;                        &lt;int&gt;\n1 College degree                3312\n2 High school degree            2569\n3 Less than high school degree   420"
  },
  {
    "objectID": "stat100_wk04mon.html#variable-names",
    "href": "stat100_wk04mon.html#variable-names",
    "title": "Stat 100",
    "section": "Variable Names",
    "text": "Variable Names\nSometimes datasets come with terrible variable names.\n\nce &lt;- ce %&gt;%\n  rename(INCOME = FINCBTAX)\nce\n\n# A tibble: 6,301 × 9\n   NEWID PRINEARN INCOME BLS_URBN HIGH_EDU TOTEXPCQ    IRAX YEARLY_EXP HIGH_EDU2\n   &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    \n 1 0332… 01       116920 Urban          16        0 1000000          0 College …\n 2 0332… 01          200 Urban          15        0   10000          0 College …\n 3 0332… 01       117000 Urban          16        0       0          0 College …\n 4 0332… 01            0 Urban          15        0      NA          0 College …\n 5 0332… 02         2000 Urban          14        0      NA          0 College …\n 6 0332… 01          942 Urban          11        0       0          0 Less tha…\n 7 0332… 01            0 Urban          10        0       0          0 Less tha…\n 8 0332… 01        91000 Urban          15        0   15000          0 College …\n 9 0332… 02        95000 Rural          15        0      NA          0 College …\n10 0332… 01        40037 Urban          14        0  477000          0 College …\n# ℹ 6,291 more rows"
  },
  {
    "objectID": "stat100_wk04mon.html#handling-missing-data",
    "href": "stat100_wk04mon.html#handling-missing-data",
    "title": "Stat 100",
    "section": "Handling Missing Data",
    "text": "Handling Missing Data\nWant to compute mean income and mean retirement funds by location.\n\n\n\nce %&gt;%\n  group_by(BLS_URBN) %&gt;%\n  summarize(mean_INCOME = mean(INCOME),\n            mean_IRAX = mean(IRAX),\n            households = n())\n\n# A tibble: 2 × 4\n  BLS_URBN mean_INCOME mean_IRAX households\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;      &lt;int&gt;\n1 Rural         40440.        NA        349\n2 Urban         63772.        NA       5952\n\n\n\n\nce_aggressive &lt;- ce_raw %&gt;%\n  na.omit()\nce_aggressive\n\n# A tibble: 0 × 51\n# ℹ 51 variables: NEWID &lt;chr&gt;, PRINEARN &lt;chr&gt;, FINLWT21 &lt;dbl&gt;, FINCBTAX &lt;dbl&gt;,\n#   BLS_URBN &lt;dbl&gt;, POPSIZE &lt;dbl&gt;, EDUC_REF &lt;chr&gt;, EDUCA2 &lt;dbl&gt;, AGE_REF &lt;dbl&gt;,\n#   AGE2 &lt;dbl&gt;, SEX_REF &lt;dbl&gt;, SEX2 &lt;dbl&gt;, REF_RACE &lt;dbl&gt;, RACE2 &lt;dbl&gt;,\n#   HISP_REF &lt;dbl&gt;, HISP2 &lt;dbl&gt;, FAM_TYPE &lt;dbl&gt;, MARITAL1 &lt;dbl&gt;, REGION &lt;dbl&gt;,\n#   SMSASTAT &lt;dbl&gt;, HIGH_EDU &lt;chr&gt;, EHOUSNGC &lt;dbl&gt;, TOTEXPCQ &lt;dbl&gt;,\n#   FOODCQ &lt;dbl&gt;, TRANSCQ &lt;dbl&gt;, HEALTHCQ &lt;dbl&gt;, ENTERTCQ &lt;dbl&gt;, EDUCACQ &lt;dbl&gt;,\n#   TOBACCCQ &lt;dbl&gt;, STUDFINX &lt;dbl&gt;, IRAX &lt;dbl&gt;, CUTENURE &lt;dbl&gt;, …"
  },
  {
    "objectID": "stat100_wk04mon.html#handling-missing-data-1",
    "href": "stat100_wk04mon.html#handling-missing-data-1",
    "title": "Stat 100",
    "section": "Handling Missing Data",
    "text": "Handling Missing Data\n\n\n\nce_moderate &lt;- ce %&gt;%\n  drop_na(IRAX, INCOME, BLS_URBN) %&gt;%\n  group_by(BLS_URBN) %&gt;%  \n  summarize(mean_INCOME = mean(INCOME),\n            mean_IRAX = mean(IRAX),\n            households = n())\n\nce_moderate\n\n# A tibble: 2 × 4\n  BLS_URBN mean_INCOME mean_IRAX households\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;      &lt;int&gt;\n1 Rural         38651.    37008.         63\n2 Urban         58987.    94512.        991\n\n\n\n\nce_light &lt;- ce %&gt;%\n  group_by(BLS_URBN) %&gt;%\n  summarize(mean_INCOME = mean(INCOME, na.rm = TRUE),\n            mean_IRAX = mean(IRAX, na.rm = TRUE), \n            households = n())\n\nce_light\n\n# A tibble: 2 × 4\n  BLS_URBN mean_INCOME mean_IRAX households\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;      &lt;int&gt;\n1 Rural         40440.    37008.        349\n2 Urban         63772.    94512.       5952"
  },
  {
    "objectID": "stat100_wk04mon.html#multiple-groupings",
    "href": "stat100_wk04mon.html#multiple-groupings",
    "title": "Stat 100",
    "section": "Multiple Groupings",
    "text": "Multiple Groupings\n\nce %&gt;%\n  group_by(BLS_URBN, HIGH_EDU2) %&gt;%\n  summarize(mean_INCOME = mean(INCOME, na.rm = TRUE),\n            mean_IRAX = mean(IRAX, na.rm = TRUE), \n            households = n()) %&gt;%\n  arrange(mean_IRAX)\n\n# A tibble: 6 × 5\n# Groups:   BLS_URBN [2]\n  BLS_URBN HIGH_EDU2                    mean_INCOME mean_IRAX households\n  &lt;chr&gt;    &lt;chr&gt;                              &lt;dbl&gt;     &lt;dbl&gt;      &lt;int&gt;\n1 Rural    Less than high school degree      14715.        0          39\n2 Urban    Less than high school degree      23046.     8270.        381\n3 Rural    High school degree                31087.    15543.        192\n4 Urban    High school degree                39147.    30533.       2377\n5 Rural    College degree                    64161.   105148.        118\n6 Urban    College degree                    86957.   168767.       3194"
  },
  {
    "objectID": "stat100_wk04mon.html#piping-into-ggplot2",
    "href": "stat100_wk04mon.html#piping-into-ggplot2",
    "title": "Stat 100",
    "section": "Piping into ggplot2",
    "text": "Piping into ggplot2\n\n\nce %&gt;%\n  group_by(BLS_URBN, HIGH_EDU2) %&gt;%\n  summarize(mean_INCOME = mean(INCOME, na.rm = TRUE),\n            mean_IRAX = mean(IRAX, na.rm = TRUE), \n            households = n()) %&gt;%\n  ggplot(mapping = aes(x = mean_INCOME,\n                       y = mean_IRAX, \n                       shape = BLS_URBN,\n                       color = HIGH_EDU2)) +\n  geom_point(size = 5)"
  },
  {
    "objectID": "stat100_wk04mon.html#data-joins",
    "href": "stat100_wk04mon.html#data-joins",
    "title": "Stat 100",
    "section": "Data Joins",
    "text": "Data Joins\n\nOften in the data analysis workflow, we have more than one data source, which means more than one dataframe, and we want to combine these dataframes.\nNeed principled way to combine.\n\nNeed a key that links two dataframes together.\n\nThese multiple dataframes are called relational data."
  },
  {
    "objectID": "stat100_wk04mon.html#ce-data",
    "href": "stat100_wk04mon.html#ce-data",
    "title": "Stat 100",
    "section": "CE Data",
    "text": "CE Data\n\n\nHousehold survey but data are also collected on individuals\n\nfmli: household data\nmemi: household member-level data\n\n\n\n\nfmli &lt;- read_csv(\"data/fmli.csv\", \n                 na = c(\"NA\", \".\")) %&gt;%\n  select(NEWID, PRINEARN, FINCBTAX,\n         BLS_URBN, HIGH_EDU)\nmemi &lt;- read_csv(\"data/memi.csv\", \n                 na = c(\"NA\", \".\")) %&gt;%\n  select(NEWID, MEMBNO, AGE, SEX, EARNTYPE)\n\n\nWant to add variables on the principal earner from the member data frame to the household data frame"
  },
  {
    "objectID": "stat100_wk04mon.html#ce-data-1",
    "href": "stat100_wk04mon.html#ce-data-1",
    "title": "Stat 100",
    "section": "CE Data",
    "text": "CE Data\nKey variable(s)?\n\n\n\nfmli\n\n# A tibble: 6,301 × 5\n   NEWID    PRINEARN FINCBTAX BLS_URBN HIGH_EDU\n   &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n 1 03324174 01         116920        1 16      \n 2 03324204 01            200        1 15      \n 3 03324214 01         117000        1 16      \n 4 03324244 01              0        1 15      \n 5 03324274 02           2000        1 14      \n 6 03324284 01            942        1 11      \n 7 03324294 01              0        1 10      \n 8 03324304 01          91000        1 15      \n 9 03324324 02          95000        2 15      \n10 03324334 01          40037        1 14      \n# ℹ 6,291 more rows\n\n\n\n\nmemi\n\n# A tibble: 15,412 × 5\n   NEWID    MEMBNO   AGE   SEX EARNTYPE\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 03552611      1    58     2        2\n 2 03552641      1    54     1        1\n 3 03552641      2    49     2       NA\n 4 03552651      1    39     2       NA\n 5 03552651      2    10     2       NA\n 6 03552651      3    32     1       NA\n 7 03552651      4     7     1       NA\n 8 03552651      5     9     1       NA\n 9 03552681      1    38     1        3\n10 03552681      2    34     2       NA\n# ℹ 15,402 more rows"
  },
  {
    "objectID": "stat100_wk04mon.html#ce-data-2",
    "href": "stat100_wk04mon.html#ce-data-2",
    "title": "Stat 100",
    "section": "CE Data",
    "text": "CE Data\n\nKey variables?\n\nProblem with class?\n\n\n\nclass(fmli$NEWID)\n\n[1] \"character\"\n\nclass(memi$NEWID)\n\n[1] \"character\"\n\nclass(fmli$PRINEARN)\n\n[1] \"character\"\n\nclass(memi$MEMBNO)\n\n[1] \"numeric\""
  },
  {
    "objectID": "stat100_wk04mon.html#ce-data-3",
    "href": "stat100_wk04mon.html#ce-data-3",
    "title": "Stat 100",
    "section": "CE Data",
    "text": "CE Data\n\nKey variables?\n\nProblem with class?\n\n\n\nfmli &lt;- mutate(fmli, PRINEARN = as.integer(PRINEARN))\nclass(fmli$PRINEARN)\n\n[1] \"integer\"\n\nclass(memi$MEMBNO)\n\n[1] \"numeric\""
  },
  {
    "objectID": "stat100_wk04mon.html#ce-data-4",
    "href": "stat100_wk04mon.html#ce-data-4",
    "title": "Stat 100",
    "section": "CE Data",
    "text": "CE Data\n\nWant to add columns of memi to fmli that correspond to the principal earner’s memi data\n\nWhat type of join is that?"
  },
  {
    "objectID": "stat100_wk04mon.html#the-world-of-joins",
    "href": "stat100_wk04mon.html#the-world-of-joins",
    "title": "Stat 100",
    "section": "The World of Joins",
    "text": "The World of Joins\n\nMutating joins: Add new variables to one dataset from matching observations in another.\n\nleft_join() (and right_join())\ninner_join()\nfull_join()\n\nThere are also filtering joins but we won’t cover those today."
  },
  {
    "objectID": "stat100_wk04mon.html#example-dataframes",
    "href": "stat100_wk04mon.html#example-dataframes",
    "title": "Stat 100",
    "section": "Example Dataframes",
    "text": "Example Dataframes\nHere I created the data frames by hand.\n\nstaff &lt;- data.frame(member = c(\"Prof McConville\", \"Lety\", \"Kate\",\n                               \"Thor\", \"Mally\", \"Dylan\", \"Nick\"),\n                 Year = c(2006, 2024, 2023, 2025, 2025, 2025, 2025),\n                 Food = c(\"tikka masala\", \"chicken wings\", \"sushi\",\n                          \"Sun HUDS Brunch\", \"quesadillas\",\n                          \"shepards pie\", \"burgers\"),\n                 Neighborhood = c(\"Somerville\", \"River Central\", \"Quad\", \n                                  \"River East\", \"River Central\",\n                                  \"Quad\", \"River Central\"))\nhousing &lt;- data.frame(Neighborhoods = c(\"Yard\", \"River East\",\n                                        \"River Central\", \"River West\",\n                                        \"Quad\"),\n                      Steps = c(75, 600, 450, 1100, 1200))"
  },
  {
    "objectID": "stat100_wk04mon.html#example-dataframes-1",
    "href": "stat100_wk04mon.html#example-dataframes-1",
    "title": "Stat 100",
    "section": "Example Dataframes",
    "text": "Example Dataframes\n\nstaff\n\n           member Year            Food  Neighborhood\n1 Prof McConville 2006    tikka masala    Somerville\n2            Lety 2024   chicken wings River Central\n3            Kate 2023           sushi          Quad\n4            Thor 2025 Sun HUDS Brunch    River East\n5           Mally 2025     quesadillas River Central\n6           Dylan 2025    shepards pie          Quad\n7            Nick 2025         burgers River Central\n\nhousing\n\n  Neighborhoods Steps\n1          Yard    75\n2    River East   600\n3 River Central   450\n4    River West  1100\n5          Quad  1200"
  },
  {
    "objectID": "stat100_wk04mon.html#left_join",
    "href": "stat100_wk04mon.html#left_join",
    "title": "Stat 100",
    "section": "left_join()",
    "text": "left_join()\n\nstaff_new &lt;- left_join(staff, housing)\n\nError in `left_join()`:\n! `by` must be supplied when `x` and `y` have no common variables.\nℹ Use `cross_join()` to perform a cross-join.\n\nstaff_new\n\nError in eval(expr, envir, enclos): object 'staff_new' not found"
  },
  {
    "objectID": "stat100_wk04mon.html#left_join-1",
    "href": "stat100_wk04mon.html#left_join-1",
    "title": "Stat 100",
    "section": "left_join()",
    "text": "left_join()\n\nstaff_new &lt;- left_join(staff, housing, join_by(\"Neighborhood\" == \"Neighborhoods\"))\nstaff_new\n\n           member Year            Food  Neighborhood Steps\n1 Prof McConville 2006    tikka masala    Somerville    NA\n2            Lety 2024   chicken wings River Central   450\n3            Kate 2023           sushi          Quad  1200\n4            Thor 2025 Sun HUDS Brunch    River East   600\n5           Mally 2025     quesadillas River Central   450\n6           Dylan 2025    shepards pie          Quad  1200\n7            Nick 2025         burgers River Central   450"
  },
  {
    "objectID": "stat100_wk04mon.html#inner_join",
    "href": "stat100_wk04mon.html#inner_join",
    "title": "Stat 100",
    "section": "inner_join()",
    "text": "inner_join()\n\nstaff_housing &lt;- inner_join(staff, housing, join_by(\"Neighborhood\" == \"Neighborhoods\"))\nstaff_housing\n\n  member Year            Food  Neighborhood Steps\n1   Lety 2024   chicken wings River Central   450\n2   Kate 2023           sushi          Quad  1200\n3   Thor 2025 Sun HUDS Brunch    River East   600\n4  Mally 2025     quesadillas River Central   450\n5  Dylan 2025    shepards pie          Quad  1200\n6   Nick 2025         burgers River Central   450"
  },
  {
    "objectID": "stat100_wk04mon.html#full_join",
    "href": "stat100_wk04mon.html#full_join",
    "title": "Stat 100",
    "section": "full_join()",
    "text": "full_join()\n\nstaff_housing &lt;- full_join(staff, housing, join_by(\"Neighborhood\" == \"Neighborhoods\"))\nstaff_housing\n\n           member Year            Food  Neighborhood Steps\n1 Prof McConville 2006    tikka masala    Somerville    NA\n2            Lety 2024   chicken wings River Central   450\n3            Kate 2023           sushi          Quad  1200\n4            Thor 2025 Sun HUDS Brunch    River East   600\n5           Mally 2025     quesadillas River Central   450\n6           Dylan 2025    shepards pie          Quad  1200\n7            Nick 2025         burgers River Central   450\n8            &lt;NA&gt;   NA            &lt;NA&gt;          Yard    75\n9            &lt;NA&gt;   NA            &lt;NA&gt;    River West  1100"
  },
  {
    "objectID": "stat100_wk04mon.html#back-to-our-example",
    "href": "stat100_wk04mon.html#back-to-our-example",
    "title": "Stat 100",
    "section": "Back to our Example",
    "text": "Back to our Example\n\nWhat kind of join do we want for the Consumer Expenditure data?\n\nWant to add columns of memi to fmli that correspond to the principal earner’s memi data\n\nAlso going to create smaller data frames for us to play with:\n\n\n\n\nfmli_small &lt;- filter(fmli, NEWID %in% c(\"03530051\",\n                                        \"03327224\",\n                                        \"03324324\",\n                                        \"03324244\"))\nfmli_small\n\n# A tibble: 4 × 5\n  NEWID    PRINEARN FINCBTAX BLS_URBN HIGH_EDU\n  &lt;chr&gt;       &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n1 03324244        1        0        1 15      \n2 03324324        2    95000        2 15      \n3 03327224        1        0        1 14      \n4 03530051        3    70000        1 11      \n\n\n\n\nmemi_small &lt;- filter(memi, NEWID %in% c(\"03530051\",\n                                        \"03327224\",\n                                        \"03324324\",\n                                        \"03324244\"))\nmemi_small\n\n# A tibble: 10 × 5\n   NEWID    MEMBNO   AGE   SEX EARNTYPE\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 03324244      1    37     1        1\n 2 03324324      1    51     1        1\n 3 03324324      2    53     2        1\n 4 03327224      1    28     2        3\n 5 03327224      2    32     1        2\n 6 03327224      3     1     2       NA\n 7 03530051      1    43     1       NA\n 8 03530051      2    16     1       NA\n 9 03530051      3    44     1        3\n10 03530051      4     5     2       NA"
  },
  {
    "objectID": "stat100_wk04mon.html#look-at-the-possible-joins",
    "href": "stat100_wk04mon.html#look-at-the-possible-joins",
    "title": "Stat 100",
    "section": "Look at the Possible Joins",
    "text": "Look at the Possible Joins\n\nleft_join(fmli_small, memi_small) \n\nJoining with `by = join_by(NEWID)`\n\n\n# A tibble: 10 × 9\n   NEWID    PRINEARN FINCBTAX BLS_URBN HIGH_EDU MEMBNO   AGE   SEX EARNTYPE\n   &lt;chr&gt;       &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 03324244        1        0        1 15            1    37     1        1\n 2 03324324        2    95000        2 15            1    51     1        1\n 3 03324324        2    95000        2 15            2    53     2        1\n 4 03327224        1        0        1 14            1    28     2        3\n 5 03327224        1        0        1 14            2    32     1        2\n 6 03327224        1        0        1 14            3     1     2       NA\n 7 03530051        3    70000        1 11            1    43     1       NA\n 8 03530051        3    70000        1 11            2    16     1       NA\n 9 03530051        3    70000        1 11            3    44     1        3\n10 03530051        3    70000        1 11            4     5     2       NA"
  },
  {
    "objectID": "stat100_wk04mon.html#look-at-the-possible-joins-1",
    "href": "stat100_wk04mon.html#look-at-the-possible-joins-1",
    "title": "Stat 100",
    "section": "Look at the Possible Joins",
    "text": "Look at the Possible Joins\n\nBe careful. This erroneous example made my R crash when I tried it on the full data frames.\n\n\n\n\nleft_join(fmli_small, memi_small, join_by(\"PRINEARN\" == \"MEMBNO\"))\n\n# A tibble: 13 × 9\n   NEWID.x  PRINEARN FINCBTAX BLS_URBN HIGH_EDU NEWID.y    AGE   SEX EARNTYPE\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 03324244        1        0        1 15       03324244    37     1        1\n 2 03324244        1        0        1 15       03324324    51     1        1\n 3 03324244        1        0        1 15       03327224    28     2        3\n 4 03324244        1        0        1 15       03530051    43     1       NA\n 5 03324324        2    95000        2 15       03324324    53     2        1\n 6 03324324        2    95000        2 15       03327224    32     1        2\n 7 03324324        2    95000        2 15       03530051    16     1       NA\n 8 03327224        1        0        1 14       03324244    37     1        1\n 9 03327224        1        0        1 14       03324324    51     1        1\n10 03327224        1        0        1 14       03327224    28     2        3\n11 03327224        1        0        1 14       03530051    43     1       NA\n12 03530051        3    70000        1 11       03327224     1     2       NA\n13 03530051        3    70000        1 11       03530051    44     1        3\n\n\n\n\ncount(fmli_small, PRINEARN)\n\n# A tibble: 3 × 2\n  PRINEARN     n\n     &lt;int&gt; &lt;int&gt;\n1        1     2\n2        2     1\n3        3     1\n\ncount(memi_small, MEMBNO)\n\n# A tibble: 4 × 2\n  MEMBNO     n\n   &lt;dbl&gt; &lt;int&gt;\n1      1     4\n2      2     3\n3      3     2\n4      4     1"
  },
  {
    "objectID": "stat100_wk04mon.html#look-at-the-possible-joins-2",
    "href": "stat100_wk04mon.html#look-at-the-possible-joins-2",
    "title": "Stat 100",
    "section": "Look at the Possible Joins",
    "text": "Look at the Possible Joins\n\nleft_join(fmli_small, memi_small, join_by(\"NEWID\" == \"NEWID\", \"PRINEARN\" == \"MEMBNO\"))\n\n# A tibble: 4 × 8\n  NEWID    PRINEARN FINCBTAX BLS_URBN HIGH_EDU   AGE   SEX EARNTYPE\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 03324244        1        0        1 15          37     1        1\n2 03324324        2    95000        2 15          53     2        1\n3 03327224        1        0        1 14          28     2        3\n4 03530051        3    70000        1 11          44     1        3"
  },
  {
    "objectID": "stat100_wk04mon.html#look-at-the-possible-joins-3",
    "href": "stat100_wk04mon.html#look-at-the-possible-joins-3",
    "title": "Stat 100",
    "section": "Look at the Possible Joins",
    "text": "Look at the Possible Joins\n\ninner_join(fmli_small, memi_small, join_by(\"NEWID\" == \"NEWID\", \"PRINEARN\" == \"MEMBNO\"))\n\n# A tibble: 4 × 8\n  NEWID    PRINEARN FINCBTAX BLS_URBN HIGH_EDU   AGE   SEX EARNTYPE\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 03324244        1        0        1 15          37     1        1\n2 03324324        2    95000        2 15          53     2        1\n3 03327224        1        0        1 14          28     2        3\n4 03530051        3    70000        1 11          44     1        3\n\n\n\nWhy does this give us the same answer as left_join for this situation?"
  },
  {
    "objectID": "stat100_wk04mon.html#look-at-the-possible-joins-4",
    "href": "stat100_wk04mon.html#look-at-the-possible-joins-4",
    "title": "Stat 100",
    "section": "Look at the Possible Joins",
    "text": "Look at the Possible Joins\n\nfull_join(fmli_small, memi_small, join_by(\"NEWID\" == \"NEWID\", \"PRINEARN\" == \"MEMBNO\"))\n\n# A tibble: 10 × 8\n   NEWID    PRINEARN FINCBTAX BLS_URBN HIGH_EDU   AGE   SEX EARNTYPE\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 03324244        1        0        1 15          37     1        1\n 2 03324324        2    95000        2 15          53     2        1\n 3 03327224        1        0        1 14          28     2        3\n 4 03530051        3    70000        1 11          44     1        3\n 5 03324324        1       NA       NA &lt;NA&gt;        51     1        1\n 6 03327224        2       NA       NA &lt;NA&gt;        32     1        2\n 7 03327224        3       NA       NA &lt;NA&gt;         1     2       NA\n 8 03530051        1       NA       NA &lt;NA&gt;        43     1       NA\n 9 03530051        2       NA       NA &lt;NA&gt;        16     1       NA\n10 03530051        4       NA       NA &lt;NA&gt;         5     2       NA"
  },
  {
    "objectID": "stat100_wk04mon.html#joining-tips",
    "href": "stat100_wk04mon.html#joining-tips",
    "title": "Stat 100",
    "section": "Joining Tips",
    "text": "Joining Tips\n\nfmli &lt;- left_join(fmli, memi, join_by(\"NEWID\" == \"NEWID\", \"PRINEARN\" == \"MEMBNO\"))\n\n\nFIRST: conceptualize for yourself what you think you want the final dataset to look like!\nCheck initial dimensions and final dimensions.\nUse variable names when joining even if they are the same."
  },
  {
    "objectID": "stat100_wk04mon.html#naming-wrangled-data",
    "href": "stat100_wk04mon.html#naming-wrangled-data",
    "title": "Stat 100",
    "section": "Naming Wrangled Data",
    "text": "Naming Wrangled Data\nShould I name my new dataframe ce or ce1?\n\nMy answer:\n\nIs your new dataset structurally different? If so, give it a new name.\nAre you removing values you will need for a future analysis within the same document? If so, give it a new name.\nAre you just adding to or cleaning the data? If so, then write over the original."
  },
  {
    "objectID": "stat100_wk02mon.html#announcements",
    "href": "stat100_wk02mon.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nClass in full swing:\n\nSections: Can find your assigned section in my.harvard but need to go to the linked spreadsheet to find the room!\nOffice hours\nWrap-ups on Th 3-4pm and Fri 10:30 - 11:30am in SC 309\nLecture quiz will be released in Gradescope after class on Wednesday."
  },
  {
    "objectID": "stat100_wk02mon.html#day-2-goals",
    "href": "stat100_wk02mon.html#day-2-goals",
    "title": "Stat 100",
    "section": "Day 2 Goals",
    "text": "Day 2 Goals\n\nDiscuss course structure & goals (lecture, section, wrap-ups, office hours, assessments…)\nPresent important course policies (engagement, code of conduct, chatGPT, …)\nGet started in RStudio and with Quarto documents"
  },
  {
    "objectID": "stat100_wk02mon.html#data-analysis-workflow",
    "href": "stat100_wk02mon.html#data-analysis-workflow",
    "title": "Stat 100",
    "section": "Data Analysis Workflow",
    "text": "Data Analysis Workflow"
  },
  {
    "objectID": "stat100_wk02mon.html#stat-100-weekly-flow",
    "href": "stat100_wk02mon.html#stat-100-weekly-flow",
    "title": "Stat 100",
    "section": "Stat 100 Weekly Flow",
    "text": "Stat 100 Weekly Flow"
  },
  {
    "objectID": "stat100_wk02mon.html#forms-of-assessment",
    "href": "stat100_wk02mon.html#forms-of-assessment",
    "title": "Stat 100",
    "section": "Forms of Assessment",
    "text": "Forms of Assessment\n\n\n\nWeekly lecture quizzes:\n\nAddress important concepts covered.\nFind out what is still confusing.\nCan drop one quiz grade.\n\n\n\n\nWeekly problem sets:\n\nPractice concepts.\nTime during section will be devoted to starting the next p-set.\nCan drop one p-set grade and get 4 extension days.\n\n\n\n\n\n\n\nExams:\n\nFormat: In-class Exam & Oral Exam.\nWill have both a Mid-term and Final.\n\n\n\n\nParticipation/Engagement:\n\nIn lecture and section.\nBy Oct 9th, must:\n\nAttend at least one office hour.\nPost at least two messages on Slack."
  },
  {
    "objectID": "stat100_wk02mon.html#this-weeks-assessments",
    "href": "stat100_wk02mon.html#this-weeks-assessments",
    "title": "Stat 100",
    "section": "This Week’s Assessments",
    "text": "This Week’s Assessments\n\nLecture quiz releases on Wed at 11:45am.\nP-Set 1 will be posted to Posit Cloud on Wednesday.\n\nDue the following Tuesday, September 19th at 5pm on Gradescope.\nIn section this week, you will get started on P-Set 1 and will learn how to submit to Gradescope."
  },
  {
    "objectID": "stat100_wk02mon.html#professor-mcconville-in-spring-2023",
    "href": "stat100_wk02mon.html#professor-mcconville-in-spring-2023",
    "title": "Stat 100",
    "section": "Professor McConville in Spring 2023",
    "text": "Professor McConville in Spring 2023\n\n\n\nBBC Science Focus"
  },
  {
    "objectID": "stat100_wk02mon.html#professor-mcconville-in-summer-2023",
    "href": "stat100_wk02mon.html#professor-mcconville-in-summer-2023",
    "title": "Stat 100",
    "section": "Professor McConville in Summer 2023",
    "text": "Professor McConville in Summer 2023\n\n\n\n\n\nRicardo IV Tamayo\n\n\n\nGenerative AI Engagements\n\nParticipated in Stats Dept reading group on generative AI\nWent to conference talks on generative AI\nTalked to ChatGPT"
  },
  {
    "objectID": "stat100_wk02mon.html#chatgpt",
    "href": "stat100_wk02mon.html#chatgpt",
    "title": "Stat 100",
    "section": "",
    "text": "ChatGPT Strengths\n\nVery dangerous automated data analysis.\n\nHard to determine if its conclusions are based on the data at hand or the data the large language model was built on.\n\n\n\nStat 100 Goals\n\nLearn that data analysis is a humanistic endeavor where context drives the data analysis process.\n\nStill need to be careful about biases and data quality.\n\n\n\n\n\n\n\nA personalized tutor (especially for coding) that tells you the answers.\n\n\n\nFor you to learn to code and to think statistically.\n\nThe Stat 100 Teaching Team will help support that learning.\n\n\n\n\n\n\n\nInstantly generating written blurbs.\n\n\n\nFor you to learn how to communicate about your data work and data ethics."
  },
  {
    "objectID": "stat100_wk02mon.html#my-view-on-the-current-role-of-chatgpt-in-data-work",
    "href": "stat100_wk02mon.html#my-view-on-the-current-role-of-chatgpt-in-data-work",
    "title": "Stat 100",
    "section": "My View on the (Current) Role of ChatGPT in Data Work",
    "text": "My View on the (Current) Role of ChatGPT in Data Work\n\nTo generate code to realize fully conceived ideas that aren’t novel or advanced.\nSeasoned data scientist can/should:\n\nVerify the correctness of the code.\nIdentify any assumptions or defaults ChatGPT is employing.\nRun the code in R and draw conclusions based on the data to reduce risk of perpetuating biases that exist in ChatGPT’s training data.\n\nSo, ChatGPT might be useful to you after Stat 100 but will be detrimental to your learning during Stat 100."
  },
  {
    "objectID": "stat100_wk02mon.html#ai-policy-for-stat-100",
    "href": "stat100_wk02mon.html#ai-policy-for-stat-100",
    "title": "Stat 100",
    "section": "AI Policy for Stat 100",
    "text": "AI Policy for Stat 100\n\nWhile A.I. tools, such as ChatGPT, are being used to generate code and analyze data, goals of this course are to develop your own ability to write code and to thoughtfully extract knowledge from data. Therefore, we expect that all work students submit for this course will be their own. This includes code, written work, and oral assessments. We specifically forbid the use of ChatGPT or any other generative artificial intelligence (AI) tools at all stages of the work process, including preliminary ones, unless the assignment specifically states that it is allowed. Violations of this policy will be considered academic misconduct. We draw your attention to the fact that different classes at Harvard could implement different AI policies, and it is the student’s responsibility to conform to expectations for each course.\n\n\nThe goal of this policy is not to lessen your access to support but to ensure we are achieving the learning objectives of the course. Please see the Avenues for Help section of the syllabus for ways to get support in Stat 100."
  },
  {
    "objectID": "stat100_wk02mon.html#engagement",
    "href": "stat100_wk02mon.html#engagement",
    "title": "Stat 100",
    "section": "Engagement",
    "text": "Engagement\n\n\n \n\n\n\n\n\n\nBeing actively present is key.\n\n\n\nDuring lecture and section, remove distractions.\n\nWhen we are on our computers, close email, social media, news, etc.\nIf you will be using a computer/tablet for taking notes, please sit outside the technology-free zone (first 6 rows of the middle section) starting next lecture so as not to distract classmates.\nHide your phone.\n\nI have high expectations but know that all of you (regardless of your stats or computing background) have the ability to meet them."
  },
  {
    "objectID": "stat100_wk02mon.html#identity-and-the-classroom",
    "href": "stat100_wk02mon.html#identity-and-the-classroom",
    "title": "Stat 100",
    "section": "Identity and the Classroom",
    "text": "Identity and the Classroom\n\nAcknowledge that my perspectives and experiences have shaped how I teach this course and how I approach my data work\nSome of my identities place me in dominant groups while others in marginalized groups\nStrive to bring examples and scholarly contributions that value knowledge from folks with a wide variety of identities\nStrive to be a open listener and recognize your thoughts as a generous offer and a vote of confidence in my ability to hear and be transformed by you\nAsk that you reflect on your own identities, privileges, and power and how they impact your engagement with Stat 100"
  },
  {
    "objectID": "stat100_wk02mon.html#code-of-conduct",
    "href": "stat100_wk02mon.html#code-of-conduct",
    "title": "Stat 100",
    "section": "Code of Conduct",
    "text": "Code of Conduct\n\nWe expect all members of STAT 100 to make participation a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n\nWe expect everyone to act and interact in ways that contribute to an open, welcoming diverse, inclusive, and healthy community of learners. You can contribute to a positive learning environment by demonstrating empathy and kindness, being respectful of differing viewpoints and experiences, and giving and gracefully accepting constructive feedback.\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.0."
  },
  {
    "objectID": "stat100_wk02mon.html#ways-to-get-support",
    "href": "stat100_wk02mon.html#ways-to-get-support",
    "title": "Stat 100",
    "section": "Ways to Get Support",
    "text": "Ways to Get Support\n\nAttend, attend, attend when you don’t feel ill\nParticipate in the Stat 100 Slack Workspace\nCome to Office Hours and Wrap-up Sessions\nCreate study groups with your classmates\n\nDon’t know anyone in the course?\n\nUse Slack or Section to each to know other students in the course!\nFill out this Stat 100 study group matching form"
  },
  {
    "objectID": "stat100_wk02mon.html#language-r-this-r-that-q-what",
    "href": "stat100_wk02mon.html#language-r-this-r-that-q-what",
    "title": "Stat 100",
    "section": "Language: R-This, R-That, Q-What?",
    "text": "Language: R-This, R-That, Q-What?\n\n\n\n \n\n\nR is the name of the programming language.\n\n\n\nRStudio is the pretty interface and is hosted on a Posit Cloud Server.\n\n\n\n\nQuarto is the type of file where we will record all of our work (code, output, narrative)."
  },
  {
    "objectID": "stat100_wk02mon.html#main-components-of-rstudio-lay-out",
    "href": "stat100_wk02mon.html#main-components-of-rstudio-lay-out",
    "title": "Stat 100",
    "section": "Main Components of RStudio Lay-Out",
    "text": "Main Components of RStudio Lay-Out"
  },
  {
    "objectID": "stat100_wk02mon.html#reminders",
    "href": "stat100_wk02mon.html#reminders",
    "title": "Stat 100",
    "section": "Reminders",
    "text": "Reminders\n\nClass in full swing:\n\nSections: Can find your assigned section in my.harvard but need to go to the linked spreadsheet to find the room!\nOffice hours\nWrap-ups on Th 3-4pm and Fri 10:30 - 11:30am in SC 309\nLecture quiz will be released in Gradescope after class on Wednesday."
  },
  {
    "objectID": "stat100_wk04wed.html#announcements",
    "href": "stat100_wk04wed.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nOral practice in section this week.\n\nGoals for Today\n\nFinish up data joins.\nCover data collection/acquisition."
  },
  {
    "objectID": "stat100_wk04wed.html#which-are-you",
    "href": "stat100_wk04wed.html#which-are-you",
    "title": "Stat 100",
    "section": "Which Are YOU?",
    "text": "Which Are YOU?\n\n\nData Visualizer\n\n\n\nvia GIPHY\n\n\nData Wrangler\n\n\n\nvia GIPHY"
  },
  {
    "objectID": "stat100_wk04wed.html#load-necessary-packages",
    "href": "stat100_wk04wed.html#load-necessary-packages",
    "title": "Stat 100",
    "section": "Load Necessary Packages",
    "text": "Load Necessary Packages\n\n\n\n\n\ndplyr is part of this collection of data science packages.\n\n# Load necessary packages\nlibrary(tidyverse)"
  },
  {
    "objectID": "stat100_wk04wed.html#data-setting-bureau-of-labor-statistics-bls-consumer-expenditure-survey",
    "href": "stat100_wk04wed.html#data-setting-bureau-of-labor-statistics-bls-consumer-expenditure-survey",
    "title": "Stat 100",
    "section": "Data Setting: Bureau of Labor Statistics (BLS) Consumer Expenditure Survey",
    "text": "Data Setting: Bureau of Labor Statistics (BLS) Consumer Expenditure Survey\nBLS Mission: “Measures labor market activity, working conditions, price changes, and productivity in the U.S. economy to support public and private decision making.”\nData: Last quarter of the 2016 BLS Consumer Expenditure Survey.\n\nlibrary(tidyverse)\n\nce_raw &lt;- read_csv(\"data/fmli.csv\", \n                 na = c(\"NA\", \".\"))\nglimpse(ce_raw)\n\nRows: 6,301\nColumns: 51\n$ NEWID    &lt;chr&gt; \"03324174\", \"03324204\", \"03324214\", \"03324244\", \"03324274\", \"…\n$ PRINEARN &lt;chr&gt; \"01\", \"01\", \"01\", \"01\", \"02\", \"01\", \"01\", \"01\", \"02\", \"01\", \"…\n$ FINLWT21 &lt;dbl&gt; 25984.767, 6581.018, 20208.499, 18078.372, 20111.619, 19907.3…\n$ FINCBTAX &lt;dbl&gt; 116920, 200, 117000, 0, 2000, 942, 0, 91000, 95000, 40037, 10…\n$ BLS_URBN &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ POPSIZE  &lt;dbl&gt; 2, 3, 4, 2, 2, 2, 1, 2, 5, 2, 3, 2, 2, 3, 4, 3, 3, 1, 4, 1, 1…\n$ EDUC_REF &lt;chr&gt; \"16\", \"15\", \"16\", \"15\", \"14\", \"11\", \"10\", \"13\", \"12\", \"12\", \"…\n$ EDUCA2   &lt;dbl&gt; 15, 15, 13, NA, NA, NA, NA, 15, 15, 14, 12, 12, NA, NA, NA, 1…\n$ AGE_REF  &lt;dbl&gt; 63, 50, 47, 37, 51, 63, 77, 37, 51, 64, 26, 59, 81, 51, 67, 4…\n$ AGE2     &lt;dbl&gt; 50, 47, 46, NA, NA, NA, NA, 36, 53, 67, 44, 62, NA, NA, NA, 4…\n$ SEX_REF  &lt;dbl&gt; 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1…\n$ SEX2     &lt;dbl&gt; 2, 2, 1, NA, NA, NA, NA, 2, 2, 1, 1, 1, NA, NA, NA, 1, NA, 1,…\n$ REF_RACE &lt;dbl&gt; 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1…\n$ RACE2    &lt;dbl&gt; 1, 4, 1, NA, NA, NA, NA, 1, 1, 1, 1, 1, NA, NA, NA, 2, NA, 1,…\n$ HISP_REF &lt;dbl&gt; 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1…\n$ HISP2    &lt;dbl&gt; 2, 2, 1, NA, NA, NA, NA, 2, 2, 2, 2, 2, NA, NA, NA, 2, NA, 2,…\n$ FAM_TYPE &lt;dbl&gt; 3, 4, 1, 8, 9, 9, 8, 3, 1, 1, 3, 1, 8, 9, 8, 5, 9, 4, 8, 3, 2…\n$ MARITAL1 &lt;dbl&gt; 1, 1, 1, 5, 3, 3, 2, 1, 1, 1, 1, 1, 2, 3, 5, 1, 3, 1, 3, 1, 1…\n$ REGION   &lt;dbl&gt; 4, 4, 3, 4, 4, 3, 4, 1, 3, 2, 1, 4, 1, 3, 3, 3, 2, 1, 2, 4, 3…\n$ SMSASTAT &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ HIGH_EDU &lt;chr&gt; \"16\", \"15\", \"16\", \"15\", \"14\", \"11\", \"10\", \"15\", \"15\", \"14\", \"…\n$ EHOUSNGC &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ TOTEXPCQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ FOODCQ   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ TRANSCQ  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ HEALTHCQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ENTERTCQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ EDUCACQ  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ TOBACCCQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ STUDFINX &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ IRAX     &lt;dbl&gt; 1000000, 10000, 0, NA, NA, 0, 0, 15000, NA, 477000, NA, NA, N…\n$ CUTENURE &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 4, 1, 1, 2, 1, 2, 2, 2, 2, 4, 1, 1, 1, 4, 4…\n$ FAM_SIZE &lt;dbl&gt; 4, 6, 2, 1, 2, 2, 1, 5, 2, 2, 4, 2, 1, 2, 1, 4, 2, 4, 1, 3, 3…\n$ VEHQ     &lt;dbl&gt; 3, 5, 0, 4, 2, 0, 0, 2, 4, 2, 3, 2, 1, 3, 1, 2, 4, 4, 0, 2, 3…\n$ ROOMSQ   &lt;dbl&gt; 8, 5, 6, 4, 4, 4, 7, 5, 4, 9, 6, 10, 4, 7, 5, 6, 6, 8, 18, 4,…\n$ INC_HRS1 &lt;dbl&gt; 40, 40, 40, 44, 40, NA, NA, 40, 40, NA, 40, NA, NA, NA, NA, 4…\n$ INC_HRS2 &lt;dbl&gt; 30, 40, 52, NA, NA, NA, NA, 40, 40, NA, 65, NA, NA, NA, NA, 6…\n$ EARNCOMP &lt;dbl&gt; 3, 2, 2, 1, 4, 7, 8, 2, 2, 8, 2, 8, 8, 7, 8, 2, 7, 3, 1, 2, 1…\n$ NO_EARNR &lt;dbl&gt; 4, 2, 2, 1, 2, 1, 0, 2, 2, 0, 2, 0, 0, 1, 0, 2, 1, 3, 1, 2, 1…\n$ OCCUCOD1 &lt;chr&gt; \"03\", \"03\", \"05\", \"03\", \"04\", \"\", \"\", \"12\", \"04\", \"\", \"01\", \"…\n$ OCCUCOD2 &lt;chr&gt; \"04\", \"02\", \"01\", \"\", \"\", \"\", \"\", \"02\", \"03\", \"\", \"11\", \"\", \"…\n$ STATE    &lt;chr&gt; \"41\", \"15\", \"48\", \"06\", \"06\", \"48\", \"06\", \"42\", \"\", \"27\", \"25…\n$ DIVISION &lt;dbl&gt; 9, 9, 7, 9, 9, 7, 9, 2, NA, 4, 1, 8, 2, 5, 6, 7, 3, 2, 3, 9, …\n$ TOTXEST  &lt;dbl&gt; 15452, 11459, 15738, 25978, 588, 0, 0, 7261, 9406, -1414, 141…\n$ CREDFINX &lt;dbl&gt; 0, NA, 0, NA, 5, NA, NA, NA, NA, 0, NA, 0, NA, NA, NA, 2, 35,…\n$ CREDITB  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ CREDITX  &lt;dbl&gt; 4000, 5000, 2000, NA, 7000, 1800, NA, 6000, NA, 719, NA, 1200…\n$ BUILDING &lt;chr&gt; \"01\", \"01\", \"01\", \"02\", \"08\", \"01\", \"01\", \"01\", \"01\", \"01\", \"…\n$ ST_HOUS  &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ INT_PHON &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ INT_HOME &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…"
  },
  {
    "objectID": "stat100_wk04wed.html#ce-data",
    "href": "stat100_wk04wed.html#ce-data",
    "title": "Stat 100",
    "section": "CE Data",
    "text": "CE Data\n\n\nHousehold survey but data are also collected on individuals\n\nfmli: household data\nmemi: household member-level data\n\n\n\n\nfmli &lt;- read_csv(\"data/fmli.csv\", \n                 na = c(\"NA\", \".\")) %&gt;%\n  select(NEWID, PRINEARN, FINCBTAX,\n         BLS_URBN, HIGH_EDU)\nmemi &lt;- read_csv(\"data/memi.csv\", \n                 na = c(\"NA\", \".\")) %&gt;%\n  select(NEWID, MEMBNO, AGE, SEX, EARNTYPE)\n\nfmli &lt;- mutate(fmli, PRINEARN = as.integer(PRINEARN))\n\n\nWant to add variables on the principal earner from the member data frame to the household data frame"
  },
  {
    "objectID": "stat100_wk04wed.html#smaller-sets-of-ce-data",
    "href": "stat100_wk04wed.html#smaller-sets-of-ce-data",
    "title": "Stat 100",
    "section": "Smaller Sets of CE Data",
    "text": "Smaller Sets of CE Data\n\n\n\nfmli_small &lt;- filter(fmli, NEWID %in% c(\"03530051\",\n                                        \"03327224\",\n                                        \"03324324\",\n                                        \"03324244\"))\nfmli_small\n\n# A tibble: 4 × 5\n  NEWID    PRINEARN FINCBTAX BLS_URBN HIGH_EDU\n  &lt;chr&gt;       &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n1 03324244        1        0        1 15      \n2 03324324        2    95000        2 15      \n3 03327224        1        0        1 14      \n4 03530051        3    70000        1 11      \n\n\n\n\nmemi_small &lt;- filter(memi, NEWID %in% c(\"03530051\",\n                                        \"03327224\",\n                                        \"03324324\",\n                                        \"03324244\"))\nmemi_small\n\n# A tibble: 10 × 5\n   NEWID    MEMBNO   AGE   SEX EARNTYPE\n   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 03324244      1    37     1        1\n 2 03324324      1    51     1        1\n 3 03324324      2    53     2        1\n 4 03327224      1    28     2        3\n 5 03327224      2    32     1        2\n 6 03327224      3     1     2       NA\n 7 03530051      1    43     1       NA\n 8 03530051      2    16     1       NA\n 9 03530051      3    44     1        3\n10 03530051      4     5     2       NA"
  },
  {
    "objectID": "stat100_wk04wed.html#look-at-the-possible-joins",
    "href": "stat100_wk04wed.html#look-at-the-possible-joins",
    "title": "Stat 100",
    "section": "Look at the Possible Joins",
    "text": "Look at the Possible Joins\n\nfull_join(fmli_small, memi_small, join_by(\"NEWID\" == \"NEWID\", \"PRINEARN\" == \"MEMBNO\"))\n\n# A tibble: 10 × 8\n   NEWID    PRINEARN FINCBTAX BLS_URBN HIGH_EDU   AGE   SEX EARNTYPE\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 03324244        1        0        1 15          37     1        1\n 2 03324324        2    95000        2 15          53     2        1\n 3 03327224        1        0        1 14          28     2        3\n 4 03530051        3    70000        1 11          44     1        3\n 5 03324324        1       NA       NA &lt;NA&gt;        51     1        1\n 6 03327224        2       NA       NA &lt;NA&gt;        32     1        2\n 7 03327224        3       NA       NA &lt;NA&gt;         1     2       NA\n 8 03530051        1       NA       NA &lt;NA&gt;        43     1       NA\n 9 03530051        2       NA       NA &lt;NA&gt;        16     1       NA\n10 03530051        4       NA       NA &lt;NA&gt;         5     2       NA"
  },
  {
    "objectID": "stat100_wk04wed.html#joining-tips",
    "href": "stat100_wk04wed.html#joining-tips",
    "title": "Stat 100",
    "section": "Joining Tips",
    "text": "Joining Tips\n\nfmli &lt;- left_join(fmli, memi, join_by(\"NEWID\" == \"NEWID\", \"PRINEARN\" == \"MEMBNO\"))\n\n\nFIRST: conceptualize for yourself what you think you want the final dataset to look like!\nCheck initial dimensions and final dimensions.\nUse variable names when joining even if they are the same."
  },
  {
    "objectID": "stat100_wk04wed.html#naming-wrangled-data",
    "href": "stat100_wk04wed.html#naming-wrangled-data",
    "title": "Stat 100",
    "section": "Naming Wrangled Data",
    "text": "Naming Wrangled Data\nShould I name my new dataframe ce or ce1?\n\nMy answer:\n\nIs your new dataset structurally different? If so, give it a new name.\nAre you removing values you will need for a future analysis within the same document? If so, give it a new name.\nAre you just adding to or cleaning the data? If so, then write over the original."
  },
  {
    "objectID": "stat100_wk04wed.html#now-for-data-collection",
    "href": "stat100_wk04wed.html#now-for-data-collection",
    "title": "Stat 100",
    "section": "Now for Data Collection",
    "text": "Now for Data Collection"
  },
  {
    "objectID": "stat100_wk04wed.html#motivating-our-discussion-of-data-collection",
    "href": "stat100_wk04wed.html#motivating-our-discussion-of-data-collection",
    "title": "Stat 100",
    "section": "Motivating Our Discussion of Data Collection",
    "text": "Motivating Our Discussion of Data Collection"
  },
  {
    "objectID": "stat100_wk04wed.html#who-are-the-data-supposed-to-represent",
    "href": "stat100_wk04wed.html#who-are-the-data-supposed-to-represent",
    "title": "Stat 100",
    "section": "Who are the data supposed to represent?",
    "text": "Who are the data supposed to represent?\n\n\n\n\n\nKey questions:\n\nWhat evidence is there that the data are representative?\nWho is present? Who is absent?\nWho is overrepresented? Who is underrepresented?"
  },
  {
    "objectID": "stat100_wk04wed.html#who-are-the-data-supposed-to-represent-1",
    "href": "stat100_wk04wed.html#who-are-the-data-supposed-to-represent-1",
    "title": "Stat 100",
    "section": "Who are the data supposed to represent?",
    "text": "Who are the data supposed to represent?\n\n\n\n\n\nCensus: We have data on the whole population!"
  },
  {
    "objectID": "stat100_wk04wed.html#who-are-the-data-supposed-to-represent-2",
    "href": "stat100_wk04wed.html#who-are-the-data-supposed-to-represent-2",
    "title": "Stat 100",
    "section": "Who are the data supposed to represent?",
    "text": "Who are the data supposed to represent?"
  },
  {
    "objectID": "stat100_wk04wed.html#who-are-the-data-supposed-to-represent-3",
    "href": "stat100_wk04wed.html#who-are-the-data-supposed-to-represent-3",
    "title": "Stat 100",
    "section": "Who are the data supposed to represent?",
    "text": "Who are the data supposed to represent?\n\n\n\n\n\nKey questions:\n\nWhat evidence is there that the sample is representative of the population?\nWho is present? Who is absent?\nWho is overrepresented? Who is underrepresented?"
  },
  {
    "objectID": "stat100_wk04wed.html#sampling-bias",
    "href": "stat100_wk04wed.html#sampling-bias",
    "title": "Stat 100",
    "section": "Sampling Bias",
    "text": "Sampling Bias\n\n\n\n\n\nSampling bias: When the sampled units are systematically different from the non-sampled units on the variables of interest."
  },
  {
    "objectID": "stat100_wk04wed.html#random-sampling",
    "href": "stat100_wk04wed.html#random-sampling",
    "title": "Stat 100",
    "section": "Random Sampling",
    "text": "Random Sampling\nUse random sampling (a random mechanism for selecting cases from the population) to remove sampling bias.\nTypes of random sampling\n\nSimple random sampling\nCluster sampling\nStratified random sampling\nSystematic sampling\n\n\nWhy aren’t all samples generated using simple random sampling?"
  },
  {
    "objectID": "stat100_wk04wed.html#responsibilities-to-research-subjects",
    "href": "stat100_wk04wed.html#responsibilities-to-research-subjects",
    "title": "Stat 100",
    "section": "Responsibilities to Research Subjects",
    "text": "Responsibilities to Research Subjects\n\n“The ethical statistician protects and respects the rights and interests of human and animal subjects at all stages of their involvement in a project. This includes respondents to the census or to surveys, those whose data are contained in administrative records, and subjects of physically or psychologically invasive research.”"
  },
  {
    "objectID": "stat100_wk04wed.html#detour-from-our-detour",
    "href": "stat100_wk04wed.html#detour-from-our-detour",
    "title": "Stat 100",
    "section": "Detour from Our Detour",
    "text": "Detour from Our Detour\n\n\nlibrary(tidyverse)\nlibrary(NHANES)\n\nggplot(data = NHANES, \n       mapping = aes(x = Age,\n                     y = Height)) +\n  geom_point(alpha = 0.1) +\n  geom_smooth(color = \"skyblue\")"
  },
  {
    "objectID": "stat100_wk04wed.html#detour-from-our-detour-1",
    "href": "stat100_wk04wed.html#detour-from-our-detour-1",
    "title": "Stat 100",
    "section": "Detour from Our Detour",
    "text": "Detour from Our Detour\n\n\nlibrary(tidyverse)\nlibrary(NHANES)\nlibrary(emojifont)\n\nNHANES &lt;- mutate(NHANES, \n          heart = fontawesome(\"fa-heart\"))\n\nggplot(data = NHANES, \n       mapping = aes(x = Age,\n                     y = Height,\n                     label = heart)) +\n  geom_text(alpha = 0.1, color = \"red\",\n            family='fontawesome-webfont',\n            size = 16) +\n  stat_smooth(color = \"deeppink\")"
  },
  {
    "objectID": "stat100_wk09wed.html#announcements",
    "href": "stat100_wk09wed.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nDon’t forget that the midterm exam rewrites are due on Thursday at 5pm on Gradescope.\n\nMake sure to use the Quarto doc in the Midterm Exam (Rewrites) project on Posit Cloud.\n\n🎉 We are now accepting Course Assistant/Teaching Fellow applications for Stat 100 for next semester. To apply, fill out this application by Nov 15th.\n\nAbout 10-12 hours of work per week.\n\nPrimary responsibilities: Attend weekly team meetings, lead a discussion section, hold office hours, grade assessments.\n\n\nGoals for Today\n\n\n\nConfidence interval interpretations\nSet-up the structure of hypothesis testing\n\n\n\nDetermine if Harvard students have ESP!"
  },
  {
    "objectID": "stat100_wk09wed.html#interpreting-confidence-intervals",
    "href": "stat100_wk09wed.html#interpreting-confidence-intervals",
    "title": "Stat 100",
    "section": "Interpreting Confidence Intervals",
    "text": "Interpreting Confidence Intervals\nExample: Estimating average household income before taxes in the US\n\n\nSE Method Formula:\n\\[\n\\mbox{statistic} \\pm{\\mbox{ME}}\n\\]\n\n\n# A tibble: 1 × 3\n     ME  lower  upper\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1968. 60512. 64448.\n\n\n\n“The margin of [sampling] error can be described as the ‘penalty’ in precision for not talking to everyone in a given population. It describes the range that an answer likely falls between if the survey had reached everyone in a population, instead of just a sample of that population.” – Courtney Kennedy, Director of Survey Research at Pew Research Center\nCI = interval of plausible values for the parameter\n\n\nSafe interpretation: I am P% confident that {insert what the parameter represents in context} is between {insert lower bound} and {insert upper bound}."
  },
  {
    "objectID": "stat100_wk09wed.html#caution-confidence-intervals-in-the-wild",
    "href": "stat100_wk09wed.html#caution-confidence-intervals-in-the-wild",
    "title": "Stat 100",
    "section": "Caution: Confidence intervals in the wild",
    "text": "Caution: Confidence intervals in the wild\nStatement in an article for The BMJ (British Medical Journal):"
  },
  {
    "objectID": "stat100_wk09wed.html#statistical-inference",
    "href": "stat100_wk09wed.html#statistical-inference",
    "title": "Stat 100",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\n\n\n\n\n\n\n\n\n\n\n\nGoal: Draw conclusions about the population based on a sample.\nMain Flavors:\n\nEstimating numerical quantities.\nTesting conjectures."
  },
  {
    "objectID": "stat100_wk09wed.html#example-does-extrasensory-perception-esp-exist",
    "href": "stat100_wk09wed.html#example-does-extrasensory-perception-esp-exist",
    "title": "Stat 100",
    "section": "Example: Does Extrasensory Perception (ESP) exist?",
    "text": "Example: Does Extrasensory Perception (ESP) exist?\n\n\n\n\n\n\n\nDaryl Bem and Ben Honorton\n\n\n\n\n\nBem and Honorton conducted extrasensory perception studies:\n\n\nA “sender” randomly chooses an object out of 4 possible objects and sends that information to a “receiver”.\nThe “receiver” is then given a set of 4 possible objects and they must decide which one most resembles the object sent to them.\n\n\nOut of 329 trials, the “receivers” correctly identified the object 106 times."
  },
  {
    "objectID": "stat100_wk09wed.html#esp-example",
    "href": "stat100_wk09wed.html#esp-example",
    "title": "Stat 100",
    "section": "ESP Example",
    "text": "ESP Example\nLet’s consider the following questions:\n\n\nIf ESP does not exist and the “receivers” are guessing, how often would we expect them to be correct?\nFor each sample (set of 329 trials), do we expect the proportion of correct guesses to be equal? Why or why not?\nIs it possible to randomly guess correctly 106 out of 329 times (i.e., 32% of the time)?\nHow unusual is it to guess correctly 106 out of 329 times if ESP doesn’t exist?\n\n\n\nTo help us answer d., we need a sampling distribution for the sample proportion where we assume the “receivers” were purely guessing!"
  },
  {
    "objectID": "stat100_wk09wed.html#sampling-distribution-of-a-statistic",
    "href": "stat100_wk09wed.html#sampling-distribution-of-a-statistic",
    "title": "Stat 100",
    "section": "Sampling Distribution of a Statistic",
    "text": "Sampling Distribution of a Statistic\n\n\nSteps for (Approximate) Distribution:\n\n\nDecide on a sample size, \\(n\\).\nRandomly select a sample of size \\(n\\) from the population.\nCompute the sample statistic.\nPut the sample back in.\nRepeat Steps (2) - (4) many (1000+) times."
  },
  {
    "objectID": "stat100_wk09wed.html#sampling-distribution-of-a-statistic-1",
    "href": "stat100_wk09wed.html#sampling-distribution-of-a-statistic-1",
    "title": "Stat 100",
    "section": "Sampling Distribution of a Statistic",
    "text": "Sampling Distribution of a Statistic\n\n\nSteps for (Approximate) Distribution:\n\n\nDecide on a sample size, \\(n\\).\nRandomly select a sample of size \\(n\\) from the population.\nCompute the sample statistic.\nPut the sample back in.\nRepeat Steps (2) - (4) many (1000+) times."
  },
  {
    "objectID": "stat100_wk09wed.html#sampling-distribution-under-no-esp",
    "href": "stat100_wk09wed.html#sampling-distribution-under-no-esp",
    "title": "Stat 100",
    "section": "Sampling Distribution Under No ESP",
    "text": "Sampling Distribution Under No ESP\nSteps for (Approximate) Distribution:\n\n\nDecide on a sample size, \\(n\\).\nRandomly select a sample of size \\(n\\) from the population.\n\n\n\nlibrary(mosaic)\nrflip(n = 329, prob = 0.25)\n\n\nFlipping 329 coins [ Prob(Heads) = 0.25 ] ...\n\nT T T H T T T T T H T T T T T T T T T T T T T T T T T H T T T H T T T T\nT T T T H H H T T T T T T T T T T H T T T T T H T T T T T T T H T H T H\nH H T T T H H T T H T H T T T T T T H H T T H T T T T T T T T T T H H H\nH H T H H H T T T T T H T H T T H T T T T T H T T T T T T T T T H H T T\nH T T T T T H H T T T T T T H H T T H T T T T T T T T T T T H T H T T T\nT T H H T T T T H T H H T H H T T T T H T T H T H T H T T T H T T T T T\nT T T H T T T T T T T H T T T T H H T H T T H T T T T T H T T T H T H T\nH T H H T H T T T T T T T T T T H T T T T T T T T H T H T T T T T H T T\nT H T T H T T T H T T H T H T T H H H T T T T T T T T T T H H T H T T T\nT T T T T\n\nNumber of Heads: 83 [Proportion Heads: 0.252279635258359]"
  },
  {
    "objectID": "stat100_wk09wed.html#sampling-distribution-under-no-esp-1",
    "href": "stat100_wk09wed.html#sampling-distribution-under-no-esp-1",
    "title": "Stat 100",
    "section": "Sampling Distribution Under No ESP",
    "text": "Sampling Distribution Under No ESP\n\n\nCompute the sample statistic.\n\n\n\nrflip(n = 329, prob = 0.25, summarize = TRUE)\n\n    n heads tails prob\n1 329    64   265 0.25\n\n\n\n\n\nPut the sample back in.\nRepeat Steps (2) - (4) many (1000+) times.\n\n\n\nguess_sampling_dist &lt;- do(1000)*rflip(n = 329, prob = 0.25)\nguess_sampling_dist\n\n       n heads tails      prop\n1    329    86   243 0.2613982\n2    329    84   245 0.2553191\n3    329    83   246 0.2522796\n4    329    84   245 0.2553191\n5    329    70   259 0.2127660\n6    329    81   248 0.2462006\n7    329    92   237 0.2796353\n8    329    70   259 0.2127660\n9    329    82   247 0.2492401\n10   329    75   254 0.2279635\n11   329    73   256 0.2218845\n12   329    71   258 0.2158055\n13   329    71   258 0.2158055\n14   329    75   254 0.2279635\n15   329    89   240 0.2705167\n16   329    86   243 0.2613982\n17   329    72   257 0.2188450\n18   329    77   252 0.2340426\n19   329    91   238 0.2765957\n20   329    77   252 0.2340426\n21   329    87   242 0.2644377\n22   329    75   254 0.2279635\n23   329    82   247 0.2492401\n24   329    89   240 0.2705167\n25   329    91   238 0.2765957\n26   329    89   240 0.2705167\n27   329    77   252 0.2340426\n28   329    82   247 0.2492401\n29   329    91   238 0.2765957\n30   329    86   243 0.2613982\n31   329    77   252 0.2340426\n32   329    71   258 0.2158055\n33   329    76   253 0.2310030\n34   329    83   246 0.2522796\n35   329    83   246 0.2522796\n36   329    94   235 0.2857143\n37   329    84   245 0.2553191\n38   329    83   246 0.2522796\n39   329    88   241 0.2674772\n40   329    86   243 0.2613982\n41   329    77   252 0.2340426\n42   329    82   247 0.2492401\n43   329    84   245 0.2553191\n44   329    94   235 0.2857143\n45   329    78   251 0.2370821\n46   329    62   267 0.1884498\n47   329    72   257 0.2188450\n48   329    84   245 0.2553191\n49   329    82   247 0.2492401\n50   329    73   256 0.2218845\n51   329    89   240 0.2705167\n52   329    92   237 0.2796353\n53   329    75   254 0.2279635\n54   329    75   254 0.2279635\n55   329    79   250 0.2401216\n56   329    75   254 0.2279635\n57   329    81   248 0.2462006\n58   329    76   253 0.2310030\n59   329    79   250 0.2401216\n60   329    75   254 0.2279635\n61   329   100   229 0.3039514\n62   329    85   244 0.2583587\n63   329    83   246 0.2522796\n64   329    89   240 0.2705167\n65   329    78   251 0.2370821\n66   329    81   248 0.2462006\n67   329    67   262 0.2036474\n68   329    94   235 0.2857143\n69   329    83   246 0.2522796\n70   329    76   253 0.2310030\n71   329    87   242 0.2644377\n72   329    68   261 0.2066869\n73   329    79   250 0.2401216\n74   329    91   238 0.2765957\n75   329    83   246 0.2522796\n76   329    93   236 0.2826748\n77   329    89   240 0.2705167\n78   329    98   231 0.2978723\n79   329    69   260 0.2097264\n80   329    80   249 0.2431611\n81   329    82   247 0.2492401\n82   329    72   257 0.2188450\n83   329    79   250 0.2401216\n84   329    85   244 0.2583587\n85   329    90   239 0.2735562\n86   329    88   241 0.2674772\n87   329    78   251 0.2370821\n88   329    73   256 0.2218845\n89   329    72   257 0.2188450\n90   329    83   246 0.2522796\n91   329    78   251 0.2370821\n92   329    76   253 0.2310030\n93   329    90   239 0.2735562\n94   329    79   250 0.2401216\n95   329    68   261 0.2066869\n96   329    90   239 0.2735562\n97   329    92   237 0.2796353\n98   329    93   236 0.2826748\n99   329    98   231 0.2978723\n100  329    90   239 0.2735562\n101  329    92   237 0.2796353\n102  329    78   251 0.2370821\n103  329    69   260 0.2097264\n104  329    83   246 0.2522796\n105  329    88   241 0.2674772\n106  329    84   245 0.2553191\n107  329    68   261 0.2066869\n108  329    77   252 0.2340426\n109  329    85   244 0.2583587\n110  329    86   243 0.2613982\n111  329    91   238 0.2765957\n112  329    84   245 0.2553191\n113  329    91   238 0.2765957\n114  329    90   239 0.2735562\n115  329    69   260 0.2097264\n116  329    70   259 0.2127660\n117  329    81   248 0.2462006\n118  329    84   245 0.2553191\n119  329    79   250 0.2401216\n120  329    81   248 0.2462006\n121  329   101   228 0.3069909\n122  329    74   255 0.2249240\n123  329    66   263 0.2006079\n124  329    78   251 0.2370821\n125  329    77   252 0.2340426\n126  329    94   235 0.2857143\n127  329    97   232 0.2948328\n128  329    73   256 0.2218845\n129  329    77   252 0.2340426\n130  329    83   246 0.2522796\n131  329    76   253 0.2310030\n132  329    80   249 0.2431611\n133  329    87   242 0.2644377\n134  329    86   243 0.2613982\n135  329    74   255 0.2249240\n136  329    89   240 0.2705167\n137  329    80   249 0.2431611\n138  329    69   260 0.2097264\n139  329    91   238 0.2765957\n140  329    73   256 0.2218845\n141  329    91   238 0.2765957\n142  329    81   248 0.2462006\n143  329    82   247 0.2492401\n144  329    76   253 0.2310030\n145  329    91   238 0.2765957\n146  329    84   245 0.2553191\n147  329    83   246 0.2522796\n148  329    82   247 0.2492401\n149  329    92   237 0.2796353\n150  329    94   235 0.2857143\n151  329    92   237 0.2796353\n152  329    79   250 0.2401216\n153  329    82   247 0.2492401\n154  329    83   246 0.2522796\n155  329    66   263 0.2006079\n156  329    85   244 0.2583587\n157  329    88   241 0.2674772\n158  329   101   228 0.3069909\n159  329    75   254 0.2279635\n160  329    81   248 0.2462006\n161  329    77   252 0.2340426\n162  329    93   236 0.2826748\n163  329    79   250 0.2401216\n164  329    87   242 0.2644377\n165  329    90   239 0.2735562\n166  329    76   253 0.2310030\n167  329    71   258 0.2158055\n168  329    80   249 0.2431611\n169  329    80   249 0.2431611\n170  329    84   245 0.2553191\n171  329    94   235 0.2857143\n172  329    87   242 0.2644377\n173  329    84   245 0.2553191\n174  329    69   260 0.2097264\n175  329    79   250 0.2401216\n176  329    78   251 0.2370821\n177  329    85   244 0.2583587\n178  329    90   239 0.2735562\n179  329    80   249 0.2431611\n180  329    85   244 0.2583587\n181  329    90   239 0.2735562\n182  329    83   246 0.2522796\n183  329    80   249 0.2431611\n184  329    73   256 0.2218845\n185  329    70   259 0.2127660\n186  329    79   250 0.2401216\n187  329    93   236 0.2826748\n188  329    80   249 0.2431611\n189  329    85   244 0.2583587\n190  329    85   244 0.2583587\n191  329    85   244 0.2583587\n192  329    80   249 0.2431611\n193  329    84   245 0.2553191\n194  329    71   258 0.2158055\n195  329    86   243 0.2613982\n196  329    92   237 0.2796353\n197  329    77   252 0.2340426\n198  329    93   236 0.2826748\n199  329    78   251 0.2370821\n200  329    83   246 0.2522796\n201  329    97   232 0.2948328\n202  329    82   247 0.2492401\n203  329    82   247 0.2492401\n204  329    78   251 0.2370821\n205  329    93   236 0.2826748\n206  329    81   248 0.2462006\n207  329    80   249 0.2431611\n208  329    72   257 0.2188450\n209  329    72   257 0.2188450\n210  329    86   243 0.2613982\n211  329    80   249 0.2431611\n212  329    89   240 0.2705167\n213  329    79   250 0.2401216\n214  329    84   245 0.2553191\n215  329    78   251 0.2370821\n216  329    74   255 0.2249240\n217  329    74   255 0.2249240\n218  329    90   239 0.2735562\n219  329    74   255 0.2249240\n220  329    83   246 0.2522796\n221  329    85   244 0.2583587\n222  329    71   258 0.2158055\n223  329    72   257 0.2188450\n224  329    94   235 0.2857143\n225  329    86   243 0.2613982\n226  329    74   255 0.2249240\n227  329    83   246 0.2522796\n228  329    78   251 0.2370821\n229  329    75   254 0.2279635\n230  329    86   243 0.2613982\n231  329    75   254 0.2279635\n232  329    83   246 0.2522796\n233  329    85   244 0.2583587\n234  329    80   249 0.2431611\n235  329    83   246 0.2522796\n236  329    86   243 0.2613982\n237  329    78   251 0.2370821\n238  329    88   241 0.2674772\n239  329    73   256 0.2218845\n240  329    75   254 0.2279635\n241  329    83   246 0.2522796\n242  329    82   247 0.2492401\n243  329    79   250 0.2401216\n244  329    78   251 0.2370821\n245  329    79   250 0.2401216\n246  329    76   253 0.2310030\n247  329    73   256 0.2218845\n248  329    71   258 0.2158055\n249  329    87   242 0.2644377\n250  329    86   243 0.2613982\n251  329    78   251 0.2370821\n252  329    82   247 0.2492401\n253  329    76   253 0.2310030\n254  329    85   244 0.2583587\n255  329    81   248 0.2462006\n256  329    76   253 0.2310030\n257  329    84   245 0.2553191\n258  329    86   243 0.2613982\n259  329    94   235 0.2857143\n260  329    73   256 0.2218845\n261  329    82   247 0.2492401\n262  329    82   247 0.2492401\n263  329    83   246 0.2522796\n264  329    91   238 0.2765957\n265  329    63   266 0.1914894\n266  329    87   242 0.2644377\n267  329    74   255 0.2249240\n268  329    83   246 0.2522796\n269  329    82   247 0.2492401\n270  329    73   256 0.2218845\n271  329    73   256 0.2218845\n272  329    74   255 0.2249240\n273  329    90   239 0.2735562\n274  329    72   257 0.2188450\n275  329    76   253 0.2310030\n276  329    92   237 0.2796353\n277  329    87   242 0.2644377\n278  329    72   257 0.2188450\n279  329    82   247 0.2492401\n280  329    70   259 0.2127660\n281  329    88   241 0.2674772\n282  329    93   236 0.2826748\n283  329    79   250 0.2401216\n284  329    84   245 0.2553191\n285  329    86   243 0.2613982\n286  329    76   253 0.2310030\n287  329    92   237 0.2796353\n288  329    88   241 0.2674772\n289  329    88   241 0.2674772\n290  329    85   244 0.2583587\n291  329    91   238 0.2765957\n292  329    89   240 0.2705167\n293  329    73   256 0.2218845\n294  329    79   250 0.2401216\n295  329    87   242 0.2644377\n296  329    93   236 0.2826748\n297  329    62   267 0.1884498\n298  329    90   239 0.2735562\n299  329    72   257 0.2188450\n300  329    86   243 0.2613982\n301  329    90   239 0.2735562\n302  329    83   246 0.2522796\n303  329    85   244 0.2583587\n304  329    77   252 0.2340426\n305  329    86   243 0.2613982\n306  329    79   250 0.2401216\n307  329    84   245 0.2553191\n308  329    92   237 0.2796353\n309  329    82   247 0.2492401\n310  329    87   242 0.2644377\n311  329    90   239 0.2735562\n312  329    75   254 0.2279635\n313  329    79   250 0.2401216\n314  329    61   268 0.1854103\n315  329    90   239 0.2735562\n316  329    87   242 0.2644377\n317  329    76   253 0.2310030\n318  329    67   262 0.2036474\n319  329    75   254 0.2279635\n320  329    85   244 0.2583587\n321  329    72   257 0.2188450\n322  329    66   263 0.2006079\n323  329    77   252 0.2340426\n324  329    85   244 0.2583587\n325  329    91   238 0.2765957\n326  329    83   246 0.2522796\n327  329    95   234 0.2887538\n328  329    90   239 0.2735562\n329  329    67   262 0.2036474\n330  329    87   242 0.2644377\n331  329    81   248 0.2462006\n332  329    85   244 0.2583587\n333  329    77   252 0.2340426\n334  329    87   242 0.2644377\n335  329    78   251 0.2370821\n336  329    86   243 0.2613982\n337  329    77   252 0.2340426\n338  329    80   249 0.2431611\n339  329    87   242 0.2644377\n340  329    76   253 0.2310030\n341  329    84   245 0.2553191\n342  329    93   236 0.2826748\n343  329    80   249 0.2431611\n344  329   102   227 0.3100304\n345  329    84   245 0.2553191\n346  329    95   234 0.2887538\n347  329    84   245 0.2553191\n348  329    91   238 0.2765957\n349  329    68   261 0.2066869\n350  329    77   252 0.2340426\n351  329    77   252 0.2340426\n352  329    86   243 0.2613982\n353  329    85   244 0.2583587\n354  329    88   241 0.2674772\n355  329    75   254 0.2279635\n356  329    96   233 0.2917933\n357  329    78   251 0.2370821\n358  329    90   239 0.2735562\n359  329    94   235 0.2857143\n360  329    86   243 0.2613982\n361  329    69   260 0.2097264\n362  329    86   243 0.2613982\n363  329    88   241 0.2674772\n364  329    95   234 0.2887538\n365  329    80   249 0.2431611\n366  329    72   257 0.2188450\n367  329    79   250 0.2401216\n368  329    85   244 0.2583587\n369  329    90   239 0.2735562\n370  329    91   238 0.2765957\n371  329    78   251 0.2370821\n372  329    87   242 0.2644377\n373  329    80   249 0.2431611\n374  329    83   246 0.2522796\n375  329    78   251 0.2370821\n376  329    86   243 0.2613982\n377  329    94   235 0.2857143\n378  329    76   253 0.2310030\n379  329    87   242 0.2644377\n380  329    94   235 0.2857143\n381  329    78   251 0.2370821\n382  329    78   251 0.2370821\n383  329    69   260 0.2097264\n384  329    86   243 0.2613982\n385  329    83   246 0.2522796\n386  329    72   257 0.2188450\n387  329    76   253 0.2310030\n388  329    79   250 0.2401216\n389  329    74   255 0.2249240\n390  329    74   255 0.2249240\n391  329    95   234 0.2887538\n392  329    86   243 0.2613982\n393  329    85   244 0.2583587\n394  329    80   249 0.2431611\n395  329    85   244 0.2583587\n396  329    90   239 0.2735562\n397  329    79   250 0.2401216\n398  329    65   264 0.1975684\n399  329    87   242 0.2644377\n400  329    79   250 0.2401216\n401  329    89   240 0.2705167\n402  329    89   240 0.2705167\n403  329    80   249 0.2431611\n404  329    82   247 0.2492401\n405  329    96   233 0.2917933\n406  329    76   253 0.2310030\n407  329    77   252 0.2340426\n408  329    78   251 0.2370821\n409  329    95   234 0.2887538\n410  329    86   243 0.2613982\n411  329    89   240 0.2705167\n412  329    82   247 0.2492401\n413  329    85   244 0.2583587\n414  329    83   246 0.2522796\n415  329    88   241 0.2674772\n416  329    84   245 0.2553191\n417  329    88   241 0.2674772\n418  329    76   253 0.2310030\n419  329    64   265 0.1945289\n420  329    75   254 0.2279635\n421  329    86   243 0.2613982\n422  329    82   247 0.2492401\n423  329    95   234 0.2887538\n424  329    88   241 0.2674772\n425  329    64   265 0.1945289\n426  329    86   243 0.2613982\n427  329    75   254 0.2279635\n428  329    91   238 0.2765957\n429  329    89   240 0.2705167\n430  329    88   241 0.2674772\n431  329    84   245 0.2553191\n432  329    81   248 0.2462006\n433  329    93   236 0.2826748\n434  329    78   251 0.2370821\n435  329    82   247 0.2492401\n436  329    77   252 0.2340426\n437  329    82   247 0.2492401\n438  329    95   234 0.2887538\n439  329    90   239 0.2735562\n440  329    85   244 0.2583587\n441  329    82   247 0.2492401\n442  329    80   249 0.2431611\n443  329    66   263 0.2006079\n444  329    83   246 0.2522796\n445  329    87   242 0.2644377\n446  329    80   249 0.2431611\n447  329    80   249 0.2431611\n448  329    72   257 0.2188450\n449  329    69   260 0.2097264\n450  329    84   245 0.2553191\n451  329    68   261 0.2066869\n452  329    68   261 0.2066869\n453  329    72   257 0.2188450\n454  329    90   239 0.2735562\n455  329    80   249 0.2431611\n456  329    73   256 0.2218845\n457  329    89   240 0.2705167\n458  329    94   235 0.2857143\n459  329    88   241 0.2674772\n460  329    86   243 0.2613982\n461  329    82   247 0.2492401\n462  329    67   262 0.2036474\n463  329    82   247 0.2492401\n464  329    96   233 0.2917933\n465  329    84   245 0.2553191\n466  329    63   266 0.1914894\n467  329    78   251 0.2370821\n468  329    79   250 0.2401216\n469  329    82   247 0.2492401\n470  329    79   250 0.2401216\n471  329    79   250 0.2401216\n472  329    72   257 0.2188450\n473  329    92   237 0.2796353\n474  329    71   258 0.2158055\n475  329    80   249 0.2431611\n476  329    84   245 0.2553191\n477  329    88   241 0.2674772\n478  329    86   243 0.2613982\n479  329    82   247 0.2492401\n480  329    82   247 0.2492401\n481  329    80   249 0.2431611\n482  329    78   251 0.2370821\n483  329    88   241 0.2674772\n484  329    93   236 0.2826748\n485  329    79   250 0.2401216\n486  329    79   250 0.2401216\n487  329    83   246 0.2522796\n488  329    86   243 0.2613982\n489  329    65   264 0.1975684\n490  329    78   251 0.2370821\n491  329    83   246 0.2522796\n492  329    77   252 0.2340426\n493  329    86   243 0.2613982\n494  329    85   244 0.2583587\n495  329    81   248 0.2462006\n496  329    89   240 0.2705167\n497  329    79   250 0.2401216\n498  329    76   253 0.2310030\n499  329    92   237 0.2796353\n500  329    72   257 0.2188450\n501  329    86   243 0.2613982\n502  329    87   242 0.2644377\n503  329    78   251 0.2370821\n504  329    79   250 0.2401216\n505  329    71   258 0.2158055\n506  329    61   268 0.1854103\n507  329    81   248 0.2462006\n508  329    89   240 0.2705167\n509  329    80   249 0.2431611\n510  329    95   234 0.2887538\n511  329    75   254 0.2279635\n512  329    86   243 0.2613982\n513  329    66   263 0.2006079\n514  329    82   247 0.2492401\n515  329    86   243 0.2613982\n516  329    73   256 0.2218845\n517  329    77   252 0.2340426\n518  329    86   243 0.2613982\n519  329    90   239 0.2735562\n520  329    65   264 0.1975684\n521  329    82   247 0.2492401\n522  329    89   240 0.2705167\n523  329    86   243 0.2613982\n524  329    75   254 0.2279635\n525  329    75   254 0.2279635\n526  329    91   238 0.2765957\n527  329    81   248 0.2462006\n528  329    68   261 0.2066869\n529  329    80   249 0.2431611\n530  329    90   239 0.2735562\n531  329    70   259 0.2127660\n532  329    79   250 0.2401216\n533  329    86   243 0.2613982\n534  329    92   237 0.2796353\n535  329    88   241 0.2674772\n536  329    73   256 0.2218845\n537  329    81   248 0.2462006\n538  329    65   264 0.1975684\n539  329    83   246 0.2522796\n540  329    86   243 0.2613982\n541  329    71   258 0.2158055\n542  329    83   246 0.2522796\n543  329    96   233 0.2917933\n544  329    78   251 0.2370821\n545  329    82   247 0.2492401\n546  329    89   240 0.2705167\n547  329    72   257 0.2188450\n548  329    74   255 0.2249240\n549  329    73   256 0.2218845\n550  329    78   251 0.2370821\n551  329    75   254 0.2279635\n552  329    81   248 0.2462006\n553  329    73   256 0.2218845\n554  329    75   254 0.2279635\n555  329    95   234 0.2887538\n556  329    75   254 0.2279635\n557  329    79   250 0.2401216\n558  329    69   260 0.2097264\n559  329    74   255 0.2249240\n560  329    90   239 0.2735562\n561  329    99   230 0.3009119\n562  329    88   241 0.2674772\n563  329    84   245 0.2553191\n564  329    91   238 0.2765957\n565  329    86   243 0.2613982\n566  329    82   247 0.2492401\n567  329    91   238 0.2765957\n568  329    84   245 0.2553191\n569  329    77   252 0.2340426\n570  329    84   245 0.2553191\n571  329    84   245 0.2553191\n572  329    78   251 0.2370821\n573  329    80   249 0.2431611\n574  329    92   237 0.2796353\n575  329    65   264 0.1975684\n576  329    89   240 0.2705167\n577  329    70   259 0.2127660\n578  329    93   236 0.2826748\n579  329    88   241 0.2674772\n580  329    83   246 0.2522796\n581  329    74   255 0.2249240\n582  329    96   233 0.2917933\n583  329    73   256 0.2218845\n584  329    87   242 0.2644377\n585  329    70   259 0.2127660\n586  329    96   233 0.2917933\n587  329    79   250 0.2401216\n588  329    76   253 0.2310030\n589  329    88   241 0.2674772\n590  329    79   250 0.2401216\n591  329    78   251 0.2370821\n592  329    88   241 0.2674772\n593  329    92   237 0.2796353\n594  329    78   251 0.2370821\n595  329    70   259 0.2127660\n596  329    79   250 0.2401216\n597  329    79   250 0.2401216\n598  329    75   254 0.2279635\n599  329    82   247 0.2492401\n600  329    82   247 0.2492401\n601  329    81   248 0.2462006\n602  329    85   244 0.2583587\n603  329    90   239 0.2735562\n604  329    89   240 0.2705167\n605  329    82   247 0.2492401\n606  329    93   236 0.2826748\n607  329    78   251 0.2370821\n608  329    75   254 0.2279635\n609  329    73   256 0.2218845\n610  329    80   249 0.2431611\n611  329    89   240 0.2705167\n612  329    91   238 0.2765957\n613  329    65   264 0.1975684\n614  329    86   243 0.2613982\n615  329    74   255 0.2249240\n616  329    59   270 0.1793313\n617  329    81   248 0.2462006\n618  329    81   248 0.2462006\n619  329    93   236 0.2826748\n620  329    74   255 0.2249240\n621  329    79   250 0.2401216\n622  329    78   251 0.2370821\n623  329    91   238 0.2765957\n624  329    80   249 0.2431611\n625  329    85   244 0.2583587\n626  329    86   243 0.2613982\n627  329    85   244 0.2583587\n628  329    86   243 0.2613982\n629  329    73   256 0.2218845\n630  329    83   246 0.2522796\n631  329    73   256 0.2218845\n632  329    92   237 0.2796353\n633  329    74   255 0.2249240\n634  329    84   245 0.2553191\n635  329    83   246 0.2522796\n636  329    77   252 0.2340426\n637  329    93   236 0.2826748\n638  329    78   251 0.2370821\n639  329    74   255 0.2249240\n640  329    88   241 0.2674772\n641  329    93   236 0.2826748\n642  329    86   243 0.2613982\n643  329    86   243 0.2613982\n644  329    79   250 0.2401216\n645  329    80   249 0.2431611\n646  329    68   261 0.2066869\n647  329    85   244 0.2583587\n648  329    69   260 0.2097264\n649  329    91   238 0.2765957\n650  329    82   247 0.2492401\n651  329    85   244 0.2583587\n652  329    69   260 0.2097264\n653  329    93   236 0.2826748\n654  329    82   247 0.2492401\n655  329    88   241 0.2674772\n656  329    85   244 0.2583587\n657  329    88   241 0.2674772\n658  329    76   253 0.2310030\n659  329    91   238 0.2765957\n660  329    91   238 0.2765957\n661  329    87   242 0.2644377\n662  329    78   251 0.2370821\n663  329    70   259 0.2127660\n664  329    79   250 0.2401216\n665  329    77   252 0.2340426\n666  329    78   251 0.2370821\n667  329    80   249 0.2431611\n668  329    93   236 0.2826748\n669  329    82   247 0.2492401\n670  329    86   243 0.2613982\n671  329    89   240 0.2705167\n672  329    76   253 0.2310030\n673  329    87   242 0.2644377\n674  329    98   231 0.2978723\n675  329    77   252 0.2340426\n676  329   100   229 0.3039514\n677  329    81   248 0.2462006\n678  329    74   255 0.2249240\n679  329    93   236 0.2826748\n680  329    71   258 0.2158055\n681  329    86   243 0.2613982\n682  329    81   248 0.2462006\n683  329    86   243 0.2613982\n684  329    79   250 0.2401216\n685  329    73   256 0.2218845\n686  329    84   245 0.2553191\n687  329    84   245 0.2553191\n688  329    81   248 0.2462006\n689  329    80   249 0.2431611\n690  329    79   250 0.2401216\n691  329    82   247 0.2492401\n692  329    85   244 0.2583587\n693  329    83   246 0.2522796\n694  329    82   247 0.2492401\n695  329    81   248 0.2462006\n696  329    92   237 0.2796353\n697  329    86   243 0.2613982\n698  329    74   255 0.2249240\n699  329    88   241 0.2674772\n700  329    80   249 0.2431611\n701  329    82   247 0.2492401\n702  329    79   250 0.2401216\n703  329    76   253 0.2310030\n704  329    75   254 0.2279635\n705  329    87   242 0.2644377\n706  329    76   253 0.2310030\n707  329    86   243 0.2613982\n708  329    82   247 0.2492401\n709  329    86   243 0.2613982\n710  329    83   246 0.2522796\n711  329    82   247 0.2492401\n712  329    96   233 0.2917933\n713  329    77   252 0.2340426\n714  329    78   251 0.2370821\n715  329    98   231 0.2978723\n716  329    84   245 0.2553191\n717  329    78   251 0.2370821\n718  329    93   236 0.2826748\n719  329    83   246 0.2522796\n720  329    87   242 0.2644377\n721  329    88   241 0.2674772\n722  329    81   248 0.2462006\n723  329    75   254 0.2279635\n724  329    72   257 0.2188450\n725  329    95   234 0.2887538\n726  329    92   237 0.2796353\n727  329    85   244 0.2583587\n728  329    82   247 0.2492401\n729  329    87   242 0.2644377\n730  329    73   256 0.2218845\n731  329    85   244 0.2583587\n732  329    80   249 0.2431611\n733  329   105   224 0.3191489\n734  329    66   263 0.2006079\n735  329    88   241 0.2674772\n736  329    77   252 0.2340426\n737  329    69   260 0.2097264\n738  329    89   240 0.2705167\n739  329    86   243 0.2613982\n740  329    71   258 0.2158055\n741  329    73   256 0.2218845\n742  329    87   242 0.2644377\n743  329    79   250 0.2401216\n744  329    84   245 0.2553191\n745  329    90   239 0.2735562\n746  329    83   246 0.2522796\n747  329    84   245 0.2553191\n748  329    96   233 0.2917933\n749  329    79   250 0.2401216\n750  329    78   251 0.2370821\n751  329    84   245 0.2553191\n752  329    85   244 0.2583587\n753  329    76   253 0.2310030\n754  329    82   247 0.2492401\n755  329    86   243 0.2613982\n756  329    84   245 0.2553191\n757  329    80   249 0.2431611\n758  329    85   244 0.2583587\n759  329    93   236 0.2826748\n760  329    77   252 0.2340426\n761  329    77   252 0.2340426\n762  329    85   244 0.2583587\n763  329    80   249 0.2431611\n764  329    69   260 0.2097264\n765  329    91   238 0.2765957\n766  329    73   256 0.2218845\n767  329    84   245 0.2553191\n768  329    95   234 0.2887538\n769  329    89   240 0.2705167\n770  329    75   254 0.2279635\n771  329    94   235 0.2857143\n772  329    84   245 0.2553191\n773  329    84   245 0.2553191\n774  329    78   251 0.2370821\n775  329    80   249 0.2431611\n776  329    88   241 0.2674772\n777  329    72   257 0.2188450\n778  329    85   244 0.2583587\n779  329    82   247 0.2492401\n780  329    89   240 0.2705167\n781  329    70   259 0.2127660\n782  329    72   257 0.2188450\n783  329    86   243 0.2613982\n784  329    73   256 0.2218845\n785  329    84   245 0.2553191\n786  329    72   257 0.2188450\n787  329    78   251 0.2370821\n788  329    85   244 0.2583587\n789  329    84   245 0.2553191\n790  329    70   259 0.2127660\n791  329    83   246 0.2522796\n792  329    80   249 0.2431611\n793  329    79   250 0.2401216\n794  329    93   236 0.2826748\n795  329    97   232 0.2948328\n796  329    83   246 0.2522796\n797  329    87   242 0.2644377\n798  329    82   247 0.2492401\n799  329    73   256 0.2218845\n800  329    77   252 0.2340426\n801  329    77   252 0.2340426\n802  329    87   242 0.2644377\n803  329    71   258 0.2158055\n804  329    77   252 0.2340426\n805  329    60   269 0.1823708\n806  329    75   254 0.2279635\n807  329    84   245 0.2553191\n808  329    76   253 0.2310030\n809  329    79   250 0.2401216\n810  329    80   249 0.2431611\n811  329    79   250 0.2401216\n812  329    82   247 0.2492401\n813  329    93   236 0.2826748\n814  329    70   259 0.2127660\n815  329    76   253 0.2310030\n816  329   102   227 0.3100304\n817  329    77   252 0.2340426\n818  329    83   246 0.2522796\n819  329    82   247 0.2492401\n820  329    78   251 0.2370821\n821  329    85   244 0.2583587\n822  329    96   233 0.2917933\n823  329    92   237 0.2796353\n824  329    67   262 0.2036474\n825  329   101   228 0.3069909\n826  329    85   244 0.2583587\n827  329    72   257 0.2188450\n828  329    83   246 0.2522796\n829  329    93   236 0.2826748\n830  329    79   250 0.2401216\n831  329    82   247 0.2492401\n832  329   100   229 0.3039514\n833  329    87   242 0.2644377\n834  329    81   248 0.2462006\n835  329    90   239 0.2735562\n836  329    87   242 0.2644377\n837  329    73   256 0.2218845\n838  329    91   238 0.2765957\n839  329    83   246 0.2522796\n840  329    95   234 0.2887538\n841  329    80   249 0.2431611\n842  329    82   247 0.2492401\n843  329    77   252 0.2340426\n844  329    91   238 0.2765957\n845  329    88   241 0.2674772\n846  329    83   246 0.2522796\n847  329    79   250 0.2401216\n848  329    75   254 0.2279635\n849  329    88   241 0.2674772\n850  329    80   249 0.2431611\n851  329    89   240 0.2705167\n852  329    86   243 0.2613982\n853  329    76   253 0.2310030\n854  329    85   244 0.2583587\n855  329    79   250 0.2401216\n856  329    71   258 0.2158055\n857  329    81   248 0.2462006\n858  329    83   246 0.2522796\n859  329    79   250 0.2401216\n860  329    79   250 0.2401216\n861  329    68   261 0.2066869\n862  329    87   242 0.2644377\n863  329    70   259 0.2127660\n864  329    81   248 0.2462006\n865  329    68   261 0.2066869\n866  329    85   244 0.2583587\n867  329    72   257 0.2188450\n868  329    77   252 0.2340426\n869  329    79   250 0.2401216\n870  329    91   238 0.2765957\n871  329    76   253 0.2310030\n872  329    77   252 0.2340426\n873  329    92   237 0.2796353\n874  329    86   243 0.2613982\n875  329    78   251 0.2370821\n876  329    96   233 0.2917933\n877  329    83   246 0.2522796\n878  329    84   245 0.2553191\n879  329    79   250 0.2401216\n880  329    90   239 0.2735562\n881  329    78   251 0.2370821\n882  329    75   254 0.2279635\n883  329    89   240 0.2705167\n884  329    93   236 0.2826748\n885  329    78   251 0.2370821\n886  329    75   254 0.2279635\n887  329    81   248 0.2462006\n888  329    75   254 0.2279635\n889  329    77   252 0.2340426\n890  329    93   236 0.2826748\n891  329    92   237 0.2796353\n892  329    91   238 0.2765957\n893  329    89   240 0.2705167\n894  329    92   237 0.2796353\n895  329    89   240 0.2705167\n896  329    96   233 0.2917933\n897  329    86   243 0.2613982\n898  329    81   248 0.2462006\n899  329    75   254 0.2279635\n900  329    82   247 0.2492401\n901  329    83   246 0.2522796\n902  329    77   252 0.2340426\n903  329    85   244 0.2583587\n904  329    79   250 0.2401216\n905  329    76   253 0.2310030\n906  329    83   246 0.2522796\n907  329    67   262 0.2036474\n908  329    67   262 0.2036474\n909  329    94   235 0.2857143\n910  329    78   251 0.2370821\n911  329    74   255 0.2249240\n912  329    81   248 0.2462006\n913  329    83   246 0.2522796\n914  329    92   237 0.2796353\n915  329    79   250 0.2401216\n916  329    78   251 0.2370821\n917  329    90   239 0.2735562\n918  329    99   230 0.3009119\n919  329    74   255 0.2249240\n920  329    94   235 0.2857143\n921  329    92   237 0.2796353\n922  329    78   251 0.2370821\n923  329    95   234 0.2887538\n924  329    68   261 0.2066869\n925  329    81   248 0.2462006\n926  329    99   230 0.3009119\n927  329    84   245 0.2553191\n928  329    88   241 0.2674772\n929  329    86   243 0.2613982\n930  329    68   261 0.2066869\n931  329    85   244 0.2583587\n932  329    81   248 0.2462006\n933  329    97   232 0.2948328\n934  329    89   240 0.2705167\n935  329    79   250 0.2401216\n936  329    87   242 0.2644377\n937  329    90   239 0.2735562\n938  329    92   237 0.2796353\n939  329    82   247 0.2492401\n940  329    86   243 0.2613982\n941  329    80   249 0.2431611\n942  329    75   254 0.2279635\n943  329    79   250 0.2401216\n944  329    71   258 0.2158055\n945  329    94   235 0.2857143\n946  329    77   252 0.2340426\n947  329    93   236 0.2826748\n948  329    84   245 0.2553191\n949  329    79   250 0.2401216\n950  329    87   242 0.2644377\n951  329    80   249 0.2431611\n952  329    84   245 0.2553191\n953  329    78   251 0.2370821\n954  329    97   232 0.2948328\n955  329    71   258 0.2158055\n956  329    74   255 0.2249240\n957  329    93   236 0.2826748\n958  329    86   243 0.2613982\n959  329    81   248 0.2462006\n960  329    96   233 0.2917933\n961  329    79   250 0.2401216\n962  329    78   251 0.2370821\n963  329    84   245 0.2553191\n964  329    99   230 0.3009119\n965  329    78   251 0.2370821\n966  329    77   252 0.2340426\n967  329    84   245 0.2553191\n968  329    84   245 0.2553191\n969  329    85   244 0.2583587\n970  329    88   241 0.2674772\n971  329    86   243 0.2613982\n972  329    78   251 0.2370821\n973  329    89   240 0.2705167\n974  329    76   253 0.2310030\n975  329    82   247 0.2492401\n976  329    86   243 0.2613982\n977  329    80   249 0.2431611\n978  329    95   234 0.2887538\n979  329    73   256 0.2218845\n980  329    81   248 0.2462006\n981  329    78   251 0.2370821\n982  329    71   258 0.2158055\n983  329    86   243 0.2613982\n984  329    77   252 0.2340426\n985  329    86   243 0.2613982\n986  329    93   236 0.2826748\n987  329    77   252 0.2340426\n988  329    82   247 0.2492401\n989  329    80   249 0.2431611\n990  329    92   237 0.2796353\n991  329    73   256 0.2218845\n992  329    89   240 0.2705167\n993  329    84   245 0.2553191\n994  329    86   243 0.2613982\n995  329    85   244 0.2583587\n996  329    70   259 0.2127660\n997  329    84   245 0.2553191\n998  329    85   244 0.2583587\n999  329    75   254 0.2279635\n1000 329    82   247 0.2492401"
  },
  {
    "objectID": "stat100_wk09wed.html#sampling-distribution-under-no-esp-2",
    "href": "stat100_wk09wed.html#sampling-distribution-under-no-esp-2",
    "title": "Stat 100",
    "section": "Sampling Distribution Under No ESP",
    "text": "Sampling Distribution Under No ESP\n\n\nggplot(data = guess_sampling_dist,\n       mapping = aes(x = prop)) +\n  geom_histogram(color = \"white\",\n                 bins = 20)\n\n\n\n\n\n\n\n\n\n\nWhat value should our sampling distribution be centered around if the receivers are just guessing?"
  },
  {
    "objectID": "stat100_wk09wed.html#sampling-distribution-under-no-esp-3",
    "href": "stat100_wk09wed.html#sampling-distribution-under-no-esp-3",
    "title": "Stat 100",
    "section": "Sampling Distribution Under No ESP",
    "text": "Sampling Distribution Under No ESP\n\n\nHow do the study results compare to the sampling distribution under no ESP?\n\nHow unusual is it to guess correctly 106 out of 329 times if ESP doesn’t exist?\n\n\n\n\n\np_hat &lt;- 106/329\nggplot(data = guess_sampling_dist,\n       mapping = aes(x = prop)) +\n  geom_histogram(color = \"white\",\n                 bins = 20) +\n  geom_vline(xintercept = p_hat,\n             color = \"orange\",\n             size = 2)\n\n\n\n\n\n\n\n\n\n\n\nDo Bem and Honorton have evidence that ESP exists?"
  },
  {
    "objectID": "stat100_wk09wed.html#do-harvardians-have-esp",
    "href": "stat100_wk09wed.html#do-harvardians-have-esp",
    "title": "Stat 100",
    "section": "Do Harvardians Have ESP?",
    "text": "Do Harvardians Have ESP?\n\n\nIn pairs:\n\n\nDecide who is going to be the sender and who is going to be the receiver.\nSender: Think of one of these images.\nReceiver: Guess which image the sender was thinking of.\nNow switch roles and do it again!\nOnce you have both played each role, each person should add a tally mark on the chalkboard."
  },
  {
    "objectID": "stat100_wk09wed.html#do-harvardians-have-esp-1",
    "href": "stat100_wk09wed.html#do-harvardians-have-esp-1",
    "title": "Stat 100",
    "section": "Do Harvardians Have ESP?",
    "text": "Do Harvardians Have ESP?\nWhat do we need to modify in the code to answer the question?\n\nguess_sampling_dist &lt;- do(1000)*rflip(n = 80, prob = 0.25)\np_hat &lt;- 27/80\nggplot(data = guess_sampling_dist, mapping = aes(x = prop)) +\n  geom_histogram(color = \"white\", bins = 20) +\n  geom_vline(xintercept = p_hat, color = \"orange\", size = 2)"
  },
  {
    "objectID": "stat100_wk09wed.html#reminders",
    "href": "stat100_wk09wed.html#reminders",
    "title": "Stat 100",
    "section": "Reminders:",
    "text": "Reminders:\n\n\nDon’t forget that the midterm exam rewrites are due on Thursday at 5pm on Gradescope.\n\nMake sure to use the Quarto doc in the Midterm Exam (Rewrites) project on Posit Cloud.\n\n🎉 We are now accepting Course Assistant/Teaching Fellow applications for Stat 100 for next semester. To apply, fill out this application by Nov 15th.\n\nAbout 10-12 hours of work per week.\n\nPrimary responsibilities: Attend weekly team meetings, lead a discussion section, hold office hours, grade assessments."
  },
  {
    "objectID": "stat100_wk05wed.html#announcements",
    "href": "stat100_wk05wed.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nNo lecture on Monday – University Holiday.\nSome Monday Office Hours will also be cancelled. Make sure to check the office hours schedule.\nDiscuss upcoming exam:\n\nMidterm next week\n\nIn-class: Wed, Oct 11th 10:30 - 11:45am\nOral: Wed afternoon - Fri, Oct 13th\nNo sections during midterm exam week!\n\n\n\nGoals for Today\n\n\n\nIntroduce statistical modeling\nSimple linear regression model\n\n\n\nMeasuring correlation"
  },
  {
    "objectID": "stat100_wk05wed.html#thoughts-on-data-collection-goals",
    "href": "stat100_wk05wed.html#thoughts-on-data-collection-goals",
    "title": "Stat 100",
    "section": "Thoughts on Data Collection Goals",
    "text": "Thoughts on Data Collection Goals\n\nRandom assignment allows you to explore causal relationships between your explanatory variables and the predictor variables because the randomization makes the explanatory groups roughly similar.\nHow do we draw causal conclusions from studies without random assignment?\n\nWith extreme care! Try to control for all possible confounding variables.\nDiscuss the associations/correlations you found. Use domain knowledge to address potentially causal links.\nTake more stats to learn more about causal inference.\n\nBut also consider the goals of your analysis. Often the research question isn’t causal.\n\n\nBottom Line: We often have to use imperfect data to make decisions."
  },
  {
    "objectID": "stat100_wk05wed.html#conclusions-conclusions",
    "href": "stat100_wk05wed.html#conclusions-conclusions",
    "title": "Stat 100",
    "section": "Conclusions, Conclusions",
    "text": "Conclusions, Conclusions"
  },
  {
    "objectID": "stat100_wk05wed.html#recap",
    "href": "stat100_wk05wed.html#recap",
    "title": "Stat 100",
    "section": "Recap",
    "text": "Recap"
  },
  {
    "objectID": "stat100_wk05wed.html#form-of-the-model",
    "href": "stat100_wk05wed.html#form-of-the-model",
    "title": "Stat 100",
    "section": "Form of the Model",
    "text": "Form of the Model\n\n\\[\ny = f(x) + \\epsilon\n\\]\n\nGoal:\n\nDetermine a reasonable form for \\(f()\\). (Ex: Line, curve, …)\nEstimate \\(f()\\) with \\(\\hat{f}()\\) using the data.\nGenerate predicted values: \\(\\hat y = \\hat{f}(x)\\)."
  },
  {
    "objectID": "stat100_wk05wed.html#sample-correlation-coefficient",
    "href": "stat100_wk05wed.html#sample-correlation-coefficient",
    "title": "Stat 100",
    "section": "(Sample) Correlation Coefficient",
    "text": "(Sample) Correlation Coefficient\n\nMeasures the strength and direction of linear relationship between two quantitative variables\nSymbol: \\(r\\)\nAlways between -1 and 1\nSign indicates the direction of the relationship\nMagnitude indicates the strength of the linear relationship\n\n\n\ncandy %&gt;%\n  summarize(cor = cor(pricepercent, winpercent))\n\n# A tibble: 1 × 1\n    cor\n  &lt;dbl&gt;\n1 0.345"
  },
  {
    "objectID": "stat100_wk05wed.html#new-example",
    "href": "stat100_wk05wed.html#new-example",
    "title": "Stat 100",
    "section": "New Example",
    "text": "New Example\n\n\n\n# Correlation coefficients\ndat2 %&gt;%\n  group_by(dataset) %&gt;%\n  summarize(cor = cor(x, y))\n\n# A tibble: 12 × 2\n   dataset        cor\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 away       -0.0641\n 2 bullseye   -0.0686\n 3 circle     -0.0683\n 4 dino       -0.0645\n 5 dots       -0.0603\n 6 h_lines    -0.0617\n 7 high_lines -0.0685\n 8 slant_down -0.0690\n 9 star       -0.0630\n10 v_lines    -0.0694\n11 wide_lines -0.0666\n12 x_shape    -0.0656\n\n\n\n\nConclude that \\(x\\) and \\(y\\) have the same relationship across these different datasets because the correlation is the same?"
  },
  {
    "objectID": "stat100_wk05wed.html#reminders",
    "href": "stat100_wk05wed.html#reminders",
    "title": "Stat 100",
    "section": "Reminders",
    "text": "Reminders\n\nNo lecture on Monday – University Holiday.\nSome Monday Office Hours will also be cancelled. Make sure to check the office hours schedule.\nMidterm next week\n\nIn-class: Wed, Oct 11th 10:30 - 11:45am\nOral: Wed afternoon - Fri, Oct 13th\n\nNo sections during midterm exam week!"
  },
  {
    "objectID": "stat100_wk10wed.html#announcements",
    "href": "stat100_wk10wed.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\n🎉 We are now accepting Course Assistant/Teaching Fellow applications for Stat 100 for next semester. To apply, fill out this application by Nov 15th.\nNo wrap-up session on Friday due to the university holiday.\n\nYou are all invited to the Info Session on Data Science Internships: Mon at 4pm in SC 316!\n\nGoals for Today\n\n\n\nCoding goals (Stat 100 & beyond)\nAdvice on the next stats/coding class\n\n\n\nDecisions in a hypothesis test\n\nTypes of errors\n\nThe power of a hypothesis test"
  },
  {
    "objectID": "stat100_wk10wed.html#that-next-stats-class",
    "href": "stat100_wk10wed.html#that-next-stats-class",
    "title": "Stat 100",
    "section": "That Next Stats Class",
    "text": "That Next Stats Class\n\nBut first… Common Question: How should I describe my post-Stat 100 coding abilities?\nPotential Answer: You have learned how to write code to analyze data. This includes visualization (ggplot2), data wrangling (dplyr), data importation (readr), modeling, inference (infer) and communication (with Quarto).\nFollow-up Question: So what coding is there left to learn?\nAnswer: Learning how to program. This includes topics such as control flow, iteration, creating functions, and vectorization."
  },
  {
    "objectID": "stat100_wk10wed.html#that-next-coding-class",
    "href": "stat100_wk10wed.html#that-next-coding-class",
    "title": "Stat 100",
    "section": "That Next Coding Class",
    "text": "That Next Coding Class\n\n\nStat 108: Introduction to Statistical Computing with R\nCompSci 32: Computational Thinking and Problem Solving\nCompSci 50: Introduction to Computer Science\nAP 10: Computing with Python for Scientists and Engineers"
  },
  {
    "objectID": "stat100_wk10wed.html#that-next-modeling-course",
    "href": "stat100_wk10wed.html#that-next-modeling-course",
    "title": "Stat 100",
    "section": "That Next Modeling Course",
    "text": "That Next Modeling Course\n\n\nStat 109A: Data Science I & Stat 109B: Data Science II\nStat 139: Linear Models\nMany of the upper-level stats courses are modeling courses (but they do have pre-reqs)."
  },
  {
    "objectID": "stat100_wk10wed.html#that-next-theorymethods-course",
    "href": "stat100_wk10wed.html#that-next-theorymethods-course",
    "title": "Stat 100",
    "section": "That Next Theory/Methods Course",
    "text": "That Next Theory/Methods Course\n\n\nStat 110: Introduction to Probability\nStat 111: Introduction to Statistical Inference"
  },
  {
    "objectID": "stat100_wk10wed.html#that-next-visualization-course",
    "href": "stat100_wk10wed.html#that-next-visualization-course",
    "title": "Stat 100",
    "section": "That Next Visualization Course",
    "text": "That Next Visualization Course\n\n\nStat 108: Introduction to Statistical Computing with R\nCompSci 171: Visualization\nStat 106: Data Science for Sports Analytics\n\nNot on the books yet but should be coming next academic year."
  },
  {
    "objectID": "stat100_wk10wed.html#penguins-example",
    "href": "stat100_wk10wed.html#penguins-example",
    "title": "Stat 100",
    "section": "Penguins Example",
    "text": "Penguins Example\nLet’s return to the penguins data and ask if flipper length varies, on average, by the sex of the penguin.\nResearch Question: Does flipper length differ by sex?\nResponse Variable:\n\nExplanatory Variable:\n\nStatistical Hypotheses:"
  },
  {
    "objectID": "stat100_wk10wed.html#exploratory-data-analysis",
    "href": "stat100_wk10wed.html#exploratory-data-analysis",
    "title": "Stat 100",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\n\nlibrary(infer)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\npenguins %&gt;%\n  drop_na(sex) %&gt;%\nggplot(mapping = aes(x = sex,\n                     y = flipper_length_mm)) +\n  geom_boxplot()"
  },
  {
    "objectID": "stat100_wk10wed.html#two-sided-hypothesis-test",
    "href": "stat100_wk10wed.html#two-sided-hypothesis-test",
    "title": "Stat 100",
    "section": "Two-Sided Hypothesis Test",
    "text": "Two-Sided Hypothesis Test\n\n\n# Compute observed test statistic\ntest_stat &lt;- penguins %&gt;%\n  drop_na(sex) %&gt;%\n  specify(flipper_length_mm ~ sex) %&gt;%\n  calculate(stat =\"diff in means\",\n            order = c(\"female\", \"male\"))\ntest_stat\n\n\nResponse: flipper_length_mm (numeric)\nExplanatory: sex (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 -7.14\n\n\n\n\n# Generate null distribution \nnull_dist &lt;- penguins %&gt;%\n  drop_na(sex) %&gt;%\n  specify(flipper_length_mm ~ sex) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  calculate(stat =\"diff in means\",\n            order = c(\"female\", \"male\"))"
  },
  {
    "objectID": "stat100_wk10wed.html#two-sided-hypothesis-test-1",
    "href": "stat100_wk10wed.html#two-sided-hypothesis-test-1",
    "title": "Stat 100",
    "section": "Two-Sided Hypothesis Test",
    "text": "Two-Sided Hypothesis Test\n\n\n# Graph null distribution with test statistic\nvisualize(null_dist) +\n  geom_vline(xintercept = test_stat$stat,\n             color = \"deeppink\", size = 2) +\n  geom_vline(xintercept = abs(test_stat$stat),\n             color = \"deeppink\", size = 2)"
  },
  {
    "objectID": "stat100_wk10wed.html#two-sided-hypothesis-test-2",
    "href": "stat100_wk10wed.html#two-sided-hypothesis-test-2",
    "title": "Stat 100",
    "section": "Two-Sided Hypothesis Test",
    "text": "Two-Sided Hypothesis Test\n\n\n# Compute p-value\np_value &lt;- null_dist %&gt;%\n  get_p_value(obs_stat = test_stat,\n              direction = \"two_sided\")\np_value\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\n\nInterpretation of \\(p\\)-value: If the mean flipper length does not differ by sex in the population, the probability of observing a difference in the sample means of at least 7.142316 mm (in magnitude) is equal to 0.\nConclusion: These data represent evidence that flipper length does vary by sex."
  },
  {
    "objectID": "stat100_wk10wed.html#thoughts-on-power",
    "href": "stat100_wk10wed.html#thoughts-on-power",
    "title": "Stat 100",
    "section": "Thoughts on Power",
    "text": "Thoughts on Power\n\nWhat aspects of the test did the player actually have control over?\nWhy is it easier to set \\(\\alpha\\) than to set \\(\\beta\\) or power?\nConsidering power before collecting data is very important!\nThe danger of under-powered studies\n\nEX: Turning right at a red light"
  },
  {
    "objectID": "stat100_wk10wed.html#reminders",
    "href": "stat100_wk10wed.html#reminders",
    "title": "Stat 100",
    "section": "Reminders:",
    "text": "Reminders:\n\n\n🎉 We are now accepting Course Assistant/Teaching Fellow applications for Stat 100 for next semester. To apply, fill out this application by Nov 15th.\n\nAbout 10-12 hours of work per week.\n\nPrimary responsibilities: Attend weekly team meetings, lead a discussion section, hold office hours, grade assessments."
  },
  {
    "objectID": "stat100_wk11mon.html#announcements",
    "href": "stat100_wk11mon.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\n🎉 We are now accepting Course Assistant/Teaching Fellow applications for Stat 100 for next semester. To apply, fill out this application by Nov 15th.\nYou are all invited to the Info Session on Data Science Internships today at 4pm in SC 316!\n\nGoals for Today\n\n\n\nFinish discussion power\nStatistical inference zoom out\n\n\n\nA hearty p-values discussion\nKey probability concepts"
  },
  {
    "objectID": "stat100_wk11mon.html#thoughts-on-power",
    "href": "stat100_wk11mon.html#thoughts-on-power",
    "title": "Stat 100",
    "section": "Thoughts on Power",
    "text": "Thoughts on Power\n\n\n\nWhat aspects of the test did the player actually have control over?\nWhy is it easier to set \\(\\alpha\\) than to set \\(\\beta\\) or power?\nConsidering power before conducting a study is very important!\nThe danger of under-powered studies\n\nEX: Turning right at a red light"
  },
  {
    "objectID": "stat100_wk11mon.html#reminders",
    "href": "stat100_wk11mon.html#reminders",
    "title": "Stat 100",
    "section": "Reminders:",
    "text": "Reminders:\n\n\n🎉 We are now accepting Course Assistant/Teaching Fellow applications for Stat 100 for next semester. To apply, fill out this application by Nov 15th.\n\nAbout 10-12 hours of work per week.\n\nPrimary responsibilities: Attend weekly team meetings, lead a discussion section, hold office hours, grade assessments.\n\nYou are all invited to the Info Session on Data Science Internships today at 4pm in SC 316!"
  },
  {
    "objectID": "stat100_wk10mon.html#announcements",
    "href": "stat100_wk10mon.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\n🎉 We are now accepting Course Assistant/Teaching Fellow applications for Stat 100 for next semester. To apply, fill out this application by Nov 15th.\n\nAbout 10-12 hours of work per week.\n\nPrimary responsibilities: Attend weekly team meetings, lead a discussion section, hold office hours, grade assessments.\n\n\nGoals for Today\n\n\n\nLearn the language of hypothesis testing (including p-values)\nPractice framing research questions in terms of hypotheses\n\n\n\nLearn how to generate null distributions\nUse infer to conduct hypothesis tests in R"
  },
  {
    "objectID": "stat100_wk10mon.html#generating-null-distributions",
    "href": "stat100_wk10mon.html#generating-null-distributions",
    "title": "Stat 100",
    "section": "Generating Null Distributions",
    "text": "Generating Null Distributions\nFor the sample proportion in the ESP Example:\n\n\nSteps:\n\nFlip unfair coin (prop heads = 0.25) 329 times.\nCompute proportion of heads.\nRepeat 1 and 2 many times.\n\n\n\nR code using the infer package:\n\nlibrary(infer)\n\n# Construct data frame of sample results\nesp &lt;- data.frame(guess = c(rep(\"correct\", 106),\n                            rep(\"incorrect\",\n                                329 - 106)))\n\n# Generate Null Distribution\nnull_dist &lt;- esp %&gt;%\n  specify(response = guess, success = \"correct\") %&gt;%\n  hypothesize(null = \"point\", p = 0.25) %&gt;%\n  generate(reps = 1000, type = \"draw\") %&gt;%\n  calculate(stat =\"prop\")\n\n\n\n\n\nFor different variable types, we will need to move beyond using a coin to conceptualize the null distribution."
  },
  {
    "objectID": "stat100_wk10mon.html#hypothesis-testing-in-r",
    "href": "stat100_wk10mon.html#hypothesis-testing-in-r",
    "title": "Stat 100",
    "section": "Hypothesis Testing in R",
    "text": "Hypothesis Testing in R\n\n\n# Compute observed test statistic\ntest_stat &lt;- esp %&gt;%\n  specify(response = guess, success = \"correct\") %&gt;%\n  calculate(stat =\"prop\")\ntest_stat\n\n\nResponse: guess (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 0.322\n\ntest_stat &lt;- 106/329\ntest_stat\n\n[1] 0.3221884"
  },
  {
    "objectID": "stat100_wk10mon.html#hypothesis-testing-in-r-1",
    "href": "stat100_wk10mon.html#hypothesis-testing-in-r-1",
    "title": "Stat 100",
    "section": "Hypothesis Testing in R",
    "text": "Hypothesis Testing in R\n\n\n# Graph null distribution with test statistic\nvisualize(null_dist) +\n  geom_vline(xintercept = test_stat,\n             color = \"deeppink\", size = 2)"
  },
  {
    "objectID": "stat100_wk10mon.html#hypothesis-testing-in-r-2",
    "href": "stat100_wk10mon.html#hypothesis-testing-in-r-2",
    "title": "Stat 100",
    "section": "Hypothesis Testing in R",
    "text": "Hypothesis Testing in R\n\n\n# Compute p-value\np_value &lt;- null_dist %&gt;%\n  get_p_value(obs_stat = test_stat,\n              direction = \"right\")\np_value\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.001\n\n\n\n\nInterpretation of \\(p\\)-value: If ESP doesn’t exist, the probability of observing 106 or more correct identifications out of 329 trials equals 0.001.\n\n\nConclusion: Since it is so unlikely (i.e., practically impossible) to have seen such unusual results if ESP doesn’t exist, these data suggest that ESP does exist."
  },
  {
    "objectID": "stat100_wk10mon.html#penguins-example",
    "href": "stat100_wk10mon.html#penguins-example",
    "title": "Stat 100",
    "section": "Penguins Example",
    "text": "Penguins Example\nLet’s return to the penguins data and ask if flipper length varies, on average, by the sex of the penguin.\nResearch Question: Does flipper length differ by sex?\nResponse Variable:\n\nExplanatory Variable:\n\nStatistical Hypotheses:"
  },
  {
    "objectID": "stat100_wk10mon.html#exploratory-data-analysis",
    "href": "stat100_wk10mon.html#exploratory-data-analysis",
    "title": "Stat 100",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\n\n\nlibrary(palmerpenguins)\n\npenguins %&gt;%\n  drop_na(sex) %&gt;%\nggplot(mapping = aes(x = sex,\n                     y = flipper_length_mm)) +\n  geom_boxplot()"
  },
  {
    "objectID": "stat100_wk10mon.html#two-sided-hypothesis-test",
    "href": "stat100_wk10mon.html#two-sided-hypothesis-test",
    "title": "Stat 100",
    "section": "Two-Sided Hypothesis Test",
    "text": "Two-Sided Hypothesis Test\n\n\n# Compute observed test statistic\ntest_stat &lt;- penguins %&gt;%\n  drop_na(sex) %&gt;%\n  specify(flipper_length_mm ~ sex) %&gt;%\n  calculate(stat =\"diff in means\",\n            order = c(\"female\", \"male\"))\ntest_stat\n\n\nResponse: flipper_length_mm (numeric)\nExplanatory: sex (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 -7.14\n\n\n\n\n# Generate null distribution \nnull_dist &lt;- penguins %&gt;%\n  drop_na(sex) %&gt;%\n  specify(flipper_length_mm ~ sex) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  calculate(stat =\"diff in means\",\n            order = c(\"female\", \"male\"))"
  },
  {
    "objectID": "stat100_wk10mon.html#two-sided-hypothesis-test-1",
    "href": "stat100_wk10mon.html#two-sided-hypothesis-test-1",
    "title": "Stat 100",
    "section": "Two-Sided Hypothesis Test",
    "text": "Two-Sided Hypothesis Test\n\n\n# Graph null distribution with test statistic\nvisualize(null_dist) +\n  geom_vline(xintercept = test_stat$stat,\n             color = \"deeppink\", size = 2) +\n  geom_vline(xintercept = abs(test_stat$stat),\n             color = \"deeppink\", size = 2)"
  },
  {
    "objectID": "stat100_wk10mon.html#two-sided-hypothesis-test-2",
    "href": "stat100_wk10mon.html#two-sided-hypothesis-test-2",
    "title": "Stat 100",
    "section": "Two-Sided Hypothesis Test",
    "text": "Two-Sided Hypothesis Test\n\n\n# Compute p-value\np_value &lt;- null_dist %&gt;%\n  get_p_value(obs_stat = test_stat,\n              direction = \"two_sided\")\np_value\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\n\nInterpretation of \\(p\\)-value: If the mean flipper length does not differ by sex in the population, the probability of observing a difference in the sample means of at least 7.142316 mm (in magnitude) is equal to 0.\nConclusion: These data represent evidence that flipper length does vary by sex."
  },
  {
    "objectID": "stat100_wk10mon.html#reminders",
    "href": "stat100_wk10mon.html#reminders",
    "title": "Stat 100",
    "section": "Reminders:",
    "text": "Reminders:\n\n\n🎉 We are now accepting Course Assistant/Teaching Fellow applications for Stat 100 for next semester. To apply, fill out this application by Nov 15th.\n\nAbout 10-12 hours of work per week.\n\nPrimary responsibilities: Attend weekly team meetings, lead a discussion section, hold office hours, grade assessments."
  },
  {
    "objectID": "stat100_wk01wed.html#keyword",
    "href": "stat100_wk01wed.html#keyword",
    "title": "Stat 100",
    "section": "",
    "text": "What words or phrases do you think of when you hear the word “Harvard”?\n\n\nThis being a data class, I’d like to collect some data related to “statistical thinking.”\n\n\nGo to bit.ly/stat-100-think to provide the words or phrases you think of when you hear “statistical thinking.”"
  },
  {
    "objectID": "stat100_wk01wed.html#getting-started-in-stat-100",
    "href": "stat100_wk01wed.html#getting-started-in-stat-100",
    "title": "Stat 100",
    "section": "Getting Started in Stat 100",
    "text": "Getting Started in Stat 100\nStep 1: Getting Started Module in Canvas"
  },
  {
    "objectID": "stat100_wk01wed.html#stat-100-tech-materials",
    "href": "stat100_wk01wed.html#stat-100-tech-materials",
    "title": "Stat 100",
    "section": "Stat 100 Tech & Materials",
    "text": "Stat 100 Tech & Materials"
  },
  {
    "objectID": "stat100_wk01wed.html#announcements",
    "href": "stat100_wk01wed.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nLecture slide decks will always be posted and linked to a Canvas Module the day before lecture.\n\nWill also bring printed versions for those who prefer paper copies.\n\nNo section and no lecture quiz this week.\n\nBut be on the look-out for section preference form from my.harvard.\n\nOnly I will be running office hours this week at the following time:\n\nToday 1:30 - 3:00 pm in Science Center 316 (This week only)\n\nThe regular office hour schedule will be posted later this week and will start next week.\nIf able, please bring a laptop or tablet to Mondays’s lecture."
  },
  {
    "objectID": "stat100_wk01wed.html#day-1-goals",
    "href": "stat100_wk01wed.html#day-1-goals",
    "title": "Stat 100",
    "section": "Day 1 Goals",
    "text": "Day 1 Goals\n\nStart engaging in statistical thinking\nIntroduce data\nConsider hand-drawn visualizations as a way to tell stories with data\nHop into the RStudio Server using Posit Cloud"
  },
  {
    "objectID": "stat100_wk01wed.html#looking-ahead-to-day-2",
    "href": "stat100_wk01wed.html#looking-ahead-to-day-2",
    "title": "Stat 100",
    "section": "Looking Ahead to Day 2…",
    "text": "Looking Ahead to Day 2…\n\nDiscuss course structure (lecture, section, wrap-ups, office hours, assessments…)\nPresent important course policies (engagement, code of conduct, chatGPT, …)\nGet started in RStudio and with Quarto documents"
  },
  {
    "objectID": "stat100_wk01wed.html#research-interests",
    "href": "stat100_wk01wed.html#research-interests",
    "title": "Stat 100",
    "section": "Research Interests",
    "text": "Research Interests\nSurvey statistics and collaborate with"
  },
  {
    "objectID": "stat100_wk01wed.html#research-interests-1",
    "href": "stat100_wk01wed.html#research-interests-1",
    "title": "Stat 100",
    "section": "Research Interests",
    "text": "Research Interests\nWhere survey statistics meets data science"
  },
  {
    "objectID": "stat100_wk01wed.html#stat-100-is-about-developing-our-statistical-thinking-skills.",
    "href": "stat100_wk01wed.html#stat-100-is-about-developing-our-statistical-thinking-skills.",
    "title": "Stat 100",
    "section": "Stat 100 is about developing our statistical thinking skills.",
    "text": "Stat 100 is about developing our statistical thinking skills.\n\n\nWhat is statistical thinking?\n\n\n\nIt is not the same as mathematical thinking.\n\n\n\nLet’s discover what statistical thinking is through some examples."
  },
  {
    "objectID": "stat100_wk01wed.html#data-in-stat-100",
    "href": "stat100_wk01wed.html#data-in-stat-100",
    "title": "Stat 100",
    "section": "Data in Stat 100",
    "text": "Data in Stat 100\nWill use a wide-range of real and relevant data examples"
  },
  {
    "objectID": "stat100_wk01wed.html#data-in-stat-100-1",
    "href": "stat100_wk01wed.html#data-in-stat-100-1",
    "title": "Stat 100",
    "section": "Data in Stat 100",
    "text": "Data in Stat 100\n\n\n\n\n\n\n\n\nI understand that some of these topics have likely had profound impacts on your lives.\nWe will focus class time on the key course objectives but will use these current topics to empower ourselves and to see how we can productively participate with data."
  },
  {
    "objectID": "stat100_wk01wed.html#statistical-thinking",
    "href": "stat100_wk01wed.html#statistical-thinking",
    "title": "Stat 100",
    "section": "Statistical Thinking",
    "text": "Statistical Thinking\n\n\n\nUnderstanding the importance of context.\n\n\n\n Context explains the Monday jumps in the COVID counts.\n\n\n\n\n\n\nHow we encode information in a graph should be driven by our research question.\n\n\n\n Design choices impact the conclusions the viewer draws.\n\n\n\n\n\n\nHow the data are collected impacts the conclusions we can draw.\n\n\n\n Voluntary COVID test results don’t likely provide good estimates of COVID prevalence.\n\n\n\n\n\n\nOften we are using a sample of data to say something about a larger group. In this case, we should measure how certain our estimates are!\n\n\n\n We will learn to compute and interpret certainty estimates (like those in the wastewater graph) later in the course!"
  },
  {
    "objectID": "stat100_wk01wed.html#statistical-thinking-1",
    "href": "stat100_wk01wed.html#statistical-thinking-1",
    "title": "Stat 100",
    "section": "Statistical Thinking",
    "text": "Statistical Thinking\n\nAbout developing reasoning (not just learning definitions and formulae).\nDeveloping our statistical thinking skills will allow us to soundly extract knowledge from data!\nStatistical thinking requires judgment that takes time to develop.\n\nWill see examples and practice applying statistical thinking throughout the course."
  },
  {
    "objectID": "stat100_wk01wed.html#what-areis-data",
    "href": "stat100_wk01wed.html#what-areis-data",
    "title": "Stat 100",
    "section": "What are/is Data?",
    "text": "What are/is Data?\n\n\n\n“‘Raw data’ is an oxymoron.” – Lisa Gitelman\n\n\n\n“Data … is information made tractable.” – Catherine D’Ignazio and Lauren Klein"
  },
  {
    "objectID": "stat100_wk01wed.html#data-frames",
    "href": "stat100_wk01wed.html#data-frames",
    "title": "Stat 100",
    "section": "Data Frames",
    "text": "Data Frames\nData in spreadsheet-like format where:\n\nRows = Observations/cases\nColumns = Variables\n\n\n\n\n\n\n\nID\nkind\n.pred_AI\n.pred_class\ndetector\nnative\nname\nmodel\n\n\n\n\n1\nHuman\n0.9999942\nAI\nSapling\nNo\nReal TOEFL\nHuman\n\n\n2\nHuman\n0.8281448\nAI\nCrossplag\nNo\nReal TOEFL\nHuman\n\n\n3\nHuman\n0.0002137\nHuman\nCrossplag\nYes\nReal College Essays\nHuman\n\n\n4\nAI\n0.0000000\nHuman\nZeroGPT\nNA\nFake CS224N - GPT3\nGPT3\n\n\n5\nAI\n0.0017841\nHuman\nOriginalityAI\nNA\nFake CS224N - GPT3, PE\nGPT4\n\n\n6\nHuman\n0.0001783\nHuman\nHFOpenAI\nYes\nReal CS224N\nHuman\n\n\n\n\n\n\n\n\n\nData from GPT Detectors Are Biased Against Non-Native English Writers. Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou. CellPress Patterns and available in the R package detectors."
  },
  {
    "objectID": "stat100_wk01wed.html#data-frames-1",
    "href": "stat100_wk01wed.html#data-frames-1",
    "title": "Stat 100",
    "section": "Data Frames",
    "text": "Data Frames\n\n\n\n\n\n\nID\nkind\n.pred_AI\n.pred_class\ndetector\nnative\nname\nmodel\n\n\n\n\n1\nHuman\n0.9999942\nAI\nSapling\nNo\nReal TOEFL\nHuman\n\n\n2\nHuman\n0.8281448\nAI\nCrossplag\nNo\nReal TOEFL\nHuman\n\n\n3\nHuman\n0.0002137\nHuman\nCrossplag\nYes\nReal College Essays\nHuman\n\n\n4\nAI\n0.0000000\nHuman\nZeroGPT\nNA\nFake CS224N - GPT3\nGPT3\n\n\n5\nAI\n0.0017841\nHuman\nOriginalityAI\nNA\nFake CS224N - GPT3, PE\nGPT4\n\n\n6\nHuman\n0.0001783\nHuman\nHFOpenAI\nYes\nReal CS224N\nHuman\n\n\n\n\n\n\n\n\nRows = Observations/cases\nWhat are the cases? What does each row represent?"
  },
  {
    "objectID": "stat100_wk01wed.html#data-frames-2",
    "href": "stat100_wk01wed.html#data-frames-2",
    "title": "Stat 100",
    "section": "Data Frames",
    "text": "Data Frames\n\n\n\n\n\n\nID\nkind\n.pred_AI\n.pred_class\ndetector\nnative\nname\nmodel\n\n\n\n\n1\nHuman\n0.9999942\nAI\nSapling\nNo\nReal TOEFL\nHuman\n\n\n2\nHuman\n0.8281448\nAI\nCrossplag\nNo\nReal TOEFL\nHuman\n\n\n3\nHuman\n0.0002137\nHuman\nCrossplag\nYes\nReal College Essays\nHuman\n\n\n4\nAI\n0.0000000\nHuman\nZeroGPT\nNA\nFake CS224N - GPT3\nGPT3\n\n\n5\nAI\n0.0017841\nHuman\nOriginalityAI\nNA\nFake CS224N - GPT3, PE\nGPT4\n\n\n6\nHuman\n0.0001783\nHuman\nHFOpenAI\nYes\nReal CS224N\nHuman\n\n\n\n\n\n\n\n\nColumns = Variables\nVariables: Describe characteristics of the observations\n\nQuantitative: Numerical in nature\nCategorical: Values are categories\nIdentification: Uniquely identify each case"
  },
  {
    "objectID": "stat100_wk01wed.html#hand-drawn-data-viz",
    "href": "stat100_wk01wed.html#hand-drawn-data-viz",
    "title": "Stat 100",
    "section": "Hand-Drawn Data Viz",
    "text": "Hand-Drawn Data Viz\n\nOnce we have collected data, a common next step is to visualize it.\nTwo key aspects of data visualization:\n\nDetermining how you want to display the data.\nFiguring out how to tell the computer to do that mapping.\n\nHand-drawn data visualizations allow us to focus on the first part with full control over the creative process!"
  },
  {
    "objectID": "stat100_wk01wed.html#hand-drawn-data-viz-examples",
    "href": "stat100_wk01wed.html#hand-drawn-data-viz-examples",
    "title": "Stat 100",
    "section": "Hand-Drawn Data Viz Examples",
    "text": "Hand-Drawn Data Viz Examples\nDear Data\n\n“Each week, and for a year, we collected and measured a particular type of data about our lives, used this data to make a drawing on a postcard-sized sheet of paper, and then dropped the postcard in an English”postbox” (Stefanie) or an American “mailbox” (Giorgia)!“"
  },
  {
    "objectID": "stat100_wk01wed.html#goal-by-next-wed-collect-data-from-your-life-so-that-you-can-visualize-it-on-p-set-1.",
    "href": "stat100_wk01wed.html#goal-by-next-wed-collect-data-from-your-life-so-that-you-can-visualize-it-on-p-set-1.",
    "title": "Stat 100",
    "section": "Goal: By next Wed, collect data from your life so that you can visualize it on P-Set 1.",
    "text": "Goal: By next Wed, collect data from your life so that you can visualize it on P-Set 1.\n\n\n\nRecommendations\n\n\nStore the data in your favorite spreadsheet program (Google Sheets, Numbers, Excel).\nDetermine what your cases/observations will be.\nCollect data on more variables than you will likely visualize. It is hard to know beforehand what the interesting relationships will be.\n\n\n\nNext Week\n\nWill get a blank postcard and further guidance on the visualization with P-Set 1."
  },
  {
    "objectID": "stat100_wk01wed.html#section",
    "href": "stat100_wk01wed.html#section",
    "title": "Stat 100",
    "section": "",
    "text": "Demo of accessing the RStudio Server on Posit Cloud\n\n\nTry to access the RStudio Server between now and next lecture.\nCome back to the recording if need help with the steps."
  },
  {
    "objectID": "stat100_wk01wed.html#reminders",
    "href": "stat100_wk01wed.html#reminders",
    "title": "Stat 100",
    "section": "Reminders",
    "text": "Reminders\n\nIf able, please bring a laptop or tablet to Mondays’s lecture.\nNo section, no wrap-ups, and no lecture quiz this week.\nMake sure to go through the syllabus, which can be found on Canvas.\n\nWill discuss assessments and course policies on Monday.\n\nOnly I will be running office hours this week at the following time:\n\nToday 1:30 - 3:00 pm in Science Center 316 (This week only)\n\nThe regular office hour schedule will be posted later this week and will start next week.\nBe on the look-out for the section preference form."
  },
  {
    "objectID": "stat100_wk03wed.html#announcements",
    "href": "stat100_wk03wed.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nWith COVID working its way through campus right now, make sure to check the Sections spreadsheet and the Office hours spreadsheet for updates!\nLet’s go through up to upload the pngs of your postcards to the RStudio Server on Posit Cloud."
  },
  {
    "objectID": "stat100_wk03wed.html#goals-for-today",
    "href": "stat100_wk03wed.html#goals-for-today",
    "title": "Stat 100",
    "section": "Goals for Today",
    "text": "Goals for Today\n\n\n\nConsider measures for summarizing quantitative data\n\nCenter\nSpread/variability\n\nConsider measures for summarizing categorical data\n\n\n\nDefine data wrangling\nLearn to use functions in the dplyr package to summarize and wrangle data"
  },
  {
    "objectID": "stat100_wk03wed.html#load-necessary-packages",
    "href": "stat100_wk03wed.html#load-necessary-packages",
    "title": "Stat 100",
    "section": "Load Necessary Packages",
    "text": "Load Necessary Packages\n\n\n\n\n\ndplyr is part of this collection of data science packages.\n\n# Load necessary packages\nlibrary(tidyverse)"
  },
  {
    "objectID": "stat100_wk03wed.html#import-the-data",
    "href": "stat100_wk03wed.html#import-the-data",
    "title": "Stat 100",
    "section": "Import the Data",
    "text": "Import the Data\n\njuly_2019 &lt;- read_csv(\"data/july_2019.csv\")\n\n# Inspect the data\nglimpse(july_2019)\n\nRows: 192\nColumns: 8\n$ DateTime  &lt;chr&gt; \"07/04/2019 12:00:00 AM\", \"07/04/2019 12:15:00 AM\", \"07/04/2…\n$ Day       &lt;chr&gt; \"Thursday\", \"Thursday\", \"Thursday\", \"Thursday\", \"Thursday\", …\n$ Date      &lt;date&gt; 2019-07-04, 2019-07-04, 2019-07-04, 2019-07-04, 2019-07-04,…\n$ Time      &lt;time&gt; 00:00:00, 00:15:00, 00:30:00, 00:45:00, 01:00:00, 01:15:00,…\n$ Total     &lt;dbl&gt; 2, 3, 2, 0, 3, 2, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, …\n$ Westbound &lt;dbl&gt; 2, 3, 1, 0, 2, 2, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, …\n$ Eastbound &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ Occasion  &lt;chr&gt; \"Fourth of July\", \"Fourth of July\", \"Fourth of July\", \"Fourt…"
  },
  {
    "objectID": "stat100_wk03wed.html#summarizing-data",
    "href": "stat100_wk03wed.html#summarizing-data",
    "title": "Stat 100",
    "section": "Summarizing Data",
    "text": "Summarizing Data\n\n\n\n\n\n\nDateTime\nDay\nDate\nTime\nTotal\nWestbound\nEastbound\nOccasion\n\n\n\n\n07/04/2019 06:00:00 AM\nThursday\n2019-07-04\n06:00:00\n1\n1\n0\nFourth of July\n\n\n07/04/2019 06:15:00 AM\nThursday\n2019-07-04\n06:15:00\n4\n0\n4\nFourth of July\n\n\n07/04/2019 06:30:00 AM\nThursday\n2019-07-04\n06:30:00\n9\n1\n8\nFourth of July\n\n\n07/04/2019 06:45:00 AM\nThursday\n2019-07-04\n06:45:00\n5\n0\n5\nFourth of July\n\n\n07/04/2019 07:00:00 AM\nThursday\n2019-07-04\n07:00:00\n3\n3\n0\nFourth of July\n\n\n07/04/2019 07:15:00 AM\nThursday\n2019-07-04\n07:15:00\n2\n0\n2\nFourth of July\n\n\n07/04/2019 07:30:00 AM\nThursday\n2019-07-04\n07:30:00\n5\n2\n3\nFourth of July\n\n\n07/04/2019 07:45:00 AM\nThursday\n2019-07-04\n07:45:00\n2\n0\n2\nFourth of July\n\n\n\n\n\n\n\n\n\nHard to do by eyeballing a spreadsheet with many rows!"
  },
  {
    "objectID": "stat100_wk03wed.html#summarizing-data-visually",
    "href": "stat100_wk03wed.html#summarizing-data-visually",
    "title": "Stat 100",
    "section": "Summarizing Data Visually",
    "text": "Summarizing Data Visually\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor a quantitative variable, want to answer:\n\nWhat is an average value?\nWhat is the trend/shape of the variable?\nHow much variation is there from case to case?\n\n\n\n\nNeed to learn key summary statistics: Numerical values computed based on the observed cases."
  },
  {
    "objectID": "stat100_wk03wed.html#measures-of-center",
    "href": "stat100_wk03wed.html#measures-of-center",
    "title": "Stat 100",
    "section": "Measures of Center",
    "text": "Measures of Center\n\n\nMean: Average of all the observations\n\n\\(n\\) = Number of cases (sample size)\n\\(x_i\\) = value of the i-th observation\nDenote by \\(\\bar{x}\\)\n\n\\[\n\\bar{x}  = \\frac{1}{n} \\sum_{i = 1}^n x_i\n\\]\n\n# Test out on first 6 values\nhead(july_2019$Total)\n\n[1] 2 3 2 0 3 2\n\n\n\nCompute with a dplyr function:\n\nsummarize(july_2019, mean_bikes = mean(Total))\n\n# A tibble: 1 × 1\n  mean_bikes\n       &lt;dbl&gt;\n1       17.1"
  },
  {
    "objectID": "stat100_wk03wed.html#measures-of-center-1",
    "href": "stat100_wk03wed.html#measures-of-center-1",
    "title": "Stat 100",
    "section": "Measures of Center",
    "text": "Measures of Center\n\n\nMedian: Middle value\n\nHalf of the data falls below the median\nDenote by \\(m\\)\nIf \\(n\\) is even, then it is the average of the middle two values\n\n\n# Test out on first 6 values\nhead(july_2019$Total)\n\n[1] 2 3 2 0 3 2\n\n\n\nCompute with a dplyr function:\n\nsummarize(july_2019, median_bikes = median(Total))\n\n# A tibble: 1 × 1\n  median_bikes\n         &lt;dbl&gt;\n1           11"
  },
  {
    "objectID": "stat100_wk03wed.html#measures-of-center-2",
    "href": "stat100_wk03wed.html#measures-of-center-2",
    "title": "Stat 100",
    "section": "Measures of Center",
    "text": "Measures of Center\n\n\nWhy is the mean larger than the median?\n\nsummarize(july_2019, mean_bikes = mean(Total),\n          median_bikes = median(Total))\n\n# A tibble: 1 × 2\n  mean_bikes median_bikes\n       &lt;dbl&gt;        &lt;dbl&gt;\n1       17.1           11"
  },
  {
    "objectID": "stat100_wk03wed.html#computing-measures-of-center-by-groups",
    "href": "stat100_wk03wed.html#computing-measures-of-center-by-groups",
    "title": "Stat 100",
    "section": "Computing Measures of Center by Groups",
    "text": "Computing Measures of Center by Groups\nQuestion: Were there more bikes, on average, for Fourth of July or for the normal Thursday?"
  },
  {
    "objectID": "stat100_wk03wed.html#computing-measures-of-center-by-groups-1",
    "href": "stat100_wk03wed.html#computing-measures-of-center-by-groups-1",
    "title": "Stat 100",
    "section": "Computing Measures of Center by Groups",
    "text": "Computing Measures of Center by Groups\nHandy dplyr function: group_by()\n\njuly_2019_grouped &lt;- group_by(july_2019, Occasion)\njuly_2019_grouped\n\n# A tibble: 192 × 8\n# Groups:   Occasion [2]\n   DateTime            Day   Date       Time  Total Westbound Eastbound Occasion\n   &lt;chr&gt;               &lt;chr&gt; &lt;date&gt;     &lt;tim&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   \n 1 07/04/2019 12:00:0… Thur… 2019-07-04 00:00     2         2         0 Fourth …\n 2 07/04/2019 12:15:0… Thur… 2019-07-04 00:15     3         3         0 Fourth …\n 3 07/04/2019 12:30:0… Thur… 2019-07-04 00:30     2         1         1 Fourth …\n 4 07/04/2019 12:45:0… Thur… 2019-07-04 00:45     0         0         0 Fourth …\n 5 07/04/2019 01:00:0… Thur… 2019-07-04 01:00     3         2         1 Fourth …\n 6 07/04/2019 01:15:0… Thur… 2019-07-04 01:15     2         2         0 Fourth …\n 7 07/04/2019 01:30:0… Thur… 2019-07-04 01:30     1         1         0 Fourth …\n 8 07/04/2019 01:45:0… Thur… 2019-07-04 01:45     0         0         0 Fourth …\n 9 07/04/2019 02:00:0… Thur… 2019-07-04 02:00     0         0         0 Fourth …\n10 07/04/2019 02:15:0… Thur… 2019-07-04 02:15     0         0         0 Fourth …\n# ℹ 182 more rows"
  },
  {
    "objectID": "stat100_wk03wed.html#computing-measures-of-center-by-groups-2",
    "href": "stat100_wk03wed.html#computing-measures-of-center-by-groups-2",
    "title": "Stat 100",
    "section": "Computing Measures of Center by Groups",
    "text": "Computing Measures of Center by Groups\n\n\nCompute summary statistics on the grouped data frame:\n\njuly_2019_grouped &lt;- group_by(july_2019, Occasion)\nsummarize(july_2019_grouped,\n          mean_bikes = mean(Total),\n          median_bikes = median(Total))\n\n# A tibble: 2 × 3\n  Occasion        mean_bikes median_bikes\n  &lt;chr&gt;                &lt;dbl&gt;        &lt;dbl&gt;\n1 Fourth of July        10.0          9  \n2 Normal Thursday       24.2         14.5"
  },
  {
    "objectID": "stat100_wk03wed.html#chaining-dplyr-operations",
    "href": "stat100_wk03wed.html#chaining-dplyr-operations",
    "title": "Stat 100",
    "section": "Chaining dplyr Operations",
    "text": "Chaining dplyr Operations\n\n\nInstead of:\n\njuly_2019_grouped &lt;- group_by(july_2019, Occasion)\nsummarize(july_2019_grouped,\n          mean_bikes = mean(Total),\n          median_bikes = median(Total))\n\n# A tibble: 2 × 3\n  Occasion        mean_bikes median_bikes\n  &lt;chr&gt;                &lt;dbl&gt;        &lt;dbl&gt;\n1 Fourth of July        10.0          9  \n2 Normal Thursday       24.2         14.5\n\n\n\nUse the pipe:\n\njuly_2019 %&gt;%\n  group_by(Occasion) %&gt;%\n  summarize(mean_bikes = mean(Total),\n          median_bikes = median(Total))\n\n# A tibble: 2 × 3\n  Occasion        mean_bikes median_bikes\n  &lt;chr&gt;                &lt;dbl&gt;        &lt;dbl&gt;\n1 Fourth of July        10.0          9  \n2 Normal Thursday       24.2         14.5\n\n\n\n\n\n\nWhy pipe?\n\n\n\n\nYou can also use |&gt;, which is newer and often referred to as the “base R pipe.”"
  },
  {
    "objectID": "stat100_wk03wed.html#measures-of-variability",
    "href": "stat100_wk03wed.html#measures-of-variability",
    "title": "Stat 100",
    "section": "Measures of Variability",
    "text": "Measures of Variability\n\nWant a statistic that captures how much observations deviate from the mean\n\n\n\n\nFind how much each observation deviates from the mean.\nCompute the average of the deviations.\n\n\\[\n\\frac{1}{n} \\sum_{i = 1}^n (x_i - \\bar{x})\n\\]\n\n\n# Test out on first 6 values\nhead(july_2019$Total)\n\n[1] 2 3 2 0 3 2\n\n\n\n\n\nProblem?"
  },
  {
    "objectID": "stat100_wk03wed.html#measures-of-variability-1",
    "href": "stat100_wk03wed.html#measures-of-variability-1",
    "title": "Stat 100",
    "section": "Measures of Variability",
    "text": "Measures of Variability\n\nWant a statistic that captures how much observations deviate from the mean\n\n\n\nHere is my NEW proposal:\n\nFind how much each observation deviates from the mean.\nCompute the average of the squared deviations.\n\n\n\n# Test out on first 6 values\nhead(july_2019$Total)\n\n[1] 2 3 2 0 3 2"
  },
  {
    "objectID": "stat100_wk03wed.html#measures-of-variability-2",
    "href": "stat100_wk03wed.html#measures-of-variability-2",
    "title": "Stat 100",
    "section": "Measures of Variability",
    "text": "Measures of Variability\n\nWant a statistic that captures how much observations deviate from the mean\n\n\n\nHere is my ACTUAL formula:\n\nFind how much each observation deviates from the mean.\nCompute the (nearly) average of the squared deviations.\nCalled sample variance \\(s^2\\).\n\n\\[\ns^2 = \\frac{1}{n - 1} \\sum_{i = 1}^n (x_i - \\bar{x})^2\n\\]\n\nCompute with a dplyr function:\n\nsummarize(july_2019, var_bikes = var(Total))\n\n# A tibble: 1 × 1\n  var_bikes\n      &lt;dbl&gt;\n1      454."
  },
  {
    "objectID": "stat100_wk03wed.html#measures-of-variability-3",
    "href": "stat100_wk03wed.html#measures-of-variability-3",
    "title": "Stat 100",
    "section": "Measures of Variability",
    "text": "Measures of Variability\n\nWant a statistic that captures how much observations deviate from the mean\n\n\n\n\nFind how much each observation deviates from the mean.\nCompute the (nearly) average of the squared deviations.\nCalled sample variance \\(s^2\\).\nThe square root of the sample variance is called the sample standard deviation \\(s\\).\n\n\\[\ns = \\sqrt{\\frac{1}{n - 1} \\sum_{i = 1}^n (x_i - \\bar{x})^2}\n\\]\n\nCompute with a dplyr function:\n\nsummarize(july_2019, var_bikes = var(Total),\n          sd_bikes = sd(Total))\n\n# A tibble: 1 × 2\n  var_bikes sd_bikes\n      &lt;dbl&gt;    &lt;dbl&gt;\n1      454.     21.3"
  },
  {
    "objectID": "stat100_wk03wed.html#measures-of-variability-4",
    "href": "stat100_wk03wed.html#measures-of-variability-4",
    "title": "Stat 100",
    "section": "Measures of Variability",
    "text": "Measures of Variability\n\n\n\nIn addition to the sample standard deviation and the sample variance, there is the sample interquartile range (IQR):\n\n\\[\n\\mbox{IQR} = \\mbox{Q}_3 - \\mbox{Q}_1\n\\]\n\nCompute with a dplyr function:\n\nsummarize(july_2019, iqr_bikes = IQR(Total))\n\n# A tibble: 1 × 1\n  iqr_bikes\n      &lt;dbl&gt;\n1        16"
  },
  {
    "objectID": "stat100_wk03wed.html#comparing-measures-of-variability",
    "href": "stat100_wk03wed.html#comparing-measures-of-variability",
    "title": "Stat 100",
    "section": "Comparing Measures of Variability",
    "text": "Comparing Measures of Variability\n\nWhich is more robust to outliers, the IQR or \\(s\\)?\nWhich is more commonly used, the IQR or \\(s\\)?\n\n\njuly_2019 %&gt;%\n  group_by(Occasion) %&gt;%\nsummarize(sd_bikes = sd(Total),\n          iqr_bikes = IQR(Total))\n\n# A tibble: 2 × 3\n  Occasion        sd_bikes iqr_bikes\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n1 Fourth of July      8.30      14  \n2 Normal Thursday    27.2       27.2"
  },
  {
    "objectID": "stat100_wk03wed.html#return-to-the-cambridge-dogs",
    "href": "stat100_wk03wed.html#return-to-the-cambridge-dogs",
    "title": "Stat 100",
    "section": "Return to the Cambridge Dogs",
    "text": "Return to the Cambridge Dogs\nFocus on the dogs with the 5 most common names\n\ndogs &lt;- read_csv(\"https://data.cambridgema.gov/api/views/sckh-3xyx/rows.csv\")\n\n# Useful wrangling that we will come back to\ndogs_top5 &lt;- dogs %&gt;% \n  mutate(Breed = case_when(\n                       Dog_Breed == \"Mixed Breed\" ~ \"Mixed\",\n                       Dog_Breed != \"Mixed Breed\" ~ \"Single\")) %&gt;%\n  filter(Dog_Name %in% c(\"Luna\", \"Charlie\", \"Lucy\", \"Cooper\", \"Rosie\" ))"
  },
  {
    "objectID": "stat100_wk03wed.html#frequency-table",
    "href": "stat100_wk03wed.html#frequency-table",
    "title": "Stat 100",
    "section": "Frequency Table",
    "text": "Frequency Table\n\n\n\ncount(dogs_top5, Dog_Name)\n\n# A tibble: 5 × 2\n  Dog_Name     n\n  &lt;chr&gt;    &lt;int&gt;\n1 Charlie     35\n2 Cooper      23\n3 Lucy        25\n4 Luna        41\n5 Rosie       22\n\n\n\n\nggplot(data = dogs_top5, \n    mapping = aes(x = Dog_Name)) +\n  geom_bar()"
  },
  {
    "objectID": "stat100_wk03wed.html#frequency-table-1",
    "href": "stat100_wk03wed.html#frequency-table-1",
    "title": "Stat 100",
    "section": "Frequency Table",
    "text": "Frequency Table\n\n\n\ncount(dogs_top5, Dog_Name)\n\n# A tibble: 5 × 2\n  Dog_Name     n\n  &lt;chr&gt;    &lt;int&gt;\n1 Charlie     35\n2 Cooper      23\n3 Lucy        25\n4 Luna        41\n5 Rosie       22\n\n\n\n\ncount(dogs_top5, Dog_Name, sort = TRUE)\n\n# A tibble: 5 × 2\n  Dog_Name     n\n  &lt;chr&gt;    &lt;int&gt;\n1 Luna        41\n2 Charlie     35\n3 Lucy        25\n4 Cooper      23\n5 Rosie       22"
  },
  {
    "objectID": "stat100_wk03wed.html#contingency-table",
    "href": "stat100_wk03wed.html#contingency-table",
    "title": "Stat 100",
    "section": "Contingency Table",
    "text": "Contingency Table\n\n\n\ncount(dogs_top5, Dog_Name, Breed)\n\n# A tibble: 10 × 3\n   Dog_Name Breed      n\n   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;\n 1 Charlie  Mixed     12\n 2 Charlie  Single    23\n 3 Cooper   Mixed      9\n 4 Cooper   Single    14\n 5 Lucy     Mixed     10\n 6 Lucy     Single    15\n 7 Luna     Mixed     16\n 8 Luna     Single    25\n 9 Rosie    Mixed      6\n10 Rosie    Single    16\n\n\n\n\nggplot(data = dogs_top5, \n    mapping = aes(x = Dog_Name, fill = Breed)) +\n  geom_bar(position = \"dodge\")"
  },
  {
    "objectID": "stat100_wk03wed.html#conditional-proportions",
    "href": "stat100_wk03wed.html#conditional-proportions",
    "title": "Stat 100",
    "section": "Conditional Proportions",
    "text": "Conditional Proportions\n\n\n\nBeyond raw counts, we often summarize categorical data with conditional proportions.\n\nEspecially when looking for relationships!\n\n\n\n\nggplot(data = dogs_top5, \n    mapping = aes(x = Dog_Name, fill = Breed)) +\n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "stat100_wk03wed.html#conditional-proportions-1",
    "href": "stat100_wk03wed.html#conditional-proportions-1",
    "title": "Stat 100",
    "section": "Conditional Proportions",
    "text": "Conditional Proportions\n\n\n\ncount(dogs_top5, Dog_Name, Breed)\n\n# A tibble: 10 × 3\n   Dog_Name Breed      n\n   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt;\n 1 Charlie  Mixed     12\n 2 Charlie  Single    23\n 3 Cooper   Mixed      9\n 4 Cooper   Single    14\n 5 Lucy     Mixed     10\n 6 Lucy     Single    15\n 7 Luna     Mixed     16\n 8 Luna     Single    25\n 9 Rosie    Mixed      6\n10 Rosie    Single    16\n\n\n\n\ncount(dogs_top5, Dog_Name, Breed) %&gt;%\n  group_by(Dog_Name) %&gt;%\n  mutate(prop = n/sum(n))\n\n# A tibble: 10 × 4\n# Groups:   Dog_Name [5]\n   Dog_Name Breed      n  prop\n   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n 1 Charlie  Mixed     12 0.343\n 2 Charlie  Single    23 0.657\n 3 Cooper   Mixed      9 0.391\n 4 Cooper   Single    14 0.609\n 5 Lucy     Mixed     10 0.4  \n 6 Lucy     Single    15 0.6  \n 7 Luna     Mixed     16 0.390\n 8 Luna     Single    25 0.610\n 9 Rosie    Mixed      6 0.273\n10 Rosie    Single    16 0.727\n\n\n\n\n\nThe dplyr function mutate() adds new column(s) to your data frame."
  },
  {
    "objectID": "stat100_wk03wed.html#conditional-proportions-2",
    "href": "stat100_wk03wed.html#conditional-proportions-2",
    "title": "Stat 100",
    "section": "Conditional Proportions",
    "text": "Conditional Proportions\n\n\n\ncount(dogs_top5, Dog_Name, Breed) %&gt;%\n  group_by(Dog_Name) %&gt;%\n  mutate(prop = n/sum(n))\n\n# A tibble: 10 × 4\n# Groups:   Dog_Name [5]\n   Dog_Name Breed      n  prop\n   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n 1 Charlie  Mixed     12 0.343\n 2 Charlie  Single    23 0.657\n 3 Cooper   Mixed      9 0.391\n 4 Cooper   Single    14 0.609\n 5 Lucy     Mixed     10 0.4  \n 6 Lucy     Single    15 0.6  \n 7 Luna     Mixed     16 0.390\n 8 Luna     Single    25 0.610\n 9 Rosie    Mixed      6 0.273\n10 Rosie    Single    16 0.727\n\n\n\n\ncount(dogs_top5, Dog_Name, Breed) %&gt;%\n  group_by(Breed) %&gt;%\n  mutate(prop = n/sum(n))\n\n# A tibble: 10 × 4\n# Groups:   Breed [2]\n   Dog_Name Breed      n  prop\n   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n 1 Charlie  Mixed     12 0.226\n 2 Charlie  Single    23 0.247\n 3 Cooper   Mixed      9 0.170\n 4 Cooper   Single    14 0.151\n 5 Lucy     Mixed     10 0.189\n 6 Lucy     Single    15 0.161\n 7 Luna     Mixed     16 0.302\n 8 Luna     Single    25 0.269\n 9 Rosie    Mixed      6 0.113\n10 Rosie    Single    16 0.172\n\n\n\n\nHow does the interpretation change based on which variable you condition on?"
  },
  {
    "objectID": "stat100_wk03wed.html#dplyr-for-data-wrangling",
    "href": "stat100_wk03wed.html#dplyr-for-data-wrangling",
    "title": "Stat 100",
    "section": "dplyr for Data Wrangling",
    "text": "dplyr for Data Wrangling\n\nSeven common wrangling verbs:\n\nsummarize()\ncount()\n\nmutate()\n\nselect()\nfilter()\narrange()\n---_join()\n\nOne action:\n\ngroup_by()"
  },
  {
    "objectID": "stat100_wk03wed.html#return-to-mutate",
    "href": "stat100_wk03wed.html#return-to-mutate",
    "title": "Stat 100",
    "section": "Return to mutate()",
    "text": "Return to mutate()\n\n\nAdd new variables\n\ncount(dogs_top5, Dog_Name, Breed) %&gt;%\n  group_by(Dog_Name) %&gt;%\n  mutate(prop = n/sum(n))\n\n# A tibble: 10 × 4\n# Groups:   Dog_Name [5]\n   Dog_Name Breed      n  prop\n   &lt;chr&gt;    &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt;\n 1 Charlie  Mixed     12 0.343\n 2 Charlie  Single    23 0.657\n 3 Cooper   Mixed      9 0.391\n 4 Cooper   Single    14 0.609\n 5 Lucy     Mixed     10 0.4  \n 6 Lucy     Single    15 0.6  \n 7 Luna     Mixed     16 0.390\n 8 Luna     Single    25 0.610\n 9 Rosie    Mixed      6 0.273\n10 Rosie    Single    16 0.727\n\n\n\nModify existing variables\n\nclass(july_2019$DateTime)\n\n[1] \"character\"\n\njuly_2019 &lt;- july_2019 %&gt;%\n  mutate(DateTime = mdy_hms(DateTime))\nclass(july_2019$DateTime)\n\n[1] \"POSIXct\" \"POSIXt\""
  },
  {
    "objectID": "stat100_wk03wed.html#select-extract-variables",
    "href": "stat100_wk03wed.html#select-extract-variables",
    "title": "Stat 100",
    "section": "select(): Extract variables",
    "text": "select(): Extract variables\n\ndogs %&gt;%\n  select(Dog_Name, Dog_Breed)\n\n# A tibble: 3,942 × 2\n   Dog_Name       Dog_Breed                 \n   &lt;chr&gt;          &lt;chr&gt;                     \n 1 Butch          Mixed Breed               \n 2 Baxter         Mixed Breed               \n 3 Bodhi          Golden Retriever          \n 4 Ocean          Pug                       \n 5 Coco           Pug                       \n 6 Brio           LABRADOODLE               \n 7 Jolene Almeida German Shorthaired Pointer\n 8 Ruger          Labrador Retriever        \n 9 FLASH          Border Collie             \n10 Leo            French Bulldog            \n# ℹ 3,932 more rows"
  },
  {
    "objectID": "stat100_wk03wed.html#motivation-for-filter",
    "href": "stat100_wk03wed.html#motivation-for-filter",
    "title": "Stat 100",
    "section": "Motivation for filter()",
    "text": "Motivation for filter()\n\ncount(dogs, Dog_Name, sort = TRUE)\n\n# A tibble: 2,332 × 2\n   Dog_Name     n\n   &lt;chr&gt;    &lt;int&gt;\n 1 Luna        41\n 2 Charlie     35\n 3 Lucy        25\n 4 Cooper      23\n 5 Rosie       22\n 6 Olive       21\n 7 Pepper      20\n 8 Teddy       19\n 9 Coco        18\n10 Lola        17\n# ℹ 2,322 more rows"
  },
  {
    "objectID": "stat100_wk03wed.html#filter-extract-cases",
    "href": "stat100_wk03wed.html#filter-extract-cases",
    "title": "Stat 100",
    "section": "filter(): Extract cases",
    "text": "filter(): Extract cases\n\ndogs_top5 &lt;- dogs %&gt;% \n  filter(Dog_Name %in% c(\"Luna\", \"Charlie\", \"Lucy\", \"Cooper\", \"Rosie\" ))\n\ncount(dogs_top5, Dog_Name, sort = TRUE)\n\n# A tibble: 5 × 2\n  Dog_Name     n\n  &lt;chr&gt;    &lt;int&gt;\n1 Luna        41\n2 Charlie     35\n3 Lucy        25\n4 Cooper      23\n5 Rosie       22"
  },
  {
    "objectID": "stat100_wk03wed.html#arrange-sort-the-cases",
    "href": "stat100_wk03wed.html#arrange-sort-the-cases",
    "title": "Stat 100",
    "section": "arrange(): Sort the cases",
    "text": "arrange(): Sort the cases\n\n\n\ncount(dogs_top5, Dog_Name) %&gt;%\n  arrange(n)\n\n# A tibble: 5 × 2\n  Dog_Name     n\n  &lt;chr&gt;    &lt;int&gt;\n1 Rosie       22\n2 Cooper      23\n3 Lucy        25\n4 Charlie     35\n5 Luna        41\n\ncount(dogs_top5, Dog_Name) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 5 × 2\n  Dog_Name     n\n  &lt;chr&gt;    &lt;int&gt;\n1 Luna        41\n2 Charlie     35\n3 Lucy        25\n4 Cooper      23\n5 Rosie       22\n\n\n\n\ncount(dogs_top5, Dog_Name) %&gt;%\n  arrange(Dog_Name)\n\n# A tibble: 5 × 2\n  Dog_Name     n\n  &lt;chr&gt;    &lt;int&gt;\n1 Charlie     35\n2 Cooper      23\n3 Lucy        25\n4 Luna        41\n5 Rosie       22"
  },
  {
    "objectID": "stat100_wk03wed.html#reminders",
    "href": "stat100_wk03wed.html#reminders",
    "title": "Stat 100",
    "section": "Reminders",
    "text": "Reminders\n\nWith COVID working its way through campus right now, make sure to check the Sections spreadsheet and the Office hours spreadsheet for updates!"
  },
  {
    "objectID": "stat100_wk08wed.html#announcements",
    "href": "stat100_wk08wed.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nOct 30th: Hex or Treat Day in Stat 100\n\nWear a Halloween costume and get either a hex sticker or candy!!\n\n\nGoals for Today\n\n\n\nModeling & Ethics: Algorithmic bias\nSampling Distribution\n\nProperties\nConstruction in R\n\n\n\n\nEstimation"
  },
  {
    "objectID": "stat100_wk08wed.html#integrity-of-data-and-methods",
    "href": "stat100_wk08wed.html#integrity-of-data-and-methods",
    "title": "Stat 100",
    "section": "Integrity of Data and Methods",
    "text": "Integrity of Data and Methods\n\n“The ethical statistical practitioner seeks to understand and mitigate known or suspected limitations, defects, or biases in the data or methods and communicates potential impacts on the interpretation, conclusions, recommendations, decisions, or other results of statistical practices.”\n\n\n“For models and algorithms designed to inform or implement decisions repeatedly, develops and/or implements plans to validate assumptions and assess performance over time, as needed. Considers criteria and mitigation plans for model or algorithm failure and retirement.”"
  },
  {
    "objectID": "stat100_wk08wed.html#sampling-distribution-of-a-statistic",
    "href": "stat100_wk08wed.html#sampling-distribution-of-a-statistic",
    "title": "Stat 100",
    "section": "Sampling Distribution of a Statistic",
    "text": "Sampling Distribution of a Statistic\n\n\nSteps to Construct an (Approximate) Sampling Distribution:\n\nDecide on a sample size, \\(n\\).\nRandomly select a sample of size \\(n\\) from the population.\nCompute the sample statistic.\nPut the sample back in.\nRepeat Steps 2 - 4 many (1000+) times.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat happens to the center/spread/shape as we increase the sample size?\nWhat happens to the center/spread/shape if the true parameter changes?"
  },
  {
    "objectID": "stat100_wk08wed.html#lets-construct-some-sampling-distributions-using-r",
    "href": "stat100_wk08wed.html#lets-construct-some-sampling-distributions-using-r",
    "title": "Stat 100",
    "section": "Let’s Construct Some Sampling Distributions using R!",
    "text": "Let’s Construct Some Sampling Distributions using R!\nImportant Notes\n\nTo construct a sampling distribution for a statistic, we need access to the entire population so that we can take repeated samples from the population.\n\nPopulation = Harvard trees\n\nBut if we have access to the entire population, then we know the value of the population parameter.\n\nCan compute the exact mean diameter of trees in our population.\n\nThe sampling distribution is needed in the exact scenario where we can’t compute it: the scenario where we only have a single sample.\nWe will learn how to estimate the sampling distribution soon.\nToday, we have the entire population and are constructing sampling distributions anyway to study their properties!"
  },
  {
    "objectID": "stat100_wk08wed.html#new-r-package-infer",
    "href": "stat100_wk08wed.html#new-r-package-infer",
    "title": "Stat 100",
    "section": "New R Package: infer",
    "text": "New R Package: infer\n\n\n\n\n\n\n\n\n  \n\nlibrary(infer)\n\n\n\n\nWill use infer to conduct statistical inference."
  },
  {
    "objectID": "stat100_wk08wed.html#our-population-parameter",
    "href": "stat100_wk08wed.html#our-population-parameter",
    "title": "Stat 100",
    "section": "Our Population Parameter",
    "text": "Our Population Parameter\nCreate data frame of Harvard trees:\n\nlibrary(tidyverse)\nlibrary(bosTrees)\nharTrees &lt;- camTrees %&gt;%\n  filter(Ownership == \"Harvard\", SiteType == \"Tree\") %&gt;%\n  drop_na(SpeciesShort)\n\nAdd variable of interest:\n\nharTrees &lt;- harTrees %&gt;%\n  mutate(tree_of_interest = case_when(\n    SpeciesShort == \"Maple\" ~ \"yes\",\n    SpeciesShort != \"Maple\" ~ \"no\"\n  ))\ncount(harTrees, tree_of_interest)\n\n# A tibble: 2 × 2\n  tree_of_interest     n\n  &lt;chr&gt;            &lt;int&gt;\n1 no                2707\n2 yes                434"
  },
  {
    "objectID": "stat100_wk08wed.html#population-parameter",
    "href": "stat100_wk08wed.html#population-parameter",
    "title": "Stat 100",
    "section": "Population Parameter",
    "text": "Population Parameter\n\n\n# Population distribution\nggplot(data = harTrees, \n       mapping = aes(x = tree_of_interest)) +\n  geom_bar(aes(y = ..prop.., group = 1),\n           stat = \"count\") \n\n\n\n\n\n\n\n\n# True population parameter\nsummarize(harTrees, \n          parameter = mean(tree_of_interest == \"yes\"))\n\n# A tibble: 1 × 1\n  parameter\n      &lt;dbl&gt;\n1     0.138"
  },
  {
    "objectID": "stat100_wk08wed.html#key-features-of-a-sampling-distribution",
    "href": "stat100_wk08wed.html#key-features-of-a-sampling-distribution",
    "title": "Stat 100",
    "section": "Key Features of a Sampling Distribution",
    "text": "Key Features of a Sampling Distribution\nWhat did we learn about sampling distributions?\n\nCentered around the true population parameter.\nAs the sample size increases, the standard error (SE) of the statistic decreases.\nAs the sample size increases, the shape of the sampling distribution becomes more bell-shaped and symmetric.\nQuestion: How do sampling distributions help us quantify uncertainty?\nQuestion: If I am estimating a parameter in a real example, why won’t I be able to construct the sampling distribution??"
  },
  {
    "objectID": "stat100_wk08wed.html#estimation",
    "href": "stat100_wk08wed.html#estimation",
    "title": "Stat 100",
    "section": "Estimation",
    "text": "Estimation\nGoal: Estimate the value of a population parameter using data from the sample.\n\nQuestion: How do I know which population parameter I am interesting in estimating?\nAnswer: Likely depends on the research question and structure of your data!\nPoint Estimate: The corresponding statistic\n\nSingle best guess for the parameter\n\n\n\n\nlibrary(tidyverse)\nce &lt;- read_csv(\"data/fmli.csv\")\nsummarize(ce, meanFINCBTAX = mean(FINCBTAX))\n\n# A tibble: 1 × 1\n  meanFINCBTAX\n         &lt;dbl&gt;\n1       62480."
  },
  {
    "objectID": "stat100_wk08wed.html#confidence-intervals",
    "href": "stat100_wk08wed.html#confidence-intervals",
    "title": "Stat 100",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\n\nIt is time to move beyond just point estimates to interval estimates that quantify our uncertainty.\n\n\nsummarize(ce, meanFINCBTAX = mean(FINCBTAX))\n\n# A tibble: 1 × 1\n  meanFINCBTAX\n         &lt;dbl&gt;\n1       62480.\n\n\n\n\n\nConfidence Interval: Interval of plausible values for a parameter\nForm: \\(\\mbox{statistic} \\pm \\mbox{Margin of Error}\\)\nQuestion: How do we find the Margin of Error (ME)?\nAnswer: If the sampling distribution of the statistic is approximately bell-shaped and symmetric, then a statistic will be within 2 SEs of the parameter for 95% of the samples.\nForm: \\(\\mbox{statistic} \\pm 2\\mbox{SE}\\)\nCalled a 95% confidence interval (CI). (Will discuss the meaning of confidence soon)"
  },
  {
    "objectID": "stat100_wk08wed.html#confidence-intervals-1",
    "href": "stat100_wk08wed.html#confidence-intervals-1",
    "title": "Stat 100",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n95% CI Form:\n\\[\n\\mbox{statistic} \\pm 2\\mbox{SE}\n\\]\nLet’s use the ce data to produce a CI for the average household income before taxes.\n\nsummarize(ce, meanFINCBTAX = mean(FINCBTAX))\n\n# A tibble: 1 × 1\n  meanFINCBTAX\n         &lt;dbl&gt;\n1       62480.\n\n\nWhat else do we need to construct the CI?\n\nProblem: To compute the SE, we need many samples from the population. We have 1 sample.\nSolution: Approximate the sampling distribution using ONLY OUR ONE SAMPLE!"
  },
  {
    "objectID": "stat100_wk08wed.html#reminders",
    "href": "stat100_wk08wed.html#reminders",
    "title": "Stat 100",
    "section": "Reminders:",
    "text": "Reminders:\n\nOct 30th: Hex or Treat Day in Stat 100\n\nWear a Halloween costume and get either a hex sticker or candy!!"
  },
  {
    "objectID": "stat100_wk08mon.html#announcements",
    "href": "stat100_wk08mon.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nOct 30th: Hex or Treat Day in Stat 100\n\nWear a Halloween costume and get either a hex sticker or candy!!\n\n\nGoals for Today\n\n\n\nFinish up: Regression with polynomial explanatory variables\nModeling guidance\n\n\n\nSampling variability\nSampling distributions"
  },
  {
    "objectID": "stat100_wk08mon.html#sampling-distribution-of-a-statistic",
    "href": "stat100_wk08mon.html#sampling-distribution-of-a-statistic",
    "title": "Stat 100",
    "section": "Sampling Distribution of a Statistic",
    "text": "Sampling Distribution of a Statistic\nSteps to Construct an (Approximate) Sampling Distribution:\n\nDecide on a sample size, \\(n\\).\nRandomly select a sample of size \\(n\\) from the population.\nCompute the sample statistic.\nPut the sample back in.\nRepeat Steps 2 - 4 many (1000+) times."
  },
  {
    "objectID": "stat100_wk08mon.html#sampling-distribution-of-a-statistic-1",
    "href": "stat100_wk08mon.html#sampling-distribution-of-a-statistic-1",
    "title": "Stat 100",
    "section": "Sampling Distribution of a Statistic",
    "text": "Sampling Distribution of a Statistic\n\n\n\n\n\n\n\n\n\n\n\n\nCenter? Shape?\nSpread?\n\nStandard error = standard deviation of the statistic\n\n\n\n\nWhat happens to the center/spread/shape as we increase the sample size?\nWhat happens to the center/spread/shape if the true parameter changes?"
  },
  {
    "objectID": "stat100_wk07wed.html#announcements",
    "href": "stat100_wk07wed.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nDon’t forget about this week’s lecture quiz.\n\nGoals for Today\n\n\n\nHandling categorical, explanatory variables with more than 2 categories\n\n\n\nRegression with polynomial explanatory variables"
  },
  {
    "objectID": "stat100_wk07wed.html#adding-a-curve-to-your-scatterplot",
    "href": "stat100_wk07wed.html#adding-a-curve-to-your-scatterplot",
    "title": "Stat 100",
    "section": "Adding a Curve to your Scatterplot",
    "text": "Adding a Curve to your Scatterplot\n\n\nggplot(data = movies2,\n       mapping = aes(x = AudienceScore,\n                     y = RottenTomatoes,\n                     color = Genre)) +\n  geom_point(alpha = 0.5) +\n  stat_smooth(method = lm, se = FALSE, \n        formula = y ~ poly(x, degree = 2))"
  },
  {
    "objectID": "stat100_wk07wed.html#practice",
    "href": "stat100_wk07wed.html#practice",
    "title": "Stat 100",
    "section": "Practice",
    "text": "Practice\nDetermine and interpret the slope for a Chinstrap penguin using Model 1.\n \nDetermine and interpret the slope for a Adelie penguin using Model 1.\n \nIn Model 1, interpret \\(\\hat{\\beta}_2\\).\n \nDetermine and interpret the slope for a Chinstrap penguin using Model 2.\n \nDetermine and interpret the slope for a Adelie penguin using Model 2."
  },
  {
    "objectID": "stat100_wk02wed.html#announcements",
    "href": "stat100_wk02wed.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nClass in full swing:\n\nSections: Can find your assigned section in my.harvard but need to go to the linked spreadsheet to find the room!\nOffice hours\n\nFill out this form after your first visit.\n\nWrap-ups on Th 3-4pm and Fri 10:30 - 11:30am in SC 309\nLecture quiz will be released in Gradescope after class today."
  },
  {
    "objectID": "stat100_wk02wed.html#teachly",
    "href": "stat100_wk02wed.html#teachly",
    "title": "Stat 100",
    "section": "Teachly",
    "text": "Teachly\n\nTeachly is a platform that allows you to fill out a profile so that we can get to know you and your interests in stats/data science better.\nYou should have received two emails:\n\nThe general Teachly profile\nA couple additional Stat 100 related questions\n\nEach question is optional. You will not be assessed on its completion or your answers.\nWays we plan to use Teachly:\n\nTo get to know you better.\nTo find out what data applications you might be interested in seeing.\nTo tailor advice related to future statistical endeavors."
  },
  {
    "objectID": "stat100_wk02wed.html#goals-for-today",
    "href": "stat100_wk02wed.html#goals-for-today",
    "title": "Stat 100",
    "section": "Goals for Today",
    "text": "Goals for Today\n\n\nFirst Segment:\n\nMotivate data visualizations.\nDevelop language to talk about the components of a graphic.\nPractice deconstructing graphics.\nDiscuss good graphical practices.\n\n\nSecond Segment:\n\nLearn the general structure of ggplot2.\nLearn a few standard graphs for numerical/quantitative data:\n\nHistogram: one numerical variable\nSide-by-side boxplot: one numerical variable and one categorical variable\nSide-by-side violin plot: one numerical variable and one categorical variable"
  },
  {
    "objectID": "stat100_wk02wed.html#why-construct-a-graph",
    "href": "stat100_wk02wed.html#why-construct-a-graph",
    "title": "Stat 100",
    "section": "Why construct a graph?",
    "text": "Why construct a graph?\n\n\nTo explore the data.\n\n\nTo summarize the data.\n\n\nTo showcase trends and make comparisons.\n\n\nTo tell a compelling story."
  },
  {
    "objectID": "stat100_wk02wed.html#challenger",
    "href": "stat100_wk02wed.html#challenger",
    "title": "Stat 100",
    "section": "Challenger",
    "text": "Challenger\n\nOn January 27th, 1986, engineers from Morton Thiokol recommended NASA delay launch of space shuttle Challenger due to cold weather.\n\nBelieved cold weather impacted the o-rings that held the rockets together.\nUsed 13 charts in their argument.\n\nAfter a two hour conference call, the engineer’s recommendation was overruled due to lack of persuasive evidence and the launch proceeded.\nThe Challenger exploded 73 seconds into launch."
  },
  {
    "objectID": "stat100_wk02wed.html#challenger-1",
    "href": "stat100_wk02wed.html#challenger-1",
    "title": "Stat 100",
    "section": "Challenger",
    "text": "Challenger\nHere’s one of those charts."
  },
  {
    "objectID": "stat100_wk02wed.html#challenger-2",
    "href": "stat100_wk02wed.html#challenger-2",
    "title": "Stat 100",
    "section": "Challenger",
    "text": "Challenger\nHere’s another one of those charts."
  },
  {
    "objectID": "stat100_wk02wed.html#challenger-3",
    "href": "stat100_wk02wed.html#challenger-3",
    "title": "Stat 100",
    "section": "Challenger",
    "text": "Challenger\nHere’s a graphic I created from Edward Tufte’s data."
  },
  {
    "objectID": "stat100_wk02wed.html#challenger-4",
    "href": "stat100_wk02wed.html#challenger-4",
    "title": "Stat 100",
    "section": "Challenger",
    "text": "Challenger\nThis adaptation is a recreation of Edward Tufte’s graphic.\n\n\n\n\n\n\n\n\n\n\n\nFor more information on this example and other examples, check out Tufte’s book."
  },
  {
    "objectID": "stat100_wk02wed.html#now-lets-learn-the-grammar-of-graphics.",
    "href": "stat100_wk02wed.html#now-lets-learn-the-grammar-of-graphics.",
    "title": "Stat 100",
    "section": "Now let’s learn the Grammar of Graphics.",
    "text": "Now let’s learn the Grammar of Graphics.\n\nWe will use this grammar to:\n\n\nDecompose and understand existing graphs.\n\n\nCreate our own graphs with the R package ggplot2."
  },
  {
    "objectID": "stat100_wk02wed.html#grammar-of-graphics",
    "href": "stat100_wk02wed.html#grammar-of-graphics",
    "title": "Stat 100",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\n\ndata: Data frame that contains the raw data\n\nVariables used in the graph\n\ngeom: Geometric shape that the data are mapped to.\n\nEX: Point, line, bar, text, …\n\naesthetic: Visual properties of the geom\n\nEX: X (horizontal) position, y (vertical) position, color, fill, shape\n\nscale: Controls how data are mapped to the visual values of the aesthetic.\n\nEX: particular colors, log scale\n\nguide: Legend/key to help user convert visual display back to the data\n\n\nFor right now, we won’t focus on the names of particular types of graphs (e.g., scatterplot) but on the elements of graphs."
  },
  {
    "objectID": "stat100_wk02wed.html#many-ways-to-visually-tell-a-story",
    "href": "stat100_wk02wed.html#many-ways-to-visually-tell-a-story",
    "title": "Stat 100",
    "section": "Many Ways To Visually Tell A Story",
    "text": "Many Ways To Visually Tell A Story\nWashington Post’s Approach:\n\n\n\n\n\nPeriscopic’s Approach"
  },
  {
    "objectID": "stat100_wk02wed.html#bad-graphics",
    "href": "stat100_wk02wed.html#bad-graphics",
    "title": "Stat 100",
    "section": "Bad Graphics",
    "text": "Bad Graphics\nBecause of all the design choices, it is much easier to make a bad graph than a good graph."
  },
  {
    "objectID": "stat100_wk02wed.html#misleading-graphics",
    "href": "stat100_wk02wed.html#misleading-graphics",
    "title": "Stat 100",
    "section": "Misleading Graphics",
    "text": "Misleading Graphics\nBe careful that your design choices don’t cause your viewer to draw incorrect conclusions about the data:\n\n\n\n\n\n\nJust letting the software make all the design choices can still lead to misleading graphs (recall the Georgia COVID graph)."
  },
  {
    "objectID": "stat100_wk02wed.html#summary-thoughts-on-graphical-considerations",
    "href": "stat100_wk02wed.html#summary-thoughts-on-graphical-considerations",
    "title": "Stat 100",
    "section": "Summary Thoughts on Graphical Considerations",
    "text": "Summary Thoughts on Graphical Considerations\n\nGood graphics are one’s where the findings and insights are obvious to the viewer.\n\nAdd information and key context.\n\nFacilitate the comparisons that correspond to the research question.\n\nRecall the three Georgia COVID counts graphs from Day 1!\n\nData visualizations are not neutral.\nIt is easier to see the differences and similarities between different types of graphics if we learn the grammar of graphics.\nPracticing decomposing graphics should make it easier for us to compose our own graphics."
  },
  {
    "objectID": "stat100_wk02wed.html#load-necessary-packages",
    "href": "stat100_wk02wed.html#load-necessary-packages",
    "title": "Stat 100",
    "section": "Load Necessary Packages",
    "text": "Load Necessary Packages\n\n\n\n\n\nggplot2 is part of this collection of data science packages.\n\n# Load necessary packages\nlibrary(tidyverse)"
  },
  {
    "objectID": "stat100_wk02wed.html#data-setting-eco-totem-broadway-bicycle-count",
    "href": "stat100_wk02wed.html#data-setting-eco-totem-broadway-bicycle-count",
    "title": "Stat 100",
    "section": "Data Setting: Eco-Totem Broadway Bicycle Count",
    "text": "Data Setting: Eco-Totem Broadway Bicycle Count"
  },
  {
    "objectID": "stat100_wk02wed.html#import-the-data",
    "href": "stat100_wk02wed.html#import-the-data",
    "title": "Stat 100",
    "section": "Import the Data",
    "text": "Import the Data\n\njuly_2019 &lt;- read_csv(\"data/july_2019.csv\")\n\n# Inspect the data\nglimpse(july_2019)\n\nRows: 192\nColumns: 8\n$ DateTime  &lt;chr&gt; \"07/04/2019 12:00:00 AM\", \"07/04/2019 12:15:00 AM\", \"07/04/2…\n$ Day       &lt;chr&gt; \"Thursday\", \"Thursday\", \"Thursday\", \"Thursday\", \"Thursday\", …\n$ Date      &lt;date&gt; 2019-07-04, 2019-07-04, 2019-07-04, 2019-07-04, 2019-07-04,…\n$ Time      &lt;time&gt; 00:00:00, 00:15:00, 00:30:00, 00:45:00, 01:00:00, 01:15:00,…\n$ Total     &lt;dbl&gt; 2, 3, 2, 0, 3, 2, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, …\n$ Westbound &lt;dbl&gt; 2, 3, 1, 0, 2, 2, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, …\n$ Eastbound &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ Occasion  &lt;chr&gt; \"Fourth of July\", \"Fourth of July\", \"Fourth of July\", \"Fourt…"
  },
  {
    "objectID": "stat100_wk02wed.html#inspect-the-data",
    "href": "stat100_wk02wed.html#inspect-the-data",
    "title": "Stat 100",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n# Look at first few rows\nhead(july_2019)\n\n# A tibble: 6 × 8\n  DateTime             Day   Date       Time  Total Westbound Eastbound Occasion\n  &lt;chr&gt;                &lt;chr&gt; &lt;date&gt;     &lt;tim&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;   \n1 07/04/2019 12:00:00… Thur… 2019-07-04 00:00     2         2         0 Fourth …\n2 07/04/2019 12:15:00… Thur… 2019-07-04 00:15     3         3         0 Fourth …\n3 07/04/2019 12:30:00… Thur… 2019-07-04 00:30     2         1         1 Fourth …\n4 07/04/2019 12:45:00… Thur… 2019-07-04 00:45     0         0         0 Fourth …\n5 07/04/2019 01:00:00… Thur… 2019-07-04 01:00     3         2         1 Fourth …\n6 07/04/2019 01:15:00… Thur… 2019-07-04 01:15     2         2         0 Fourth …\n\n\nWhat does a row represent here?"
  },
  {
    "objectID": "stat100_wk02wed.html#inspect-the-data-1",
    "href": "stat100_wk02wed.html#inspect-the-data-1",
    "title": "Stat 100",
    "section": "Inspect the Data",
    "text": "Inspect the Data\n\n# Determine type\n# To access one variable: dataset$variable\nclass(july_2019$Day)\n\n[1] \"character\"\n\nclass(july_2019$Total)\n\n[1] \"numeric\"\n\nclass(july_2019)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\""
  },
  {
    "objectID": "stat100_wk02wed.html#grammar-of-graphics-1",
    "href": "stat100_wk02wed.html#grammar-of-graphics-1",
    "title": "Stat 100",
    "section": "Grammar of Graphics",
    "text": "Grammar of Graphics\n\ndata: Data frame that contains the raw data\n\nVariables used in the graph\n\ngeom: Geometric shape that the data are mapped to.\n\nEX: Point, line, bar, text, …\n\naesthetic: Visual properties of the geom\n\nEX: X (horizontal) position, y (vertical) position, color, fill, shape\n\nscale: Controls how data are mapped to the visual values of the aesthetic.\n\nEX: particular colors, log scale\n\nguide: Legend/key to help user convert visual display back to the data"
  },
  {
    "objectID": "stat100_wk02wed.html#ggplot2-example-code",
    "href": "stat100_wk02wed.html#ggplot2-example-code",
    "title": "Stat 100",
    "section": "ggplot2 example code",
    "text": "ggplot2 example code\nGuiding Principle: We will map variables from the data to the aesthetic attributes (e.g. location, size, shape, color) of geometric objects (e.g. points, lines, bars).\n\nggplot(data = ---, mapping = aes(---)) +\n  geom_---(---) \n\n\nThere are other layers, such as scales_---_---() and labs(), but we will wait on those."
  },
  {
    "objectID": "stat100_wk02wed.html#histograms",
    "href": "stat100_wk02wed.html#histograms",
    "title": "Stat 100",
    "section": "Histograms",
    "text": "Histograms\n\n\n\nBinned counts of data.\nGreat for assessing shape."
  },
  {
    "objectID": "stat100_wk02wed.html#data-shapes",
    "href": "stat100_wk02wed.html#data-shapes",
    "title": "Stat 100",
    "section": "Data Shapes",
    "text": "Data Shapes"
  },
  {
    "objectID": "stat100_wk02wed.html#histograms-1",
    "href": "stat100_wk02wed.html#histograms-1",
    "title": "Stat 100",
    "section": "Histograms",
    "text": "Histograms\n\n\n# Create histogram\nggplot(data = july_2019, \n       mapping = aes(x = Total)) +\n  geom_histogram()"
  },
  {
    "objectID": "stat100_wk02wed.html#histograms-2",
    "href": "stat100_wk02wed.html#histograms-2",
    "title": "Stat 100",
    "section": "Histograms",
    "text": "Histograms\n\n\n# Create histogram\nggplot(data = july_2019, \n       mapping = aes(x = Total)) +\n  geom_histogram(color = \"white\",\n                 fill = \"violetred1\",\n                 bins = 50)\n\n\n\n\n\n\n\n\n\n\n\nmapping to a variable goes in aes()\nsetting to a specific value goes in the geom_---()"
  },
  {
    "objectID": "stat100_wk02wed.html#boxplots",
    "href": "stat100_wk02wed.html#boxplots",
    "title": "Stat 100",
    "section": "Boxplots",
    "text": "Boxplots\n\n\n\nFive number summary:\n\nMinimum\nFirst quartile (Q1)\nMedian\nThird quartile (Q3)\nMaximum\n\nInterquartile range (IQR) \\(=\\) Q3 \\(-\\) Q1\nOutliers: unusual points\n\nBoxplot defines unusual as being beyond \\(1.5*IQR\\) from \\(Q1\\) or \\(Q3\\).\n\nWhiskers: reach out to the furthest point that is NOT an outlier"
  },
  {
    "objectID": "stat100_wk02wed.html#boxplots-1",
    "href": "stat100_wk02wed.html#boxplots-1",
    "title": "Stat 100",
    "section": "Boxplots",
    "text": "Boxplots\n\n\n# Create boxplot\nggplot(data = july_2019, \n       mapping = aes(x = Occasion, \n                     y = Total)) +\n  geom_boxplot()"
  },
  {
    "objectID": "stat100_wk02wed.html#boxplots-2",
    "href": "stat100_wk02wed.html#boxplots-2",
    "title": "Stat 100",
    "section": "Boxplots",
    "text": "Boxplots\n\n\nggplot(data = july_2019, \n       mapping = aes(x = Occasion, \n                     y = Total)) +\n  geom_boxplot(fill = \"springgreen3\")"
  },
  {
    "objectID": "stat100_wk02wed.html#boxplots-3",
    "href": "stat100_wk02wed.html#boxplots-3",
    "title": "Stat 100",
    "section": "Boxplots",
    "text": "Boxplots\n\n\nggplot(data = july_2019, \n       mapping = aes(x = Occasion, \n                     y = Total,\n                     fill = Occasion)) +\n  geom_boxplot()"
  },
  {
    "objectID": "stat100_wk02wed.html#boxplots-4",
    "href": "stat100_wk02wed.html#boxplots-4",
    "title": "Stat 100",
    "section": "Boxplots",
    "text": "Boxplots\n\n\nggplot(data = july_2019, \n       mapping = aes(x = Occasion, \n                     y = Total,\n                     fill = Occasion)) +\n  geom_boxplot() +\n  guides(fill = \"none\")"
  },
  {
    "objectID": "stat100_wk02wed.html#violin-plots",
    "href": "stat100_wk02wed.html#violin-plots",
    "title": "Stat 100",
    "section": "Violin Plots",
    "text": "Violin Plots\n\n\nggplot(data = july_2019, \n       mapping = aes(x = Occasion, \n                     y = Total,\n                     fill = Occasion)) +\n  geom_violin() +\n  guides(fill = \"none\")"
  },
  {
    "objectID": "stat100_wk02wed.html#boxplot-versus-violin-plots",
    "href": "stat100_wk02wed.html#boxplot-versus-violin-plots",
    "title": "Stat 100",
    "section": "Boxplot Versus Violin Plots",
    "text": "Boxplot Versus Violin Plots"
  },
  {
    "objectID": "stat100_wk02wed.html#recap-ggplot2",
    "href": "stat100_wk02wed.html#recap-ggplot2",
    "title": "Stat 100",
    "section": "Recap: ggplot2",
    "text": "Recap: ggplot2\n\nlibrary(tidyverse)\nggplot(data = ---, mapping = aes(---)) +\n  geom_---(---)"
  },
  {
    "objectID": "stat100_wk02wed.html#reminders",
    "href": "stat100_wk02wed.html#reminders",
    "title": "Stat 100",
    "section": "Reminders",
    "text": "Reminders\n\nClass in full swing:\n\nSections: Can find your assigned section in my.harvard but need to go to the linked spreadsheet to find the room!\nOffice hours\nWrap-ups on Th 3-4pm and Fri 10:30 - 11:30am in SC 309\nLecture quiz will be released in Gradescope after class today."
  },
  {
    "objectID": "stat100_wk12mon.html#announcements",
    "href": "stat100_wk12mon.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nNo sections or wrap-ups this week.\nP-Set 8 is due next Tues (5pm) but try to get most of your questions answered before Thanksgiving Break!\nNo new p-set or lecture quiz this week.\nOH schedule for Thanksgiving Week:\n\nSun, Nov 19th - Tues, Nov 21st: Happening with some modifications\nNo OHs Wed, Nov 22nd - Sun, Nov 26th!\n\n\nGoals for Today\n\n\n\nA bit of thanks.\nLearn theory-based statistical inference methods.\n\n\n\nIntroduce a new group of test statistics based on z-scores.\nGeneralize the SE method confidence interval formula."
  },
  {
    "objectID": "stat100_wk12mon.html#sample-statistics-as-random-variables",
    "href": "stat100_wk12mon.html#sample-statistics-as-random-variables",
    "title": "Stat 100",
    "section": "Sample Statistics as Random Variables",
    "text": "Sample Statistics as Random Variables\n\nSample statistics can be recast as random variables.\nNeed to figure out what random variable is a good approximation for our sample statistic.\n\nThen use the properties of that random variable to do inference.\n\nSometimes it is easier to find a good random variable approximation if we standardize our sample statistic first."
  },
  {
    "objectID": "stat100_wk12mon.html#p-value-options",
    "href": "stat100_wk12mon.html#p-value-options",
    "title": "Stat 100",
    "section": "P-value options",
    "text": "P-value options\n\n\nP-value using the generated null distribution:\n\npvalue &lt;- null_dist %&gt;%\n  get_p_value(obs_stat = t_obs,\n              direction = \"both\")\npvalue\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.012\n\n\n\nP-value using an approximate probability function:\n\n# Using t distribution\npt(q = t_obs$stat, df = 52)*2\n\n         t \n0.02468707 \n\n\n\n\nDo-it-all function:\n\nt_test(FloridaLakes, response = pH, mu = 7,\n       alternative = \"two-sided\")\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     -2.31    52  0.0247 two.sided       6.59     6.24     6.95"
  },
  {
    "objectID": "stat100_wk12mon.html#reminders",
    "href": "stat100_wk12mon.html#reminders",
    "title": "Stat 100",
    "section": "Reminders:",
    "text": "Reminders:\n\n\nNo sections or wrap-ups this week.\nP-Set 8 is due next Tues (5pm) but try to get most of your questions answered before Thanksgiving Break!\nNo new p-set or lecture quiz this week.\nOH schedule for Thanksgiving Week:\n\nSun, Nov 19th - Tues, Nov 21st: Happening with some modifications\nNo OHs Wed, Nov 22nd - Sun, Nov 26th!"
  },
  {
    "objectID": "stat100_wk09mon.html#announcements",
    "href": "stat100_wk09mon.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nOct 30th Today: Hex or Treat Day in Stat 100\n\nIf you are wearing a Halloween costume, come to the front before or after class for your hex sticker or treat!\n\n\nGoals for Today\n\n\n\nEstimation\nBootstrap distributions\n\n\n\nBootstrapped confidence intervals"
  },
  {
    "objectID": "stat100_wk09mon.html#estimation",
    "href": "stat100_wk09mon.html#estimation",
    "title": "Stat 100",
    "section": "Estimation",
    "text": "Estimation\nGoal: Estimate the value of a population parameter using data from the sample.\nSub-Goal: Quantify our uncertainty in using the sample to say something about the population.\n\nConfidence Interval (CI): Interval of plausible values for a parameter\nForm of a 95% Confidence Interval:\n\\[\\begin{align*}\n\\mbox{statistic} &\\pm \\mbox{Margin of Error}\\\\\n\\mbox{statistic} &\\pm 2\\mbox{SE}\n\\end{align*}\\]\n\n\nProblem: To compute the SE, we need many samples from the population. We have 1 sample.\nSolution: Approximate the sampling distribution using ONLY OUR ONE SAMPLE!"
  },
  {
    "objectID": "stat100_wk09mon.html#load-packages-and-data",
    "href": "stat100_wk09mon.html#load-packages-and-data",
    "title": "Stat 100",
    "section": "Load Packages and Data",
    "text": "Load Packages and Data\n\nlibrary(tidyverse)\nlibrary(infer)\n\nLet’s return to the movies dataset and estimate numerical quantities about Hollywood movies.\n\n# Read in data\nmovies &lt;- read_csv(\"https://www.lock5stat.com/datasets2e/HollywoodMovies.csv\")"
  },
  {
    "objectID": "stat100_wk09mon.html#estimation-for-a-single-mean",
    "href": "stat100_wk09mon.html#estimation-for-a-single-mean",
    "title": "Stat 100",
    "section": "Estimation for a Single Mean",
    "text": "Estimation for a Single Mean\nWhat is the average amount of money \\((\\mu)\\) made in the opening weekend?\n\n# Compute the summary statistic\nx_bar &lt;- movies %&gt;%\n  drop_na(OpeningWeekend) %&gt;%\n  specify(response = OpeningWeekend) %&gt;%\n  calculate(stat = \"mean\")\nx_bar\n\nResponse: OpeningWeekend (numeric)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  20.6\n\n\n\nWhy is our numerical quantity a mean and not a proportion or correlation here?"
  },
  {
    "objectID": "stat100_wk09mon.html#estimation-for-a-single-mean-1",
    "href": "stat100_wk09mon.html#estimation-for-a-single-mean-1",
    "title": "Stat 100",
    "section": "Estimation for a Single Mean",
    "text": "Estimation for a Single Mean\n\n\nset.seed(999)\n\n\n# Construct bootstrap distribution\nbootstrap_dist &lt;- movies %&gt;%\n  drop_na(OpeningWeekend) %&gt;%\n  specify(response = OpeningWeekend) %&gt;%\n  generate(reps =  1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"mean\")\n\n# Look at bootstrap distribution\nggplot(data = bootstrap_dist, \n       mapping = aes(x = stat)) +\n  geom_histogram(color = \"white\")"
  },
  {
    "objectID": "stat100_wk09mon.html#estimation-for-a-single-mean-se-method",
    "href": "stat100_wk09mon.html#estimation-for-a-single-mean-se-method",
    "title": "Stat 100",
    "section": "Estimation for a Single Mean – SE Method",
    "text": "Estimation for a Single Mean – SE Method\n\n\n# Get confidence interval\nci &lt;- bootstrap_dist %&gt;% \n  get_confidence_interval(type = \"se\", level = 0.95,\n                          point_estimate = x_bar)\nci\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     19.0     22.2\n\n\n\nInterpretation: The point estimate is $ 20.6M. I am 95% confidence that the average amount of money made by all Hollywood movies is between $ 19M and $ 22.2M.\nInline R code: The point estimate is $ 20.6M. I am 95% confidence that the average amount of money made by all Hollywood movies is between $ 19.0243721 M and $ 22.2 M."
  },
  {
    "objectID": "stat100_wk09mon.html#estimation-for-a-single-mean-2",
    "href": "stat100_wk09mon.html#estimation-for-a-single-mean-2",
    "title": "Stat 100",
    "section": "Estimation for a Single Mean",
    "text": "Estimation for a Single Mean\n\n\n# Visualize confidence interval\nbootstrap_dist %&gt;%\n  visualize() +\n  shade_confidence_interval(endpoints = ci)"
  },
  {
    "objectID": "stat100_wk09mon.html#estimation-for-a-single-mean-percentile-method",
    "href": "stat100_wk09mon.html#estimation-for-a-single-mean-percentile-method",
    "title": "Stat 100",
    "section": "Estimation for a Single Mean – Percentile Method",
    "text": "Estimation for a Single Mean – Percentile Method\n\n\n# Get confidence interval \nci_95 &lt;- bootstrap_dist %&gt;% \n  get_confidence_interval(type = \"percentile\",\n                          level = 0.95) \nci_95\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     19.0     22.3"
  },
  {
    "objectID": "stat100_wk09mon.html#estimation-for-difference-in-means",
    "href": "stat100_wk09mon.html#estimation-for-difference-in-means",
    "title": "Stat 100",
    "section": "Estimation for Difference in Means",
    "text": "Estimation for Difference in Means\nWhat is the difference in average amount of money made in the opening weekend between action movies and dramas \\((\\mu_1 - \\mu_2)\\)?\n\n\n# Compute the summary statistic\ndiff_x_bar &lt;- movies %&gt;%\n  drop_na(OpeningWeekend) %&gt;%\n  filter(Genre %in% c(\"Drama\", \"Action\")) %&gt;%\n  specify(OpeningWeekend ~ Genre) %&gt;%\n  calculate(stat = \"diff in means\")\ndiff_x_bar\n\n\nResponse: OpeningWeekend (numeric)\nExplanatory: Genre (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  21.7\n\n\n\n\nWhy a difference in means?"
  },
  {
    "objectID": "stat100_wk09mon.html#estimation-for-difference-in-means-1",
    "href": "stat100_wk09mon.html#estimation-for-difference-in-means-1",
    "title": "Stat 100",
    "section": "Estimation for Difference in Means",
    "text": "Estimation for Difference in Means\n\n\n# Construct bootstrap distribution\nbootstrap_dist &lt;- movies %&gt;%\n  drop_na(OpeningWeekend) %&gt;%\n  filter(Genre %in% c(\"Drama\", \"Action\")) %&gt;%\n  specify(OpeningWeekend ~ Genre) %&gt;%\n  generate(reps =  1000, type = \"bootstrap\") %&gt;%\n  calculate(stat = \"diff in means\",\n            order = c(\"Action\", \"Drama\"))\n\n# Look at bootstrap distribution\nggplot(data = bootstrap_dist,\n       mapping = aes(x = stat)) +\n  geom_histogram(color = \"white\")"
  },
  {
    "objectID": "stat100_wk09mon.html#estimation-for-difference-in-means-se-method",
    "href": "stat100_wk09mon.html#estimation-for-difference-in-means-se-method",
    "title": "Stat 100",
    "section": "Estimation for Difference in Means – SE Method",
    "text": "Estimation for Difference in Means – SE Method\n\n\n# Get confidence interval \nci_95 &lt;- bootstrap_dist %&gt;% \n  get_confidence_interval(type = \"se\", level = 0.95,\n                          point_estimate = diff_x_bar) \nci_95\n\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     15.9     27.5\n\n\n\nInterpretation: The point estimate is $ 21.7M. I am 95% confidence that action movies make, on average, between $ 15.9M and $ 27.5M more than dramas."
  },
  {
    "objectID": "stat100_wk09mon.html#comparing-cis",
    "href": "stat100_wk09mon.html#comparing-cis",
    "title": "Stat 100",
    "section": "Comparing CIs",
    "text": "Comparing CIs\n\n\nci_99 &lt;- bootstrap_dist %&gt;% \n  get_confidence_interval(type = \"se\", level = 0.99,\n                          point_estimate = diff_x_bar)\n\nbootstrap_dist %&gt;%\n  visualize() +\n  shade_confidence_interval(endpoints = ci_99,\n                            fill = \"gold1\",\n                            color = \"gold3\") +\n  shade_confidence_interval(endpoints = ci_95) \n\n\n\n\n\n\n\n\n\n\n\nWhy construct a 95% CI versus a 99% CI?\n\nNeed to dig into what we mean by confidence!"
  },
  {
    "objectID": "stat100_wk09mon.html#interpreting-confidence-intervals",
    "href": "stat100_wk09mon.html#interpreting-confidence-intervals",
    "title": "Stat 100",
    "section": "Interpreting Confidence Intervals",
    "text": "Interpreting Confidence Intervals\nExample: Estimating average household income before taxes in the US\n\n\nSE Method Formula:\n\\[\n\\mbox{statistic} \\pm{\\mbox{ME}}\n\\]\n\n\n# A tibble: 1 × 3\n     ME  lower  upper\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 1963. 60517. 64443.\n\n\n\n“The margin of [sampling] error can be described as the ‘penalty’ in precision for not talking to everyone in a given population. It describes the range that an answer likely falls between if the survey had reached everyone in a population, instead of just a sample of that population.” – Courtney Kennedy, Director of Survey Research at Pew Research Center\nCI = interval of plausible values for the parameter\n\n\nSafe interpretation: I am P% confident that {insert what the parameter represents in context} is between {insert lower bound} and {insert upper bound}."
  },
  {
    "objectID": "stat100_wk09mon.html#caution-confidence-intervals-in-the-wild",
    "href": "stat100_wk09mon.html#caution-confidence-intervals-in-the-wild",
    "title": "Stat 100",
    "section": "Caution: Confidence intervals in the wild",
    "text": "Caution: Confidence intervals in the wild\nStatement in an article for The BMJ (British Medical Journal):"
  },
  {
    "objectID": "stat100_wk09mon.html#reminders",
    "href": "stat100_wk09mon.html#reminders",
    "title": "Stat 100",
    "section": "Reminders:",
    "text": "Reminders:\n\nOct 30th Today: Hex or Treat Day in Stat 100\n\nIf you are wearing a Halloween costume, come to the front before or after class for your hex sticker or treat!"
  },
  {
    "objectID": "inference_procedures.html",
    "href": "inference_procedures.html",
    "title": "Useful Summary Tables",
    "section": "",
    "text": "Symbols and R Functions\n\n\n\n\n\n\nResponse\nExplanatory\nNumerical_Quantity\nParameter\nStatistic\nFunction\n\n\n\n\nquantitative\n-\nmean\n$\\mu$\n$\\bar{x}$\nt.test()\n\n\ncategorical\n-\nproportion\n$p$\n$\\hat{p}$\nprop.test()\n\n\nquantitative\ncategorical\ndifference in means\n$\\mu_1 - \\mu_2$\n$\\bar{x}_1 - \\bar{x}_2$\nt.test()\n\n\ncategorical\ncategorical\ndifference in proportions\n$p_1 - p_2$\n$\\hat{p}_1 - \\hat{p}_2$\nprop.test()\n\n\nquantitative\nquantitative\ncorrelation\n$\\rho$\n$r$\ncor.test()\n\n\n\n\n\n\n\n\n\n\nCommon Test Statistics and Approximate Distributions\n\n\n\n\n\n\nResponse\nExplanatory\nNumerical_Quantity\nTest_Statistic\nDistribution\nAssumptions\n\n\n\n\nquantitative\n-\nmean\n$\\frac{\\bar{x} - \\mu_o}{s/\\sqrt{n}}$\n$t(df = n - 1)$\n$n \\geq 30$ or data are normal\n\n\ncategorical\n-\nproportion\n$\\frac{\\hat{p} - p_o}{\\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}}$\n$N(0, 1)$\nTen successes, Ten failures\n\n\nquantitative\ncategorical\ndifference in means\n$\\frac{\\bar{x}_1 - \\bar{x}_2 - 0}{\\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}}$\n$t(df = \\min(n_1, n_2) - 1)$\n$n_1, n_2 \\geq 30$ or data are normal\n\n\ncategorical\ncategorical\ndifference in proportions\n$\\frac{\\hat{p}_1 - \\hat{p}_2 - 0}{\\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n_1} + \\frac{\\hat{p}(1 - \\hat{p})}{n_2}}}$\n$N(0, 1)$\nTen successes, Ten failures in each category\n\n\nquantitative\nquantitative\ncorrelation\n$\\frac{r - 0}{\\sqrt{\\frac{1 - r^2}{n - 2}}}$\n$t(df = n - 2)$\n$n \\geq 30$\n\n\n\n\n\n\n\n\n\n\nCommon Distribution-Based Confidence Interval Formulae\n\n\n\n\n\n\nResponse\nExplanatory\nNumerical_Quantity\nConfidence_Interval\nDistribution\nAssumptions\n\n\n\n\nquantitative\n-\nmean\n$\\bar{x} \\pm t^*s/\\sqrt{n}$\n$t(df = n - 1)$\n$n \\geq 30$ or data are normal\n\n\ncategorical\n-\nproportion\n$\\hat{p} \\pm z^* \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}$\n$N(0, 1)$\nTen successes, Ten failures\n\n\nquantitative\ncategorical\ndifference in means\n$\\bar{x}_1 - \\bar{x}_2 \\pm t^* \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}$\n$t(df = \\min(n_1, n_2) - 1)$\n$n_1, n_2 \\geq 30$ or data are normal\n\n\ncategorical\ncategorical\ndifference in proportions\n$\\hat{p}_1 - \\hat{p}_2 \\pm z^* \\sqrt{\\frac{\\hat{p}_1(1 - \\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2(1 - \\hat{p}_2)}{n_2}}$\n$N(0, 1)$\nTen successes, Ten failures in each category\n\n\nquantitative\nquantitative\ncorrelation\n$r \\pm t^* \\sqrt{\\frac{1 - r^2}{n - 2}}$\n$t(df = n - 2)$\n$n \\geq 30$"
  },
  {
    "objectID": "stat100_wk13mon.html#announcements",
    "href": "stat100_wk13mon.html#announcements",
    "title": "Stat 100",
    "section": "Announcements",
    "text": "Announcements\n\nRegular OH schedule ends on Tues, Dec 5th (last day of classes).\nWill have lots of office hours during Reading Period but not at the standard times.\n\nGoals for Today\n\n\n\nDiscuss more theory-based inference.\n\n\n\nSample size calculations."
  },
  {
    "objectID": "stat100_wk13mon.html#reminders",
    "href": "stat100_wk13mon.html#reminders",
    "title": "Stat 100",
    "section": "Reminders:",
    "text": "Reminders:"
  },
  {
    "objectID": "stat100_wk13mon.html#data-example",
    "href": "stat100_wk13mon.html#data-example",
    "title": "Stat 100",
    "section": "Data Example",
    "text": "Data Example\nWe have data on a random sub-sample of the 2010 American Community Survey. The American Community Survey is given every year to a random sample of US residents.\n\n# Libraries\nlibrary(tidyverse)\nlibrary(Lock5Data)\n\n# Data\ndata(ACS)\n# Focus on adults\nACS_adults &lt;- filter(ACS, Age &gt;= 18)\n\nglimpse(ACS_adults)\n\nRows: 1,936\nColumns: 9\n$ Sex             &lt;int&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, …\n$ Age             &lt;int&gt; 38, 18, 21, 55, 51, 28, 46, 80, 62, 41, 37, 42, 69, 48…\n$ Married         &lt;int&gt; 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, …\n$ Income          &lt;dbl&gt; 64.0, 0.0, 4.0, 34.0, 30.0, 13.7, 114.0, 0.0, 0.0, 0.0…\n$ HoursWk         &lt;int&gt; 40, 0, 20, 40, 40, 40, 60, 0, 0, 0, 40, 42, 0, 60, 0, …\n$ Race            &lt;fct&gt; white, black, white, other, black, white, white, white…\n$ USCitizen       &lt;int&gt; 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, …\n$ HealthInsurance &lt;int&gt; 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, …\n$ Language        &lt;int&gt; 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, …"
  }
]