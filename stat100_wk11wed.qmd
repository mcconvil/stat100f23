---
pagetitle: "Probability Concepts"
format: 
  revealjs:
    html-math-method: mathjax
    chalkboard: true
    incremental: true
    theme: [default, custom.scss]
    multiplex: true
    height: 900
    width: 1600
    slide-number: c
    auto-stretch: false
    callout-appearance: simple
    pdf-max-pages-per-slide: 1
    menu: 
      side: right
      numbers: true
    code-overflow: wrap
#    fontsize: 20
execute:
  echo: true
  warning: false
  message: false
  fig-asp: 0.75
  fig-width: 8
#  fig-align: center
  fig-dpi: 500
---

```{r}
#| include: false
#| warning: false
#| message: false
# Some global options still not available in quarto
 knitr::opts_chunk$set(fig.align = 'center')
library(knitr)
library(tidyverse)
theme_update(text = element_text(size = 13))
```

------------------------------------------------------------------------

::: columns
::: {.column .center width="45%"}
![](img/DAW.png){width="90%"}
:::

::: {.column .center width="55%"}
[Probability Concepts]{.custom-title}

<br> <br> <br>

[Kelly McConville]{.custom-subtitle}

[Stat 100 <br> Week 11 \| Fall 2023]{.custom-subtitle}
:::
:::

------------------------------------------------------------------------

## Announcements

-   Only Thursday wrap-ups this week!
-   No sections or wrap-ups during Thanksgiving Week.
-   OH schedule for Thanksgiving Week:
    -   Sun, Nov 19th - Tues, Nov 21st: [Happening with some modifications](https://docs.google.com/spreadsheets/d/1eGlvDVPFceat2xck-y0r_rhrXPZBxjkW-rmIWFqg68w/edit?usp=sharing)
    -   No OHs Wed, Nov 22nd - Sun, Nov 26th!

### Goals for Today

::: columns
::: column
-   Learn about conditional probabilities

-   Cover continuous random variables
:::

::: column
-   Learn important **named** random variables

-   Discuss the **Central Limit Theorem**
:::
:::

------------------------------------------------------------------------

### Conditional Probabilities

> "Conditioning is the soul of statistics." --- Joe Blitzstein, Stat 110 Prof

**Question**: What do we mean by "conditioning"?

-   Most polar bears are twins. Therefore, if you're a twin, you're probably a polar bear.
    -   P(twin given polar bear) $\neq$ P(polar bear given twin)
-   The p-value is a conditional probability.
    -   P-value = P(data given $H_o$) ( $\neq$ P( $H_o$ given data))

------------------------------------------------------------------------

### Conditional Probabilities

**Other favorite examples:**

-   P(have COVID given wear mask) $\neq$ P(wear mask given have COVID)
    -   In a CDC study, P(wear mask given have COVID) = 0.71 while P(have COVID given wear mask) is much lower.
-   P(it is raining given there are clouds directly overhead) $\neq$ Pr(there are clouds directly overhead given it is raining)

::: fragment
**Notation**: P(A given B) = P(A \| B)
:::

------------------------------------------------------------------------

## Example {.smaller}

Testing for COVID-19 was an important part of the *Keep Harvard Healthy* Program. There are a variety of COVID-19 tests out there but for this problem let's assume the following:

::: nonincremental
-   The test gives a **false negative** result 13% of the time where a false negative case is a person with COVID-19 but the test says they don't have it.
-   The test gives a **false positive** result 5% of the time where a false positive case is a person who doesn't have COVID-19 but the test says they do.
:::

Let's assume the true prevalence is 1%. During the 2021-2022 school year, each week they tested about 30,000 Harvard affiliates. Use the assumed percentages to fill in the following table of potential outcomes:

|                              | Positive Test Result | Negative Test Result | Total  |
|-----------------------|-----------------|-----------------|----------------|
| Actually have COVID-19       |                      |                      |        |
| Actually don't have COVID-19 |                      |                      |        |
| Total                        |                      |                      | 30,000 |

::: columns
::: column
P(test - \| have COVID) =

P(have COVID \| test -) =
:::

::: column
P(test + \| don't have COVID) =

P(don't have COVID \| test +) =
:::
:::

------------------------------------------------------------------------

## Example

The false negative rate of COVID-19 tests have varied wildly. One paper estimated it could be as high as 54%.

Recreate the table with this new false negative rate.

|                              | Positive Test Result | Negative Test Result | Total  |
|-----------------------|-----------------|-----------------|----------------|
| Actually have COVID-19       |                      |                      |        |
| Actually don't have COVID-19 |                      |                      |        |
| Total                        |                      |                      | 30,000 |

::: columns
::: column
P(test - \| have COVID) =

P(have COVID \| test -) =
:::

::: column
P(test + \| don't have COVID) =

P(don't have COVID \| test +) =
:::
:::

------------------------------------------------------------------------

### Random Variables

For a discrete random variable, care about its:

-   Distribution: $p(x) = P(X = x)$

-   Center -- Mean:

::: fragment
$$
\mu = \sum x p(x)
$$
:::

-   Spread -- Variance & Standard Deviation:

::: fragment
$$
\sigma^2 = \sum (x - \mu)^2 p(x)
$$

$$
\sigma = \sqrt{ \sum (x - \mu)^2 p(x)}
$$
:::

------------------------------------------------------------------------

### Random Variables

If a random variable, $X$, is a **continuous** RV, then it can take on any value in an interval.

-   Probability function:
    -   $P(X = x) = 0$ so

::: fragment
$$
p(x) \color{orange}{\approx} P(X = x)
$$

but if $p(4) > p(2)$ that still means that X is more likely to take on values around 4 than values around 2.
:::

------------------------------------------------------------------------

### Random Variables: Continuous

Change $\sum$ to $\int$:

$\int p(x) dx = 1$.

Center -- Mean/Expected value:

$$
\mu = \int x p(x) dx
$$

------------------------------------------------------------------------

### Random Variables: Continuous

Change $\sum$ to $\int$:

Spread -- Standard deviation:

$$
\sigma = \sqrt{ \int (x - \mu)^2 p(x) dx}
$$

# Why do we care about random variables?

[We will recast our sample statistics as random variables.]{.custom-subtitle .fragment}

[Use the distribution of the random variable to approximate the sampling distribution of our sample statistic!]{.custom-subtitle .fragment}

# Specific Named Random Variables

------------------------------------------------------------------------

### Specific Named Random Variables

-   There is a vast array of random variables out there.

-   But there are a few particular ones that we will find useful.

    -   Because these ones are used often, they have been given names.

-   Will identify these named RVs using the following format:

::: fragment
$$
X \sim \mbox{Name(values of key parameters)}
$$
:::

------------------------------------------------------------------------

### Bernoulli Random Variables

::: columns
::: column
$X \sim$ Bernoulli $(p)$

```{=tex}
\begin{align*}
X=   \left\{
\begin{array}{ll}
      1 & \mbox{success} \\
      0 & \mbox{failure} \\
\end{array} 
\right.  
\end{align*}
```
Important parameter:

$$
\begin{align*}
p & = \mbox{probability of success} \\
& = P(X = 1)
\end{align*}
$$
:::

::: column
```{r, echo = FALSE, fig.asp = 0.5}
library(tidyverse)
dat <- data.frame(y = c(.3, .7), x = c(0, 1))
ggplot(dat, aes(factor(x), y)) + 
  geom_col(fill = "deeppink3") +
  labs(title = expression(X %~% " Bernoulli(p = 0.7)"),
       x = "x", y = "p(x)")
```

Distribution:

| $x$    | 0       | 1   |
|--------|---------|-----|
| $p(x)$ | 1 - $p$ | $p$ |
:::
:::

------------------------------------------------------------------------

### Bernoulli Random Variables

::: columns
::: column
$X \sim$ Bernoulli $(p = 0.5)$

```{=tex}
\begin{align*}
X=   \left\{
\begin{array}{ll}
      1 & \mbox{success} \\
      0 & \mbox{failure} \\
\end{array} 
\right.  
\end{align*}
```
:::

::: column
```{r, echo = FALSE, fig.asp = 0.5}
library(tidyverse)
dat <- data.frame(y = c(.5, .5), x = c(0, 1))
ggplot(dat, aes(factor(x), y)) + 
  geom_col(fill = "deeppink3") +
  labs(title = expression(X %~% " Bernoulli(p = 0.5)"),
       x = "x", y = "p(x)")
```

Distribution:

| $x$    | 0   | 1   |
|--------|-----|-----|
| $p(x)$ | 0.5 | 0.5 |
:::
:::

------------------------------------------------------------------------

### Bernoulli Random Variables

::: columns
::: column
$X \sim$ Bernoulli $(p)$

```{=tex}
\begin{align*}
X=   \left\{
\begin{array}{ll}
      1 & \mbox{success} \\
      0 & \mbox{failure} \\
\end{array} 
\right.  
\end{align*}
```
:::

::: column
Distribution:

| $x$    | 0       | 1   |
|--------|---------|-----|
| $p(x)$ | 1 - $p$ | $p$ |
:::
:::

Mean:

$$
\begin{align*}
\mu &= \sum x p(x) \\
& = 1*p + 0*(1 - p) \\
& = p
\end{align*}
$$

------------------------------------------------------------------------

### Bernoulli Random Variables

::: columns
::: column
$X \sim$ Bernoulli $(p)$

```{=tex}
\begin{align*}
X=   \left\{
\begin{array}{ll}
      1 & \mbox{success} \\
      0 & \mbox{failure} \\
\end{array} 
\right.  
\end{align*}
```
:::

::: column
Distribution:

| $x$    | 0       | 1   |
|--------|---------|-----|
| $p(x)$ | 1 - $p$ | $p$ |
:::
:::

Standard deviation:

$$
\begin{align*}
\sigma & = \sqrt{ \sum (x - \mu)^2 p(x)} \\
& = \sqrt{(1 - p)^2*p + (0 - p)^2*(1 - p)}  \\
& = \sqrt{p(1 - p)}
\end{align*}
$$

------------------------------------------------------------------------

### Normal Random Variables

$X \sim$ Normal $(\mu, \sigma)$

Distribution:

$$
p(x) = \frac{1}{\sqrt{2\pi \sigma^2}}\exp{\left(-\frac{(x - \mu)^2}{2\sigma^2} \right)}
$$ where $-\infty < x < \infty$

::: columns
::: column
-   Mean: $\mu$

-   Standard deviation: $\sigma$
:::

::: column
```{r, echo = FALSE, fig.asp = 0.6}
library(tidyverse)
dat <- data.frame(x = seq(from = -4, to = 8, length.out = 1000)) %>%
  mutate(X1 = dnorm(x, mean = 0, sd = 1),
         X2 = dnorm(x, mean = 2, sd = 1),
         X3 = dnorm(x, mean = 2, sd = 2)) %>%
  pivot_longer(-x, names_to = "variable") %>%
  mutate(variable = case_when(
    variable == "X1" ~ "N(0, 1)",
    variable == "X2" ~ "N(2, 1)",
    TRUE ~ "N(2, 2)"
  ))
ggplot(dat, aes(x = x, y = value, color = variable)) + 
  geom_line(size = 2) +
  labs(title = expression(paste("X", "~", "N", "(", mu, ",", sigma, ")")),
       x = "x", y = "p(x)")
```
:::
:::

------------------------------------------------------------------------

### Normal Random Variables

$X \sim$ Normal $(\mu, \sigma)$

::: columns
::: column
**Notes:**

::: nonincremental
(a) Area under the curve = 1.

(b) Height $\approx$ how likely values are to occur

(c) Super special Normal RV:
:::

$Z \sim$ Normal $(\mu = 0, \sigma = 1)$.
:::

::: column
```{r, echo = FALSE, fig.asp = 0.6}
library(tidyverse)
dat <- data.frame(x = seq(from = -4, to = 8, length.out = 1000)) %>%
  mutate(X1 = dnorm(x, mean = 0, sd = 1),
         X2 = dnorm(x, mean = 2, sd = 1),
         X3 = dnorm(x, mean = 2, sd = 2)) %>%
  pivot_longer(-x, names_to = "variable") %>%
  mutate(variable = case_when(
    variable == "X1" ~ "N(0, 1)",
    variable == "X2" ~ "N(2, 1)",
    TRUE ~ "N(2, 2)"
  ))
ggplot(dat, aes(x = x, y = value, color = variable)) + 
  geom_line(size = 2) +
  labs(title = expression(paste("X", "~", "N", "(", mu, ",", sigma, ")")),
       x = "x", y = "p(x)")
```
:::
:::

------------------------------------------------------------------------

### Normal Random Variables

$X \sim$ Normal $(\mu, \sigma)$

::: columns
::: column
**Notes:**

-   The Normal curve will be a good approximation for **MANY distributions.**

-   But sometimes its **tails** just aren't fat enough.
:::

::: column
```{r, echo = FALSE, fig.asp = 0.6}
library(tidyverse)
dat <- data.frame(x = seq(from = -4, to = 8, length.out = 1000)) %>%
  mutate(X1 = dnorm(x, mean = 0, sd = 1),
         X2 = dnorm(x, mean = 2, sd = 1),
         X3 = dnorm(x, mean = 2, sd = 2)) %>%
  pivot_longer(-x, names_to = "variable") %>%
  mutate(variable = case_when(
    variable == "X1" ~ "N(0, 1)",
    variable == "X2" ~ "N(2, 1)",
    TRUE ~ "N(2, 2)"
  ))
ggplot(dat, aes(x = x, y = value, color = variable)) + 
  geom_line(size = 2) +
  labs(title = expression(paste("X", "~", "N", "(", mu, ",", sigma, ")")),
       x = "x", y = "p(x)")
```
:::
:::

------------------------------------------------------------------------

### t Random Variables

$X \sim$ t(df)

::: columns
::: column
Distribution:

$$
p(x) = \frac{\Gamma(\mbox{df} + 1)}{\sqrt{\mbox{df}\pi} \Gamma(2^{-1}\mbox{df})}\left(1 + \frac{x^2}{\mbox{df}} \right)^{-\frac{df + 1}{2}}
$$

where $-\infty < x < \infty$

-   Mean: 0

-   Standard deviation: $\sqrt{\mbox{df}/(\mbox{df} - 2)}$
:::

::: column
```{r, echo = FALSE}
library(tidyverse)
dat <- data.frame(x = seq(from = -5, to = 5, length.out = 1000)) %>%
  mutate(X1 = dnorm(x, mean = 0, sd = 1),
         X2 = dt(x, df = 2),
         X3 = dt(x, df = 8)) %>%
  pivot_longer(-x, names_to = "variable") %>%
  mutate(variable = case_when(
    variable == "X1" ~ "N(0, 1)",
    variable == "X2" ~ "t(2)",
    TRUE ~ "t(8)"
  ))
ggplot(dat, aes(x = x, y = value, color = variable)) + 
  geom_line(size = 1) +
  labs(title = "Standard Normal versus t",
       x = "x", y = "p(x)")
```
:::
:::

# It is time to recast some of the sample statistics we have been exploring as random variables!

------------------------------------------------------------------------

### Sample Statistics as Random Variables

Here are some of the sample statistics we've seen lately:

::: nonincremental
-   $\hat{p}$ = sample proportion of correct receiver guesses out of 329 trials

-   $\bar{x}_I - \bar{x}_N$ = difference in sample mean tuition between Ivies and non-Ivies

-   $\hat{p}_D - \hat{p}_Y$ = difference in sample improvement proportions between those who swam with dolphins and those who did not
:::

Why are these all random variables?

-   But they aren't **Bernoulli** random variables, nor **Normal** random variables, nor **t** random variables.

::: fragment
> *"All models are wrong but some are useful."* -- George Box
:::

------------------------------------------------------------------------

### Approximating These Distributions

-   $\hat{p}$ = sample proportion of correct receiver guesses out of 329 trials

::: columns
::: column
::: fragment
We generated its Null Distribution:

```{r, echo = FALSE}
library(infer)
# Construct data frame of sample results
esp <- data.frame(guess = c(rep("correct", 106),
                            rep("incorrect", 329 - 106)))

# Generate null distribution 
set.seed(4111)
null_dist <- esp %>%
  specify(response = guess, success = "correct") %>%
  hypothesize(null = "point", p = 0.25) %>%
  generate(reps = 1000, type = "draw") %>%
  calculate(stat ="prop")

# Graph null distribution with test statistic
dat <- data.frame(x = seq(16, 36, length.out = 1000)/100) %>%
  mutate(y = dnorm(x, mean = .25, sd = sqrt(.25*.75/329)))
ggplot(null_dist, aes(x = stat, y = ..density..)) +
  geom_histogram(bins = 23) +
  xlim(.16, .36) + ylim(0, 17)
```
:::
:::

::: column
::: fragment
Which is well approximated by the distribution of a N(0.25, 0.024).

```{r, echo = FALSE}
ggplot(null_dist, aes(x = stat, y = ..density..)) +
  geom_histogram(bins = 23)  +
  geom_line(data = dat, mapping = aes(x, y), color = "deeppink", size = 2) +
  xlim(.16, .36) + ylim(0, 17)
```
:::
:::
:::

------------------------------------------------------------------------

### Approximating These Distributions

-   $\bar{x}_I - \bar{x}_N$ = difference in sample mean tuition between Ivies and non-Ivies

::: columns
::: column
::: fragment
We generated its Null Distribution:

```{r, echo = FALSE}
#load the data
load("data/colleges_endow.Rdata")

#create ivy_binary
colleges <- colleges %>% 
  mutate(ivy_binary = case_when(
    tier_name == "Ivy Plus" ~ "Yes",
    tier_name != "Ivy Plus" ~ "No",
  ))

# Generate null distribution 
set.seed(4111)
null_dist <- colleges %>% 
  specify(scorecard_netprice_2013 ~ ivy_binary) %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000, type = "permute") %>% 
  calculate(stat = "diff in means", order = c("Yes", "No"))

# Graph null distribution with test statistic
# Find sd
dat <- data.frame(x = seq(-6000, 6000, length.out = 1000)) %>%
  mutate(y = dnorm(x, mean = 0, sd = (8395/t.test(scorecard_netprice_2013 ~ ivy_binary, data = colleges)$statistic)))
ggplot(null_dist, aes(x = stat, y = ..density..)) +
  geom_histogram(bins = 21) +
  xlim(-6000, 6000) + ylim(0, .00024)
```
:::
:::

::: column
::: fragment
Which is somewhat approximated by the distribution of a N(0, 1036).

```{r, echo = FALSE}
ggplot(null_dist, aes(x = stat, y = ..density..)) +
  geom_histogram(bins = 21) +
  xlim(-6000, 6000) + ylim(0, .0004)  +
  geom_line(data = dat, mapping = aes(x, y), color = "deeppink", size = 2)
```
:::
:::
:::

::: fragment
We will learn that a **standardized** version of the difference in sample means is **better** approximated by the distribution of a t(df = 7).
:::

------------------------------------------------------------------------

### Approximating These Distributions

-   $\hat{p}_D - \hat{p}_Y$ = difference in sample improvement proportions between those who swam with dolphins and those who did not

::: columns
::: column
::: fragment
We generated its Null Distribution:

```{r, echo = FALSE}
dolphins <- read.csv("data/Dolphins.txt", sep="")

null_dist <- dolphins %>%
  specify(improve ~ group, success = "yes") %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "diff in props", order = c("Treatment", "Control"))

# Graph null distribution with test statistic
phat1 <- 10/15
phat2 <- 3/15
dat <- data.frame(x = seq(-65, 65, length.out = 1000)/100) %>%
  mutate(y = dnorm(x, mean = 0, sd = sqrt(phat1*(1-phat1)/15 + phat2*(1-phat2)/15)))
ggplot(null_dist, aes(x = stat, y = ..density..)) +
  geom_histogram(bins = 23) +
  xlim(-.5, .5) + ylim(0, 7)
```
:::
:::

::: column
::: fragment
Which is **kinda somewhat** approximated by the probability function of a N(0, 0.16).

```{r, echo = FALSE}
ggplot(null_dist, aes(x = stat, y = ..density..)) +
  geom_histogram(bins = 23)  +
  geom_line(data = dat, mapping = aes(x, y), color = "deeppink", size = 2) +
  xlim(-.5, .5) + ylim(0, 7)
```
:::
:::
:::

------------------------------------------------------------------------

### Approximating These Distributions

-   How do I know **which** probability function is a good approximation for my sample statistic's distribution?

-   Once I have figured out a probability function that approximates the distribution of my sample statistic, how do I **use it** to do statistical inference?

------------------------------------------------------------------------

### Central Limit Theorem

**Central Limit Theorem (CLT):** For random samples and a large sample size $(n)$, the sampling distribution of many sample statistics is approximately normal.

**Example**: Harvard Trees

```{r, echo = FALSE, fig.asp = 0.3 }
library(tidyverse)
library(bosTrees)
harTrees <- camTrees %>%
  filter(Ownership == "Harvard", SiteType == "Tree") %>%
  drop_na(SpeciesShort)
harTrees <- harTrees %>%
  mutate(Maple = case_when(
    SpeciesShort == "Maple" ~ "yes",
    SpeciesShort != "Maple" ~ "no"
  ))

# Population distribution
p1 <- ggplot(data = harTrees, mapping = aes(x = Maple)) +
  geom_bar(aes(y = ..prop.., group = 1), stat = "count")  +
  labs(title = "The Population\n Distribution")

# Construct the sampling distribution
samp_dist <- harTrees %>% 
  rep_sample_n(size = 50, reps = 1000) %>%
  group_by(replicate) %>%
  summarize(statistic = mean(Maple == "yes"))

# Theoretical Distribution
dat <- data.frame(x = seq(1, 30, length.out = 1000)/100) %>%
  mutate(y = dnorm(x, mean = 0.138, sd = sqrt(0.138*(1 - 0.138)/50)))

# Graph the sampling distribution
p2 <- ggplot(data = samp_dist) + geom_blank() + theme_minimal()

library(cowplot)
plot_grid(p1, p2)
```

------------------------------------------------------------------------

### Approximating Sampling Distributions

**Central Limit Theorem (CLT):** For random samples and a large sample size $(n)$, the sampling distribution of many sample statistics is approximately normal.

**Example**: Harvard Trees

```{r, echo = FALSE, fig.asp = 0.3 }
# Graph the sampling distribution
p2 <- ggplot(data = samp_dist, mapping = , aes(x = statistic, y = ..density..)) +
  geom_histogram(bins = 16) +
  geom_line(data = dat, mapping = aes(x, y), color = "deeppink", size = 2) + labs(title = "The Sampling\n Distribution")

library(cowplot)
plot_grid(p1, p2)
```

-   But **which** Normal? (What is the value of $\mu$ and $\sigma$?)

------------------------------------------------------------------------

### Approximating Sampling Distributions

**Question**: But **which** normal? (What is the value of $\mu$ and $\sigma$?)

-   The sampling distribution of a statistic is always centered around:

-   The CLT also provides formula estimates of the standard error.

    -   The formula varies based on the statistic.

------------------------------------------------------------------------

### Approximating the Sampling Distribution of a Sample Proportion

CLT says: For large $n$ (At least 10 successes and 10 failures),

$$
\hat{p} \sim N \left(p, \sqrt{\frac{p(1-p)}{n}} \right)
$$

**Example**: Maples at Harvard

-   Parameter: $p$ = proportion of Maples at Harvard = 0.138

-   Statistic: $\hat{p}$ = proportion of Maples in a sample of 50 trees

::: fragment
$$
\hat{p} \sim N \left(0.138, \sqrt{\frac{0.138(1-0.138)}{50}} \right)
$$

**NOTE**: Can plug in the true parameter here because we had data on the whole population.
:::

------------------------------------------------------------------------

### Approximating the Sampling Distribution of a Sample Proportion

**Question**: What do we do when we don't have access to the whole population?

Have:

$$
\hat{p} \sim N \left(p, \sqrt{\frac{p(1-p)}{n}} \right)
$$

::: fragment
**Answer**: We will have to estimate the SE.
:::

------------------------------------------------------------------------

### Approximating the Sampling Distribution of a Sample Mean

There is a version of the CLT for many of our sample statistics.

For the sample mean, the CLT says: For large $n$ (At least 30 observations),

$$
\bar{x} \sim N \left(\mu, \frac{\sigma}{\sqrt{n}} \right)
$$

# Next time: Use the approximate distribution of the sample statistic (given by the CLT) to construct confidence intervals and to conduct hypothesis tests!

------------------------------------------------------------------------

## Reminders:

::: nonincremental
-   No sections or wrap-ups during Thanksgiving Week.
-   OH schedule for Thanksgiving Week:
    -   Sun, Nov 19th - Tues, Nov 21st: [Happening with some modifications](https://docs.google.com/spreadsheets/d/1eGlvDVPFceat2xck-y0r_rhrXPZBxjkW-rmIWFqg68w/edit?usp=sharing)
    -   No OHs Wed, Nov 22nd - Sun, Nov 26th!
:::
